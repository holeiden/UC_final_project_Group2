{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwAQtovS3jPL"
      },
      "source": [
        "# Setup environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLAIs631tODo",
        "outputId": "58ea4646-ebbb-4295-8f20-65433d537489"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "fatal: destination path '/content/drive/MyDrive/sustainbench' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!git clone https://github.com/sustainlab-group/sustainbench.git /content/drive/MyDrive/sustainbench\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "TIPUSPXkwvfu",
        "outputId": "bc76895c-1dce-478d-d1a1-4d6ada6f3593"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: '/content/drive/MyDrive/sustainbench'\n",
            "/content\n",
            "‚è¨ Downloading https://github.com/conda-forge/miniforge/releases/download/23.11.0-0/Mambaforge-23.11.0-0-Linux-x86_64.sh...\n",
            "üì¶ Installing...\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-f58f296236b3>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install -q condacolab'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcondacolab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mcondacolab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Create the Conda environment from the .yml file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/condacolab.py\u001b[0m in \u001b[0;36minstall_mambaforge\u001b[0;34m(prefix, env, run_checks)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0minstaller_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://github.com/conda-forge/miniforge/releases/download/23.11.0-0/Mambaforge-23.11.0-0-Linux-x86_64.sh\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0mchecksum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"3dfdcc162bf0df83b5025608dc2acdbbc575bd416b75701fb5863343c0517a78\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m     \u001b[0minstall_from_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstaller_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_checks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_checks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msha256\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchecksum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/condacolab.py\u001b[0m in \u001b[0;36minstall_from_url\u001b[0;34m(installer_url, prefix, env, run_checks, sha256)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"üì¶ Installing...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m     task = run(\n\u001b[0m\u001b[1;32m    102\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0;34m\"bash\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstaller_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"-bfp\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mcheck\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 505\u001b[0;31m             \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    506\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTimeoutExpired\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m             \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/subprocess.py\u001b[0m in \u001b[0;36mcommunicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1139\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stdin_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1141\u001b[0;31m                 \u001b[0mstdout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1142\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/sustainbench\n",
        "\n",
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()\n",
        "\n",
        "# Create the Conda environment from the .yml file\n",
        "!conda env create -f /content/drive/MyDrive/sustainbench/env_bench.yml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "Pa4XPpts3yZe"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from datetime import datetime\n",
        "import math\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "from tensorflow.keras.layers import Dense, LSTM\n",
        "import itertools\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvssv4vC3zq1"
      },
      "source": [
        "# 1. LSTM Baseline Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hASy1BzX4UQh"
      },
      "source": [
        "## Build the lstm model\n",
        "\n",
        "This LSTM_NeuralModel references from the oringial paper's github:  https://github.com/AnnaXWang/deep-transfer-learning-crop-prediction/blob/master/code/nnet_LSTM.py.\n",
        "\n",
        "And modified some codes to meet the updated Implementation reuqirement for TensorFlow 2.x."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3NkXjGG5f7S8"
      },
      "outputs": [],
      "source": [
        "\n",
        "class LSTM_Config():\n",
        "    B, W, C = 32,32,9\n",
        "    H = 32 #all season lengths will be 32\n",
        "    loss_lambda = 0.75\n",
        "    lstm_layers = 1\n",
        "    lstm_H = 200\n",
        "    dense = 356\n",
        "    season_len = 32\n",
        "\n",
        "    train_step = 10000000\n",
        "    lr = 0.005\n",
        "    #keep probability\n",
        "    drop_out = 0.75\n",
        "\n",
        "    def __init__(self, season_frac=None):\n",
        "        if season_frac is not None:\n",
        "            self.H = int(season_frac*self.H)\n",
        "\n",
        "def dense(input_data, H, N=None, name = \"dense\"):\n",
        "    if not N:\n",
        "        N = input_data.get_shape()[-1]\n",
        "    with tf.variable_scope(name):\n",
        "        W = tf.get_variable(\"W\", [N, H], initializer=tf.contrib.layers.variance_scaling_initializer())\n",
        "        b = tf.get_variable(\"b\", [1, H])\n",
        "        tf.summary.histogram(name + \".W\", W)\n",
        "        tf.summary.histogram(name + \".b\", b)\n",
        "        return tf.matmul(input_data, W, name=\"matmul\") + b\n",
        "\n",
        "def lstm_net(input_data,output_data,config,keep_prob = 1,name=\"lstm_net\"):\n",
        "    with tf.variable_scope(name):\n",
        "        lstm_cell = tf.nn.rnn_cell.LSTMCell(config.lstm_H,state_is_tuple=True)\n",
        "        lstm_cell = tf.nn.rnn_cell.DropoutWrapper(lstm_cell, output_keep_prob=keep_prob)\n",
        "        cell = tf.nn.rnn_cell.MultiRNNCell([lstm_cell] * config.lstm_layers,state_is_tuple=True)\n",
        "        state = cell.zero_state(config.B, tf.float32)\n",
        "        outputs, final_state = tf.nn.dynamic_rnn(cell, input_data,\n",
        "                       initial_state=state, time_major=True)\n",
        "        tf.summary.histogram(name + '.outputs', outputs)\n",
        "        output_final = tf.squeeze(tf.slice(outputs, [config.H-1,0,0] , [1,-1,-1]))\n",
        "        tf.summary.histogram(name + '.output_final', output_final)\n",
        "        fc1 = dense(output_final, config.dense, name=\"dense\")\n",
        "\n",
        "        logit = tf.squeeze(dense(fc1,1,name='logit'))\n",
        "        tf.summary.histogram(name + '.logit', logit)\n",
        "        loss_err = tf.nn.l2_loss(logit - output_data)\n",
        "        loss_reg = tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables()])\n",
        "        total_loss = config.loss_lambda * loss_err + (1 - config.loss_lambda) * loss_reg\n",
        "\n",
        "        tf.summary.scalar(name + '.loss_err', loss_err)\n",
        "        tf.summary.scalar(name + '.loss_reg', loss_reg)\n",
        "        tf.summary.scalar(name + '.loss_total', total_loss)\n",
        "\n",
        "        return logit,total_loss,fc1\n",
        "\n",
        "class LSTM_NeuralModel(tf.keras.Model):\n",
        "    def __init__(self, config, name=\"LSTM_Model\"):\n",
        "        super(LSTM_NeuralModel, self).__init__(name=name)\n",
        "        self.config = config\n",
        "        self.lstm = tf.keras.layers.LSTM(\n",
        "            units=config.lstm_H,\n",
        "            return_sequences=False,\n",
        "            return_state=False,\n",
        "            dropout=1 - config.drop_out,\n",
        "        )\n",
        "        self.dense1 = tf.keras.layers.Dense(config.dense, activation=\"relu\")\n",
        "        self.logit = tf.keras.layers.Dense(1, activation=None)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        x = tf.transpose(inputs, perm=[2, 0, 1, 3])\n",
        "        x = tf.reshape(x, [-1, self.config.W, self.config.H * self.config.C])\n",
        "\n",
        "        # LSTM layer\n",
        "        x = self.lstm(x, training=training)\n",
        "\n",
        "        # Dense layer\n",
        "        x = self.dense1(x)\n",
        "        output = self.logit(x)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swx5RHmLuqyO",
        "outputId": "e8cdc237-5291-4b01-e3a0-bb7e8febf623"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/sustainbench\n",
            "Downloading dataset to data/crop_yield...\n",
            "Downloading from Google Drive...\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/sustainbench\n",
        "\n",
        "from sustainbench import get_dataset\n",
        "from sustainbench.common.data_loaders import get_train_loader\n",
        "from sustainbench.common.data_loaders import get_eval_loader\n",
        "\n",
        "dataset = get_dataset(dataset='crop_yield', download=False, split_scheme='argentina') # argentina soybean\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CuaBymPEnhBz"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train_model(model, dataset, config, num_epochs=20):\n",
        "\n",
        "    loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=config.lr)\n",
        "\n",
        "    train_data = dataset.get_subset('train')\n",
        "    train_loader = get_train_loader('standard', train_data, batch_size=32)\n",
        "\n",
        "    model = LSTM_NeuralModel(config)\n",
        "    r2list = []\n",
        "    rmselist = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "    # Iterate through the train_loader to access batches of data\n",
        "        y_true = []\n",
        "        y_pred = []\n",
        "        batch_size = config.B\n",
        "        for batch_idx, (batch_x, batch_y) in enumerate(train_loader):\n",
        "            with tf.GradientTape() as tape:\n",
        "                predictions = model(batch_x, training=True)\n",
        "                loss = loss_fn(batch_y, predictions)\n",
        "\n",
        "            gradients = tape.gradient(loss, model.trainable_variables)\n",
        "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "            y_true.extend(batch_y.numpy().flatten())\n",
        "            y_pred.extend(predictions.numpy().flatten())\n",
        "            print(f'Epoch {epoch +1}, Batch {batch_idx +1}, Loss: {loss.numpy()}')\n",
        "\n",
        "        r2 = r2_score(y_true, y_pred)\n",
        "        rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "        r2list.append(r2)\n",
        "        rmselist.append(rmse)\n",
        "    return model, r2list, rmselist\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVrMWnNE0n2u"
      },
      "outputs": [],
      "source": [
        "def eval_model(model, dataset, config, num_epochs=20):\n",
        "    loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=config.lr)\n",
        "\n",
        "    eval_data = dataset.get_subset('val')\n",
        "    eval_loader = get_eval_loader('standard', eval_data, batch_size=32)\n",
        "\n",
        "    r2list_eval = []\n",
        "    rmselist_eval = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        y_true_eval = []\n",
        "        y_pred_eval = []\n",
        "        for batch_idx, (batch_x, batch_y) in enumerate(eval_loader):\n",
        "            with tf.GradientTape() as tape:\n",
        "                predictions = model(batch_x, training=True)\n",
        "                loss = loss_fn(batch_y, predictions)\n",
        "\n",
        "            gradients = tape.gradient(loss, model.trainable_variables)\n",
        "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "            y_true_eval.extend(batch_y.numpy().flatten())\n",
        "            y_pred_eval.extend(predictions.numpy().flatten())\n",
        "            print(f'Epoch {epoch +1}, Batch {batch_idx +1}, Loss: {loss.numpy()}')\n",
        "\n",
        "        r2_eval = r2_score(y_true_eval, y_pred_eval)\n",
        "        rmse_eval = np.sqrt(mean_squared_error(y_true_eval, y_pred_eval))\n",
        "        r2list_eval.append(r2_eval)\n",
        "        rmselist_eval.append(rmse_eval)\n",
        "    return r2list_eval, rmselist_eval\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oTvv7-nkf_dM"
      },
      "outputs": [],
      "source": [
        "def plot_training_results(r2list, rmselist, r2list_eval, rmselist_eval):\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(7, 3))\n",
        "\n",
        "    # Plot R2\n",
        "    axes[0].plot(r2list, label='R¬≤_train', color='purple', marker='o')\n",
        "    axes[0].plot(r2list_eval, label='R¬≤_eval', color='skyblue', marker='x')\n",
        "    axes[0].set_title('R¬≤ Over Epochs')\n",
        "    axes[0].set_xlabel('Epochs')\n",
        "    axes[0].set_ylabel('R¬≤')\n",
        "    axes[0].grid(True)\n",
        "    axes[0].legend()\n",
        "\n",
        "    # Plot RMSE\n",
        "    axes[1].plot(rmselist, label='RMSE_train', color='green', marker='o')\n",
        "    axes[1].plot(rmselist_eval, label='RMSE_eval', color='skyblue', marker='x')\n",
        "    axes[1].set_title('RMSE Over Epochs')\n",
        "    axes[1].set_xlabel('Epochs')\n",
        "    axes[1].set_ylabel('RMSE')\n",
        "    axes[1].grid(True)\n",
        "    axes[1].legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uppa60FwuBFe",
        "outputId": "0f4c2a53-8c96-40f2-8f7a-ed89aebb2629"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Batch 1, Loss: 7.585811614990234\n",
            "Epoch 1, Batch 2, Loss: 2.8962090015411377\n",
            "Epoch 1, Batch 3, Loss: 56.822444915771484\n",
            "Epoch 1, Batch 4, Loss: 3.477492332458496\n",
            "Epoch 1, Batch 5, Loss: 1.8745334148406982\n",
            "Epoch 1, Batch 6, Loss: 4.093189239501953\n",
            "Epoch 1, Batch 7, Loss: 3.4539170265197754\n",
            "Epoch 1, Batch 8, Loss: 2.978121280670166\n",
            "Epoch 1, Batch 9, Loss: 1.5308115482330322\n",
            "Epoch 1, Batch 10, Loss: 0.7054377794265747\n",
            "Epoch 1, Batch 11, Loss: 0.9585411548614502\n",
            "Epoch 1, Batch 12, Loss: 1.1137640476226807\n",
            "Epoch 1, Batch 13, Loss: 0.6875149607658386\n",
            "Epoch 1, Batch 14, Loss: 0.9054323434829712\n",
            "Epoch 1, Batch 15, Loss: 0.7153158783912659\n",
            "Epoch 1, Batch 16, Loss: 1.0092543363571167\n",
            "Epoch 1, Batch 17, Loss: 0.7516772747039795\n",
            "Epoch 1, Batch 18, Loss: 0.8111292719841003\n",
            "Epoch 1, Batch 19, Loss: 0.9740007519721985\n",
            "Epoch 1, Batch 20, Loss: 0.7460436820983887\n",
            "Epoch 1, Batch 21, Loss: 0.6599212884902954\n",
            "Epoch 1, Batch 22, Loss: 0.4912518262863159\n",
            "Epoch 1, Batch 23, Loss: 1.0317943096160889\n",
            "Epoch 1, Batch 24, Loss: 1.40216064453125\n",
            "Epoch 1, Batch 25, Loss: 0.9570498466491699\n",
            "Epoch 1, Batch 26, Loss: 0.4343262314796448\n",
            "Epoch 1, Batch 27, Loss: 0.8559143543243408\n",
            "Epoch 1, Batch 28, Loss: 0.9757981300354004\n",
            "Epoch 1, Batch 29, Loss: 0.678501307964325\n",
            "Epoch 1, Batch 30, Loss: 0.864490807056427\n",
            "Epoch 1, Batch 31, Loss: 0.9167231917381287\n",
            "Epoch 1, Batch 32, Loss: 0.6706323027610779\n",
            "Epoch 1, Batch 33, Loss: 0.6662663817405701\n",
            "Epoch 2, Batch 1, Loss: 0.7337448596954346\n",
            "Epoch 2, Batch 2, Loss: 1.0124704837799072\n",
            "Epoch 2, Batch 3, Loss: 0.6353319883346558\n",
            "Epoch 2, Batch 4, Loss: 0.7750365734100342\n",
            "Epoch 2, Batch 5, Loss: 0.8836550712585449\n",
            "Epoch 2, Batch 6, Loss: 0.42694178223609924\n",
            "Epoch 2, Batch 7, Loss: 0.8236514329910278\n",
            "Epoch 2, Batch 8, Loss: 0.701196014881134\n",
            "Epoch 2, Batch 9, Loss: 0.5541849136352539\n",
            "Epoch 2, Batch 10, Loss: 0.9596683382987976\n",
            "Epoch 2, Batch 11, Loss: 0.6077836751937866\n",
            "Epoch 2, Batch 12, Loss: 0.7741732597351074\n",
            "Epoch 2, Batch 13, Loss: 1.0751981735229492\n",
            "Epoch 2, Batch 14, Loss: 0.6603229641914368\n",
            "Epoch 2, Batch 15, Loss: 0.6454450488090515\n",
            "Epoch 2, Batch 16, Loss: 0.8674582242965698\n",
            "Epoch 2, Batch 17, Loss: 0.8626806735992432\n",
            "Epoch 2, Batch 18, Loss: 0.7597728371620178\n",
            "Epoch 2, Batch 19, Loss: 0.7908487319946289\n",
            "Epoch 2, Batch 20, Loss: 0.7456772923469543\n",
            "Epoch 2, Batch 21, Loss: 0.5376271605491638\n",
            "Epoch 2, Batch 22, Loss: 0.90859454870224\n",
            "Epoch 2, Batch 23, Loss: 0.772943377494812\n",
            "Epoch 2, Batch 24, Loss: 0.7309905290603638\n",
            "Epoch 2, Batch 25, Loss: 0.6220930814743042\n",
            "Epoch 2, Batch 26, Loss: 0.6251639127731323\n",
            "Epoch 2, Batch 27, Loss: 0.5233700275421143\n",
            "Epoch 2, Batch 28, Loss: 0.6113694310188293\n",
            "Epoch 2, Batch 29, Loss: 0.4238097071647644\n",
            "Epoch 2, Batch 30, Loss: 0.7147291302680969\n",
            "Epoch 2, Batch 31, Loss: 0.5410603880882263\n",
            "Epoch 2, Batch 32, Loss: 0.7056087255477905\n",
            "Epoch 2, Batch 33, Loss: 0.9396284222602844\n",
            "Epoch 3, Batch 1, Loss: 0.6920225620269775\n",
            "Epoch 3, Batch 2, Loss: 1.0560526847839355\n",
            "Epoch 3, Batch 3, Loss: 0.9000303745269775\n",
            "Epoch 3, Batch 4, Loss: 0.6913669109344482\n",
            "Epoch 3, Batch 5, Loss: 0.9396589994430542\n",
            "Epoch 3, Batch 6, Loss: 0.8756365776062012\n",
            "Epoch 3, Batch 7, Loss: 0.5111139416694641\n",
            "Epoch 3, Batch 8, Loss: 0.6000970602035522\n",
            "Epoch 3, Batch 9, Loss: 0.5677308440208435\n",
            "Epoch 3, Batch 10, Loss: 0.9939886927604675\n",
            "Epoch 3, Batch 11, Loss: 0.8399583101272583\n",
            "Epoch 3, Batch 12, Loss: 0.6645169258117676\n",
            "Epoch 3, Batch 13, Loss: 0.7113107442855835\n",
            "Epoch 3, Batch 14, Loss: 0.6257994771003723\n",
            "Epoch 3, Batch 15, Loss: 0.48152583837509155\n",
            "Epoch 3, Batch 16, Loss: 0.45453941822052\n",
            "Epoch 3, Batch 17, Loss: 0.5534162521362305\n",
            "Epoch 3, Batch 18, Loss: 0.8889116644859314\n",
            "Epoch 3, Batch 19, Loss: 0.7457086443901062\n",
            "Epoch 3, Batch 20, Loss: 0.8523422479629517\n",
            "Epoch 3, Batch 21, Loss: 0.8718268871307373\n",
            "Epoch 3, Batch 22, Loss: 0.5684326887130737\n",
            "Epoch 3, Batch 23, Loss: 1.0401774644851685\n",
            "Epoch 3, Batch 24, Loss: 0.4667612314224243\n",
            "Epoch 3, Batch 25, Loss: 0.7463517785072327\n",
            "Epoch 3, Batch 26, Loss: 0.6304817199707031\n",
            "Epoch 3, Batch 27, Loss: 0.8575947284698486\n",
            "Epoch 3, Batch 28, Loss: 0.8877876400947571\n",
            "Epoch 3, Batch 29, Loss: 0.7740781307220459\n",
            "Epoch 3, Batch 30, Loss: 0.6340177655220032\n",
            "Epoch 3, Batch 31, Loss: 0.6850588321685791\n",
            "Epoch 3, Batch 32, Loss: 0.5786545276641846\n",
            "Epoch 3, Batch 33, Loss: 0.4983096122741699\n",
            "Epoch 4, Batch 1, Loss: 0.8155977129936218\n",
            "Epoch 4, Batch 2, Loss: 0.5930757522583008\n",
            "Epoch 4, Batch 3, Loss: 0.6131066083908081\n",
            "Epoch 4, Batch 4, Loss: 0.7435591220855713\n",
            "Epoch 4, Batch 5, Loss: 0.9298182725906372\n",
            "Epoch 4, Batch 6, Loss: 0.5474051833152771\n",
            "Epoch 4, Batch 7, Loss: 0.5453859567642212\n",
            "Epoch 4, Batch 8, Loss: 0.5681458711624146\n",
            "Epoch 4, Batch 9, Loss: 0.468627005815506\n",
            "Epoch 4, Batch 10, Loss: 0.6919106245040894\n",
            "Epoch 4, Batch 11, Loss: 0.8253783583641052\n",
            "Epoch 4, Batch 12, Loss: 1.0900201797485352\n",
            "Epoch 4, Batch 13, Loss: 0.6265584230422974\n",
            "Epoch 4, Batch 14, Loss: 0.6038646101951599\n",
            "Epoch 4, Batch 15, Loss: 0.8554328680038452\n",
            "Epoch 4, Batch 16, Loss: 0.7293766140937805\n",
            "Epoch 4, Batch 17, Loss: 0.6508046388626099\n",
            "Epoch 4, Batch 18, Loss: 0.7296422719955444\n",
            "Epoch 4, Batch 19, Loss: 0.5922319889068604\n",
            "Epoch 4, Batch 20, Loss: 0.8982489109039307\n",
            "Epoch 4, Batch 21, Loss: 0.9943065047264099\n",
            "Epoch 4, Batch 22, Loss: 0.6835718154907227\n",
            "Epoch 4, Batch 23, Loss: 0.6762492060661316\n",
            "Epoch 4, Batch 24, Loss: 0.7091850638389587\n",
            "Epoch 4, Batch 25, Loss: 0.6736520528793335\n",
            "Epoch 4, Batch 26, Loss: 0.9163120985031128\n",
            "Epoch 4, Batch 27, Loss: 0.679907500743866\n",
            "Epoch 4, Batch 28, Loss: 0.6812068223953247\n",
            "Epoch 4, Batch 29, Loss: 0.6154898405075073\n",
            "Epoch 4, Batch 30, Loss: 0.6859158277511597\n",
            "Epoch 4, Batch 31, Loss: 0.4782986640930176\n",
            "Epoch 4, Batch 32, Loss: 0.7217065095901489\n",
            "Epoch 4, Batch 33, Loss: 0.5007684230804443\n",
            "Epoch 5, Batch 1, Loss: 0.6444153189659119\n",
            "Epoch 5, Batch 2, Loss: 0.5603384375572205\n",
            "Epoch 5, Batch 3, Loss: 0.5009284615516663\n",
            "Epoch 5, Batch 4, Loss: 0.6559791564941406\n",
            "Epoch 5, Batch 5, Loss: 0.6297926306724548\n",
            "Epoch 5, Batch 6, Loss: 0.7063908576965332\n",
            "Epoch 5, Batch 7, Loss: 0.9169657826423645\n",
            "Epoch 5, Batch 8, Loss: 0.757658839225769\n",
            "Epoch 5, Batch 9, Loss: 0.8314030170440674\n",
            "Epoch 5, Batch 10, Loss: 0.7984152436256409\n",
            "Epoch 5, Batch 11, Loss: 0.8459026217460632\n",
            "Epoch 5, Batch 12, Loss: 0.6430338621139526\n",
            "Epoch 5, Batch 13, Loss: 1.079472303390503\n",
            "Epoch 5, Batch 14, Loss: 0.7539085149765015\n",
            "Epoch 5, Batch 15, Loss: 0.6015210151672363\n",
            "Epoch 5, Batch 16, Loss: 0.51243656873703\n",
            "Epoch 5, Batch 17, Loss: 0.8024101257324219\n",
            "Epoch 5, Batch 18, Loss: 0.7398248910903931\n",
            "Epoch 5, Batch 19, Loss: 0.8868871331214905\n",
            "Epoch 5, Batch 20, Loss: 0.7405407428741455\n",
            "Epoch 5, Batch 21, Loss: 0.5936495065689087\n",
            "Epoch 5, Batch 22, Loss: 0.5171432495117188\n",
            "Epoch 5, Batch 23, Loss: 0.655228853225708\n",
            "Epoch 5, Batch 24, Loss: 0.598608136177063\n",
            "Epoch 5, Batch 25, Loss: 0.43719398975372314\n",
            "Epoch 5, Batch 26, Loss: 0.5721634030342102\n",
            "Epoch 5, Batch 27, Loss: 0.7603371739387512\n",
            "Epoch 5, Batch 28, Loss: 0.673941969871521\n",
            "Epoch 5, Batch 29, Loss: 0.8053638935089111\n",
            "Epoch 5, Batch 30, Loss: 0.5769221782684326\n",
            "Epoch 5, Batch 31, Loss: 0.9454151391983032\n",
            "Epoch 5, Batch 32, Loss: 0.7391958832740784\n",
            "Epoch 5, Batch 33, Loss: 0.4301699995994568\n",
            "Epoch 6, Batch 1, Loss: 0.6691688299179077\n",
            "Epoch 6, Batch 2, Loss: 0.6054208278656006\n",
            "Epoch 6, Batch 3, Loss: 0.6521074175834656\n",
            "Epoch 6, Batch 4, Loss: 0.5428328514099121\n",
            "Epoch 6, Batch 5, Loss: 0.5611352920532227\n",
            "Epoch 6, Batch 6, Loss: 0.6176068782806396\n",
            "Epoch 6, Batch 7, Loss: 0.3343537747859955\n",
            "Epoch 6, Batch 8, Loss: 0.7324728965759277\n",
            "Epoch 6, Batch 9, Loss: 0.9626150131225586\n",
            "Epoch 6, Batch 10, Loss: 0.8913633823394775\n",
            "Epoch 6, Batch 11, Loss: 0.5110522508621216\n",
            "Epoch 6, Batch 12, Loss: 0.8405012488365173\n",
            "Epoch 6, Batch 13, Loss: 0.7426727414131165\n",
            "Epoch 6, Batch 14, Loss: 0.5365809202194214\n",
            "Epoch 6, Batch 15, Loss: 0.43479031324386597\n",
            "Epoch 6, Batch 16, Loss: 0.9739619493484497\n",
            "Epoch 6, Batch 17, Loss: 0.5729886889457703\n",
            "Epoch 6, Batch 18, Loss: 0.7748498916625977\n",
            "Epoch 6, Batch 19, Loss: 0.5401372909545898\n",
            "Epoch 6, Batch 20, Loss: 0.6053320169448853\n",
            "Epoch 6, Batch 21, Loss: 0.649526834487915\n",
            "Epoch 6, Batch 22, Loss: 0.5547091364860535\n",
            "Epoch 6, Batch 23, Loss: 0.6836838126182556\n",
            "Epoch 6, Batch 24, Loss: 0.9382848739624023\n",
            "Epoch 6, Batch 25, Loss: 0.601839005947113\n",
            "Epoch 6, Batch 26, Loss: 1.070925235748291\n",
            "Epoch 6, Batch 27, Loss: 0.7805261611938477\n",
            "Epoch 6, Batch 28, Loss: 0.6821911931037903\n",
            "Epoch 6, Batch 29, Loss: 1.0294809341430664\n",
            "Epoch 6, Batch 30, Loss: 0.906807541847229\n",
            "Epoch 6, Batch 31, Loss: 0.46774446964263916\n",
            "Epoch 6, Batch 32, Loss: 1.061802625656128\n",
            "Epoch 6, Batch 33, Loss: 0.7122145891189575\n",
            "Epoch 7, Batch 1, Loss: 0.956840455532074\n",
            "Epoch 7, Batch 2, Loss: 0.9814291000366211\n",
            "Epoch 7, Batch 3, Loss: 0.9018850326538086\n",
            "Epoch 7, Batch 4, Loss: 0.38179904222488403\n",
            "Epoch 7, Batch 5, Loss: 0.8073965907096863\n",
            "Epoch 7, Batch 6, Loss: 0.644960880279541\n",
            "Epoch 7, Batch 7, Loss: 0.37994617223739624\n",
            "Epoch 7, Batch 8, Loss: 0.6827515363693237\n",
            "Epoch 7, Batch 9, Loss: 0.6911208629608154\n",
            "Epoch 7, Batch 10, Loss: 0.8976237177848816\n",
            "Epoch 7, Batch 11, Loss: 0.7062778472900391\n",
            "Epoch 7, Batch 12, Loss: 0.9395129680633545\n",
            "Epoch 7, Batch 13, Loss: 0.7679291367530823\n",
            "Epoch 7, Batch 14, Loss: 0.9039473533630371\n",
            "Epoch 7, Batch 15, Loss: 0.5838519930839539\n",
            "Epoch 7, Batch 16, Loss: 0.8711584210395813\n",
            "Epoch 7, Batch 17, Loss: 0.7544387578964233\n",
            "Epoch 7, Batch 18, Loss: 0.5282672643661499\n",
            "Epoch 7, Batch 19, Loss: 0.5548127293586731\n",
            "Epoch 7, Batch 20, Loss: 0.7273986339569092\n",
            "Epoch 7, Batch 21, Loss: 0.4773237407207489\n",
            "Epoch 7, Batch 22, Loss: 0.6581454277038574\n",
            "Epoch 7, Batch 23, Loss: 0.6356692314147949\n",
            "Epoch 7, Batch 24, Loss: 0.888648509979248\n",
            "Epoch 7, Batch 25, Loss: 0.8402241468429565\n",
            "Epoch 7, Batch 26, Loss: 0.6096274852752686\n",
            "Epoch 7, Batch 27, Loss: 0.6019983291625977\n",
            "Epoch 7, Batch 28, Loss: 0.8238505125045776\n",
            "Epoch 7, Batch 29, Loss: 0.49015146493911743\n",
            "Epoch 7, Batch 30, Loss: 0.728760838508606\n",
            "Epoch 7, Batch 31, Loss: 0.7042598724365234\n",
            "Epoch 7, Batch 32, Loss: 0.5169963240623474\n",
            "Epoch 7, Batch 33, Loss: 0.6878283023834229\n",
            "Epoch 8, Batch 1, Loss: 0.7934420108795166\n",
            "Epoch 8, Batch 2, Loss: 0.6549995541572571\n",
            "Epoch 8, Batch 3, Loss: 0.9546608924865723\n",
            "Epoch 8, Batch 4, Loss: 0.8869094252586365\n",
            "Epoch 8, Batch 5, Loss: 0.5378046035766602\n",
            "Epoch 8, Batch 6, Loss: 0.6804196238517761\n",
            "Epoch 8, Batch 7, Loss: 0.7205269932746887\n",
            "Epoch 8, Batch 8, Loss: 0.6095229983329773\n",
            "Epoch 8, Batch 9, Loss: 0.5466676950454712\n",
            "Epoch 8, Batch 10, Loss: 0.8235265016555786\n",
            "Epoch 8, Batch 11, Loss: 0.826503574848175\n",
            "Epoch 8, Batch 12, Loss: 0.7009204626083374\n",
            "Epoch 8, Batch 13, Loss: 0.6293430328369141\n",
            "Epoch 8, Batch 14, Loss: 0.8562529683113098\n",
            "Epoch 8, Batch 15, Loss: 0.5895957946777344\n",
            "Epoch 8, Batch 16, Loss: 1.0504882335662842\n",
            "Epoch 8, Batch 17, Loss: 0.5254603624343872\n",
            "Epoch 8, Batch 18, Loss: 0.8799672722816467\n",
            "Epoch 8, Batch 19, Loss: 0.7801741361618042\n",
            "Epoch 8, Batch 20, Loss: 0.36463576555252075\n",
            "Epoch 8, Batch 21, Loss: 0.7769352197647095\n",
            "Epoch 8, Batch 22, Loss: 0.5604313611984253\n",
            "Epoch 8, Batch 23, Loss: 0.5442427396774292\n",
            "Epoch 8, Batch 24, Loss: 0.7736529111862183\n",
            "Epoch 8, Batch 25, Loss: 0.5606777667999268\n",
            "Epoch 8, Batch 26, Loss: 0.4080858826637268\n",
            "Epoch 8, Batch 27, Loss: 0.8241126537322998\n",
            "Epoch 8, Batch 28, Loss: 0.8678255081176758\n",
            "Epoch 8, Batch 29, Loss: 0.4871556758880615\n",
            "Epoch 8, Batch 30, Loss: 0.7323481440544128\n",
            "Epoch 8, Batch 31, Loss: 0.641048789024353\n",
            "Epoch 8, Batch 32, Loss: 0.6446704864501953\n",
            "Epoch 8, Batch 33, Loss: 1.4942642450332642\n",
            "Epoch 9, Batch 1, Loss: 0.8605242371559143\n",
            "Epoch 9, Batch 2, Loss: 0.9411424994468689\n",
            "Epoch 9, Batch 3, Loss: 0.5011415481567383\n",
            "Epoch 9, Batch 4, Loss: 0.6789842844009399\n",
            "Epoch 9, Batch 5, Loss: 0.7035201787948608\n",
            "Epoch 9, Batch 6, Loss: 0.634058952331543\n",
            "Epoch 9, Batch 7, Loss: 0.9542461633682251\n",
            "Epoch 9, Batch 8, Loss: 0.5255671739578247\n",
            "Epoch 9, Batch 9, Loss: 0.4891166090965271\n",
            "Epoch 9, Batch 10, Loss: 0.8029569387435913\n",
            "Epoch 9, Batch 11, Loss: 0.8401479721069336\n",
            "Epoch 9, Batch 12, Loss: 0.585292398929596\n",
            "Epoch 9, Batch 13, Loss: 0.6585338115692139\n",
            "Epoch 9, Batch 14, Loss: 0.9589316844940186\n",
            "Epoch 9, Batch 15, Loss: 0.5843690633773804\n",
            "Epoch 9, Batch 16, Loss: 0.7898844480514526\n",
            "Epoch 9, Batch 17, Loss: 0.4257984459400177\n",
            "Epoch 9, Batch 18, Loss: 0.7860890030860901\n",
            "Epoch 9, Batch 19, Loss: 0.726464033126831\n",
            "Epoch 9, Batch 20, Loss: 0.49771857261657715\n",
            "Epoch 9, Batch 21, Loss: 0.5320298075675964\n",
            "Epoch 9, Batch 22, Loss: 0.749462902545929\n",
            "Epoch 9, Batch 23, Loss: 0.9402464628219604\n",
            "Epoch 9, Batch 24, Loss: 0.6617751121520996\n",
            "Epoch 9, Batch 25, Loss: 0.7775591611862183\n",
            "Epoch 9, Batch 26, Loss: 0.5518257021903992\n",
            "Epoch 9, Batch 27, Loss: 0.8392726182937622\n",
            "Epoch 9, Batch 28, Loss: 0.5198588371276855\n",
            "Epoch 9, Batch 29, Loss: 0.6174842715263367\n",
            "Epoch 9, Batch 30, Loss: 0.8697010278701782\n",
            "Epoch 9, Batch 31, Loss: 0.5184667110443115\n",
            "Epoch 9, Batch 32, Loss: 0.8797721862792969\n",
            "Epoch 9, Batch 33, Loss: 0.9093685150146484\n",
            "Epoch 10, Batch 1, Loss: 0.6849547624588013\n",
            "Epoch 10, Batch 2, Loss: 0.5733120441436768\n",
            "Epoch 10, Batch 3, Loss: 0.7636786699295044\n",
            "Epoch 10, Batch 4, Loss: 0.8142461776733398\n",
            "Epoch 10, Batch 5, Loss: 0.6718218326568604\n",
            "Epoch 10, Batch 6, Loss: 0.6873300075531006\n",
            "Epoch 10, Batch 7, Loss: 0.9555920362472534\n",
            "Epoch 10, Batch 8, Loss: 0.6609532833099365\n",
            "Epoch 10, Batch 9, Loss: 0.8579174876213074\n",
            "Epoch 10, Batch 10, Loss: 0.6357622146606445\n",
            "Epoch 10, Batch 11, Loss: 0.7233595848083496\n",
            "Epoch 10, Batch 12, Loss: 0.633841872215271\n",
            "Epoch 10, Batch 13, Loss: 0.572744607925415\n",
            "Epoch 10, Batch 14, Loss: 0.5540505647659302\n",
            "Epoch 10, Batch 15, Loss: 0.7834814786911011\n",
            "Epoch 10, Batch 16, Loss: 0.878822386264801\n",
            "Epoch 10, Batch 17, Loss: 0.9515305161476135\n",
            "Epoch 10, Batch 18, Loss: 0.5145391821861267\n",
            "Epoch 10, Batch 19, Loss: 0.6642512679100037\n",
            "Epoch 10, Batch 20, Loss: 0.4716837406158447\n",
            "Epoch 10, Batch 21, Loss: 0.8014047741889954\n",
            "Epoch 10, Batch 22, Loss: 0.9886115193367004\n",
            "Epoch 10, Batch 23, Loss: 0.6324290037155151\n",
            "Epoch 10, Batch 24, Loss: 0.5492163300514221\n",
            "Epoch 10, Batch 25, Loss: 0.6606786251068115\n",
            "Epoch 10, Batch 26, Loss: 0.7942482829093933\n",
            "Epoch 10, Batch 27, Loss: 1.0021543502807617\n",
            "Epoch 10, Batch 28, Loss: 0.8525032997131348\n",
            "Epoch 10, Batch 29, Loss: 1.0234525203704834\n",
            "Epoch 10, Batch 30, Loss: 0.6034607887268066\n",
            "Epoch 10, Batch 31, Loss: 0.5498569011688232\n",
            "Epoch 10, Batch 32, Loss: 0.697287917137146\n",
            "Epoch 10, Batch 33, Loss: 0.21105580031871796\n",
            "Epoch 11, Batch 1, Loss: 0.8523853421211243\n",
            "Epoch 11, Batch 2, Loss: 1.1292439699172974\n",
            "Epoch 11, Batch 3, Loss: 1.010819435119629\n",
            "Epoch 11, Batch 4, Loss: 0.7221955060958862\n",
            "Epoch 11, Batch 5, Loss: 0.6588845252990723\n",
            "Epoch 11, Batch 6, Loss: 0.5713430643081665\n",
            "Epoch 11, Batch 7, Loss: 0.6926526427268982\n",
            "Epoch 11, Batch 8, Loss: 0.6693288683891296\n",
            "Epoch 11, Batch 9, Loss: 0.6823915243148804\n",
            "Epoch 11, Batch 10, Loss: 0.5873804092407227\n",
            "Epoch 11, Batch 11, Loss: 0.7534153461456299\n",
            "Epoch 11, Batch 12, Loss: 0.6326541304588318\n",
            "Epoch 11, Batch 13, Loss: 1.0270771980285645\n",
            "Epoch 11, Batch 14, Loss: 0.5519404411315918\n",
            "Epoch 11, Batch 15, Loss: 0.6761736869812012\n",
            "Epoch 11, Batch 16, Loss: 0.6643164157867432\n",
            "Epoch 11, Batch 17, Loss: 0.4218582510948181\n",
            "Epoch 11, Batch 18, Loss: 0.5616225004196167\n",
            "Epoch 11, Batch 19, Loss: 0.5784780979156494\n",
            "Epoch 11, Batch 20, Loss: 0.7080345153808594\n",
            "Epoch 11, Batch 21, Loss: 0.5429654717445374\n",
            "Epoch 11, Batch 22, Loss: 0.9352784156799316\n",
            "Epoch 11, Batch 23, Loss: 0.8736918568611145\n",
            "Epoch 11, Batch 24, Loss: 0.7067559361457825\n",
            "Epoch 11, Batch 25, Loss: 0.45486217737197876\n",
            "Epoch 11, Batch 26, Loss: 0.7067216634750366\n",
            "Epoch 11, Batch 27, Loss: 0.9159244894981384\n",
            "Epoch 11, Batch 28, Loss: 0.7722314596176147\n",
            "Epoch 11, Batch 29, Loss: 0.7476831078529358\n",
            "Epoch 11, Batch 30, Loss: 0.781051516532898\n",
            "Epoch 11, Batch 31, Loss: 0.6780826449394226\n",
            "Epoch 11, Batch 32, Loss: 0.6913645267486572\n",
            "Epoch 11, Batch 33, Loss: 0.7605957984924316\n",
            "Epoch 12, Batch 1, Loss: 0.7764201760292053\n",
            "Epoch 12, Batch 2, Loss: 0.6733362078666687\n",
            "Epoch 12, Batch 3, Loss: 0.8178589344024658\n",
            "Epoch 12, Batch 4, Loss: 0.8040653467178345\n",
            "Epoch 12, Batch 5, Loss: 0.4355800449848175\n",
            "Epoch 12, Batch 6, Loss: 0.6929783821105957\n",
            "Epoch 12, Batch 7, Loss: 0.5517407059669495\n",
            "Epoch 12, Batch 8, Loss: 0.749707818031311\n",
            "Epoch 12, Batch 9, Loss: 0.5474957823753357\n",
            "Epoch 12, Batch 10, Loss: 0.7818094491958618\n",
            "Epoch 12, Batch 11, Loss: 0.6039985418319702\n",
            "Epoch 12, Batch 12, Loss: 0.8826426267623901\n",
            "Epoch 12, Batch 13, Loss: 0.5912253856658936\n",
            "Epoch 12, Batch 14, Loss: 0.8139896392822266\n",
            "Epoch 12, Batch 15, Loss: 0.5280063152313232\n",
            "Epoch 12, Batch 16, Loss: 0.6841353178024292\n",
            "Epoch 12, Batch 17, Loss: 0.5360094308853149\n",
            "Epoch 12, Batch 18, Loss: 0.5163873434066772\n",
            "Epoch 12, Batch 19, Loss: 0.7196551561355591\n",
            "Epoch 12, Batch 20, Loss: 0.7796692252159119\n",
            "Epoch 12, Batch 21, Loss: 0.885588526725769\n",
            "Epoch 12, Batch 22, Loss: 0.8598918914794922\n",
            "Epoch 12, Batch 23, Loss: 0.7202588319778442\n",
            "Epoch 12, Batch 24, Loss: 0.7520248293876648\n",
            "Epoch 12, Batch 25, Loss: 0.8432389497756958\n",
            "Epoch 12, Batch 26, Loss: 0.6892169713973999\n",
            "Epoch 12, Batch 27, Loss: 0.5948785543441772\n",
            "Epoch 12, Batch 28, Loss: 0.49806857109069824\n",
            "Epoch 12, Batch 29, Loss: 0.8311541676521301\n",
            "Epoch 12, Batch 30, Loss: 0.9074574112892151\n",
            "Epoch 12, Batch 31, Loss: 0.6397777795791626\n",
            "Epoch 12, Batch 32, Loss: 0.630789041519165\n",
            "Epoch 12, Batch 33, Loss: 1.1702414751052856\n",
            "Epoch 13, Batch 1, Loss: 0.617009162902832\n",
            "Epoch 13, Batch 2, Loss: 0.5885823369026184\n",
            "Epoch 13, Batch 3, Loss: 0.7982748746871948\n",
            "Epoch 13, Batch 4, Loss: 0.7900835275650024\n",
            "Epoch 13, Batch 5, Loss: 0.8330748081207275\n",
            "Epoch 13, Batch 6, Loss: 0.6223639249801636\n",
            "Epoch 13, Batch 7, Loss: 0.4370320439338684\n",
            "Epoch 13, Batch 8, Loss: 0.6573156118392944\n",
            "Epoch 13, Batch 9, Loss: 0.6144684553146362\n",
            "Epoch 13, Batch 10, Loss: 0.86250239610672\n",
            "Epoch 13, Batch 11, Loss: 0.5357049107551575\n",
            "Epoch 13, Batch 12, Loss: 0.6946749687194824\n",
            "Epoch 13, Batch 13, Loss: 0.5993970036506653\n",
            "Epoch 13, Batch 14, Loss: 0.8327229619026184\n",
            "Epoch 13, Batch 15, Loss: 0.6278760433197021\n",
            "Epoch 13, Batch 16, Loss: 0.6131717562675476\n",
            "Epoch 13, Batch 17, Loss: 0.7398346066474915\n",
            "Epoch 13, Batch 18, Loss: 0.7011693120002747\n",
            "Epoch 13, Batch 19, Loss: 0.6609177589416504\n",
            "Epoch 13, Batch 20, Loss: 0.6815487742424011\n",
            "Epoch 13, Batch 21, Loss: 0.813888669013977\n",
            "Epoch 13, Batch 22, Loss: 0.8872792720794678\n",
            "Epoch 13, Batch 23, Loss: 0.5412322282791138\n",
            "Epoch 13, Batch 24, Loss: 0.44376957416534424\n",
            "Epoch 13, Batch 25, Loss: 0.6240360736846924\n",
            "Epoch 13, Batch 26, Loss: 0.8744058609008789\n",
            "Epoch 13, Batch 27, Loss: 0.9310485124588013\n",
            "Epoch 13, Batch 28, Loss: 0.7173535823822021\n",
            "Epoch 13, Batch 29, Loss: 0.7585667967796326\n",
            "Epoch 13, Batch 30, Loss: 0.9871340990066528\n",
            "Epoch 13, Batch 31, Loss: 0.8538217544555664\n",
            "Epoch 13, Batch 32, Loss: 0.5803605318069458\n",
            "Epoch 13, Batch 33, Loss: 0.7515053749084473\n",
            "Epoch 14, Batch 1, Loss: 0.7892411947250366\n",
            "Epoch 14, Batch 2, Loss: 0.5612618923187256\n",
            "Epoch 14, Batch 3, Loss: 0.7276705503463745\n",
            "Epoch 14, Batch 4, Loss: 0.7572059631347656\n",
            "Epoch 14, Batch 5, Loss: 0.8102118968963623\n",
            "Epoch 14, Batch 6, Loss: 0.8176820278167725\n",
            "Epoch 14, Batch 7, Loss: 1.0940309762954712\n",
            "Epoch 14, Batch 8, Loss: 0.6099454760551453\n",
            "Epoch 14, Batch 9, Loss: 0.3702000677585602\n",
            "Epoch 14, Batch 10, Loss: 0.6743174195289612\n",
            "Epoch 14, Batch 11, Loss: 0.5916366577148438\n",
            "Epoch 14, Batch 12, Loss: 0.9520796537399292\n",
            "Epoch 14, Batch 13, Loss: 0.7476410865783691\n",
            "Epoch 14, Batch 14, Loss: 0.47561246156692505\n",
            "Epoch 14, Batch 15, Loss: 0.6193049550056458\n",
            "Epoch 14, Batch 16, Loss: 0.47463035583496094\n",
            "Epoch 14, Batch 17, Loss: 1.0491788387298584\n",
            "Epoch 14, Batch 18, Loss: 0.5939050316810608\n",
            "Epoch 14, Batch 19, Loss: 0.4233741760253906\n",
            "Epoch 14, Batch 20, Loss: 0.7166839838027954\n",
            "Epoch 14, Batch 21, Loss: 0.8720152378082275\n",
            "Epoch 14, Batch 22, Loss: 0.701023519039154\n",
            "Epoch 14, Batch 23, Loss: 0.663726806640625\n",
            "Epoch 14, Batch 24, Loss: 0.5589711666107178\n",
            "Epoch 14, Batch 25, Loss: 0.8671301603317261\n",
            "Epoch 14, Batch 26, Loss: 0.7337361574172974\n",
            "Epoch 14, Batch 27, Loss: 0.6739979982376099\n",
            "Epoch 14, Batch 28, Loss: 0.685286283493042\n",
            "Epoch 14, Batch 29, Loss: 0.9052063822746277\n",
            "Epoch 14, Batch 30, Loss: 0.7151279449462891\n",
            "Epoch 14, Batch 31, Loss: 0.6578049659729004\n",
            "Epoch 14, Batch 32, Loss: 0.8248094320297241\n",
            "Epoch 14, Batch 33, Loss: 0.7474864721298218\n",
            "Epoch 15, Batch 1, Loss: 1.058778166770935\n",
            "Epoch 15, Batch 2, Loss: 0.5597257614135742\n",
            "Epoch 15, Batch 3, Loss: 0.6844691038131714\n",
            "Epoch 15, Batch 4, Loss: 0.6527636051177979\n",
            "Epoch 15, Batch 5, Loss: 0.744901180267334\n",
            "Epoch 15, Batch 6, Loss: 0.5942227244377136\n",
            "Epoch 15, Batch 7, Loss: 1.0074613094329834\n",
            "Epoch 15, Batch 8, Loss: 0.5944176912307739\n",
            "Epoch 15, Batch 9, Loss: 0.8515582084655762\n",
            "Epoch 15, Batch 10, Loss: 0.5727723836898804\n",
            "Epoch 15, Batch 11, Loss: 0.6786038875579834\n",
            "Epoch 15, Batch 12, Loss: 0.8121702671051025\n",
            "Epoch 15, Batch 13, Loss: 1.0183342695236206\n",
            "Epoch 15, Batch 14, Loss: 0.6271229982376099\n",
            "Epoch 15, Batch 15, Loss: 0.6155227422714233\n",
            "Epoch 15, Batch 16, Loss: 0.529333770275116\n",
            "Epoch 15, Batch 17, Loss: 0.586275041103363\n",
            "Epoch 15, Batch 18, Loss: 0.4374304711818695\n",
            "Epoch 15, Batch 19, Loss: 0.7957459092140198\n",
            "Epoch 15, Batch 20, Loss: 0.8374131321907043\n",
            "Epoch 15, Batch 21, Loss: 0.6192979216575623\n",
            "Epoch 15, Batch 22, Loss: 0.7883031368255615\n",
            "Epoch 15, Batch 23, Loss: 0.5088107585906982\n",
            "Epoch 15, Batch 24, Loss: 0.6229698657989502\n",
            "Epoch 15, Batch 25, Loss: 0.8017045259475708\n",
            "Epoch 15, Batch 26, Loss: 0.7015618085861206\n",
            "Epoch 15, Batch 27, Loss: 0.660606324672699\n",
            "Epoch 15, Batch 28, Loss: 0.4204075336456299\n",
            "Epoch 15, Batch 29, Loss: 0.6779643297195435\n",
            "Epoch 15, Batch 30, Loss: 0.732520341873169\n",
            "Epoch 15, Batch 31, Loss: 0.7935242652893066\n",
            "Epoch 15, Batch 32, Loss: 1.0990887880325317\n",
            "Epoch 15, Batch 33, Loss: 0.43357086181640625\n",
            "Epoch 16, Batch 1, Loss: 0.3542218804359436\n",
            "Epoch 16, Batch 2, Loss: 0.6595521569252014\n",
            "Epoch 16, Batch 3, Loss: 0.8084739446640015\n",
            "Epoch 16, Batch 4, Loss: 0.49408209323883057\n",
            "Epoch 16, Batch 5, Loss: 0.6601111888885498\n",
            "Epoch 16, Batch 6, Loss: 0.8900909423828125\n",
            "Epoch 16, Batch 7, Loss: 0.6921319961547852\n",
            "Epoch 16, Batch 8, Loss: 0.8783812522888184\n",
            "Epoch 16, Batch 9, Loss: 0.6624792218208313\n",
            "Epoch 16, Batch 10, Loss: 0.6577836871147156\n",
            "Epoch 16, Batch 11, Loss: 0.6276755332946777\n",
            "Epoch 16, Batch 12, Loss: 0.6682974100112915\n",
            "Epoch 16, Batch 13, Loss: 0.5296738147735596\n",
            "Epoch 16, Batch 14, Loss: 0.8750994205474854\n",
            "Epoch 16, Batch 15, Loss: 0.699402928352356\n",
            "Epoch 16, Batch 16, Loss: 0.6113929748535156\n",
            "Epoch 16, Batch 17, Loss: 0.755998969078064\n",
            "Epoch 16, Batch 18, Loss: 0.80034339427948\n",
            "Epoch 16, Batch 19, Loss: 0.6014418601989746\n",
            "Epoch 16, Batch 20, Loss: 0.639026403427124\n",
            "Epoch 16, Batch 21, Loss: 0.6972372531890869\n",
            "Epoch 16, Batch 22, Loss: 0.9250747561454773\n",
            "Epoch 16, Batch 23, Loss: 0.6719322204589844\n",
            "Epoch 16, Batch 24, Loss: 0.9170185327529907\n",
            "Epoch 16, Batch 25, Loss: 0.6251670122146606\n",
            "Epoch 16, Batch 26, Loss: 0.6274746060371399\n",
            "Epoch 16, Batch 27, Loss: 0.5415487289428711\n",
            "Epoch 16, Batch 28, Loss: 0.6415937542915344\n",
            "Epoch 16, Batch 29, Loss: 0.5677484273910522\n",
            "Epoch 16, Batch 30, Loss: 0.7827334403991699\n",
            "Epoch 16, Batch 31, Loss: 0.8933557271957397\n",
            "Epoch 16, Batch 32, Loss: 0.7065556049346924\n",
            "Epoch 16, Batch 33, Loss: 1.4081900119781494\n",
            "Epoch 17, Batch 1, Loss: 0.7156139612197876\n",
            "Epoch 17, Batch 2, Loss: 0.6738733053207397\n",
            "Epoch 17, Batch 3, Loss: 0.8303576707839966\n",
            "Epoch 17, Batch 4, Loss: 0.836394190788269\n",
            "Epoch 17, Batch 5, Loss: 0.6775816679000854\n",
            "Epoch 17, Batch 6, Loss: 0.6717753410339355\n",
            "Epoch 17, Batch 7, Loss: 0.6228986382484436\n",
            "Epoch 17, Batch 8, Loss: 0.5105725526809692\n",
            "Epoch 17, Batch 9, Loss: 0.617885172367096\n",
            "Epoch 17, Batch 10, Loss: 0.6077197790145874\n",
            "Epoch 17, Batch 11, Loss: 1.1281061172485352\n",
            "Epoch 17, Batch 12, Loss: 0.9067599773406982\n",
            "Epoch 17, Batch 13, Loss: 0.728980302810669\n",
            "Epoch 17, Batch 14, Loss: 0.5648226737976074\n",
            "Epoch 17, Batch 15, Loss: 0.9168834686279297\n",
            "Epoch 17, Batch 16, Loss: 0.531265914440155\n",
            "Epoch 17, Batch 17, Loss: 0.7641271352767944\n",
            "Epoch 17, Batch 18, Loss: 0.6564086675643921\n",
            "Epoch 17, Batch 19, Loss: 0.7118597030639648\n",
            "Epoch 17, Batch 20, Loss: 0.6271531581878662\n",
            "Epoch 17, Batch 21, Loss: 0.5317426323890686\n",
            "Epoch 17, Batch 22, Loss: 0.7934515476226807\n",
            "Epoch 17, Batch 23, Loss: 0.9403957724571228\n",
            "Epoch 17, Batch 24, Loss: 0.7286603450775146\n",
            "Epoch 17, Batch 25, Loss: 0.7690500617027283\n",
            "Epoch 17, Batch 26, Loss: 0.7611899971961975\n",
            "Epoch 17, Batch 27, Loss: 0.798136830329895\n",
            "Epoch 17, Batch 28, Loss: 0.5244874954223633\n",
            "Epoch 17, Batch 29, Loss: 0.4968219995498657\n",
            "Epoch 17, Batch 30, Loss: 0.6717497110366821\n",
            "Epoch 17, Batch 31, Loss: 0.7325242161750793\n",
            "Epoch 17, Batch 32, Loss: 0.6188833713531494\n",
            "Epoch 17, Batch 33, Loss: 0.9416904449462891\n",
            "Epoch 18, Batch 1, Loss: 0.6554924249649048\n",
            "Epoch 18, Batch 2, Loss: 0.7196710109710693\n",
            "Epoch 18, Batch 3, Loss: 0.705035924911499\n",
            "Epoch 18, Batch 4, Loss: 0.722087025642395\n",
            "Epoch 18, Batch 5, Loss: 0.522547721862793\n",
            "Epoch 18, Batch 6, Loss: 0.747279703617096\n",
            "Epoch 18, Batch 7, Loss: 0.4737418293952942\n",
            "Epoch 18, Batch 8, Loss: 0.6618988513946533\n",
            "Epoch 18, Batch 9, Loss: 0.5597077012062073\n",
            "Epoch 18, Batch 10, Loss: 0.5067920088768005\n",
            "Epoch 18, Batch 11, Loss: 0.6843010187149048\n",
            "Epoch 18, Batch 12, Loss: 0.531440019607544\n",
            "Epoch 18, Batch 13, Loss: 0.5616626739501953\n",
            "Epoch 18, Batch 14, Loss: 0.7143152952194214\n",
            "Epoch 18, Batch 15, Loss: 0.7268292903900146\n",
            "Epoch 18, Batch 16, Loss: 0.8817228078842163\n",
            "Epoch 18, Batch 17, Loss: 0.6727001070976257\n",
            "Epoch 18, Batch 18, Loss: 0.5024552941322327\n",
            "Epoch 18, Batch 19, Loss: 0.663925051689148\n",
            "Epoch 18, Batch 20, Loss: 0.627327561378479\n",
            "Epoch 18, Batch 21, Loss: 0.6948209404945374\n",
            "Epoch 18, Batch 22, Loss: 1.0734831094741821\n",
            "Epoch 18, Batch 23, Loss: 0.45567193627357483\n",
            "Epoch 18, Batch 24, Loss: 0.5622277855873108\n",
            "Epoch 18, Batch 25, Loss: 0.982863187789917\n",
            "Epoch 18, Batch 26, Loss: 1.0969762802124023\n",
            "Epoch 18, Batch 27, Loss: 0.6479995250701904\n",
            "Epoch 18, Batch 28, Loss: 0.8936713933944702\n",
            "Epoch 18, Batch 29, Loss: 0.715592622756958\n",
            "Epoch 18, Batch 30, Loss: 0.8837238550186157\n",
            "Epoch 18, Batch 31, Loss: 0.7682017683982849\n",
            "Epoch 18, Batch 32, Loss: 0.5785969495773315\n",
            "Epoch 18, Batch 33, Loss: 0.6996545195579529\n",
            "Epoch 19, Batch 1, Loss: 0.3984606862068176\n",
            "Epoch 19, Batch 2, Loss: 0.6511625051498413\n",
            "Epoch 19, Batch 3, Loss: 0.5845910310745239\n",
            "Epoch 19, Batch 4, Loss: 0.5712803602218628\n",
            "Epoch 19, Batch 5, Loss: 0.8493083715438843\n",
            "Epoch 19, Batch 6, Loss: 0.8156517744064331\n",
            "Epoch 19, Batch 7, Loss: 0.6033517122268677\n",
            "Epoch 19, Batch 8, Loss: 0.5891387462615967\n",
            "Epoch 19, Batch 9, Loss: 0.7861814498901367\n",
            "Epoch 19, Batch 10, Loss: 0.7360953092575073\n",
            "Epoch 19, Batch 11, Loss: 0.9284080266952515\n",
            "Epoch 19, Batch 12, Loss: 0.8802393674850464\n",
            "Epoch 19, Batch 13, Loss: 0.8736690878868103\n",
            "Epoch 19, Batch 14, Loss: 0.5028029680252075\n",
            "Epoch 19, Batch 15, Loss: 0.5809245705604553\n",
            "Epoch 19, Batch 16, Loss: 0.8779553174972534\n",
            "Epoch 19, Batch 17, Loss: 0.5550317764282227\n",
            "Epoch 19, Batch 18, Loss: 0.9262461066246033\n",
            "Epoch 19, Batch 19, Loss: 1.088911533355713\n",
            "Epoch 19, Batch 20, Loss: 0.7110517024993896\n",
            "Epoch 19, Batch 21, Loss: 0.6650007963180542\n",
            "Epoch 19, Batch 22, Loss: 0.4952552914619446\n",
            "Epoch 19, Batch 23, Loss: 0.7154171466827393\n",
            "Epoch 19, Batch 24, Loss: 0.845364511013031\n",
            "Epoch 19, Batch 25, Loss: 0.769978404045105\n",
            "Epoch 19, Batch 26, Loss: 0.7718106508255005\n",
            "Epoch 19, Batch 27, Loss: 0.7693333625793457\n",
            "Epoch 19, Batch 28, Loss: 0.8151477575302124\n",
            "Epoch 19, Batch 29, Loss: 0.6179333329200745\n",
            "Epoch 19, Batch 30, Loss: 0.40369054675102234\n",
            "Epoch 19, Batch 31, Loss: 0.448350191116333\n",
            "Epoch 19, Batch 32, Loss: 0.7722666263580322\n",
            "Epoch 19, Batch 33, Loss: 0.18801115453243256\n",
            "Epoch 20, Batch 1, Loss: 0.9606549739837646\n",
            "Epoch 20, Batch 2, Loss: 0.8724970817565918\n",
            "Epoch 20, Batch 3, Loss: 0.695069432258606\n",
            "Epoch 20, Batch 4, Loss: 0.8309838771820068\n",
            "Epoch 20, Batch 5, Loss: 0.841942548751831\n",
            "Epoch 20, Batch 6, Loss: 0.8968644142150879\n",
            "Epoch 20, Batch 7, Loss: 0.9225392937660217\n",
            "Epoch 20, Batch 8, Loss: 0.800121545791626\n",
            "Epoch 20, Batch 9, Loss: 0.538381814956665\n",
            "Epoch 20, Batch 10, Loss: 0.6986757516860962\n",
            "Epoch 20, Batch 11, Loss: 0.691493034362793\n",
            "Epoch 20, Batch 12, Loss: 0.5840610265731812\n",
            "Epoch 20, Batch 13, Loss: 0.6232320070266724\n",
            "Epoch 20, Batch 14, Loss: 0.6686349511146545\n",
            "Epoch 20, Batch 15, Loss: 1.211145043373108\n",
            "Epoch 20, Batch 16, Loss: 0.3607833981513977\n",
            "Epoch 20, Batch 17, Loss: 0.5846449136734009\n",
            "Epoch 20, Batch 18, Loss: 0.5625982880592346\n",
            "Epoch 20, Batch 19, Loss: 0.6247165203094482\n",
            "Epoch 20, Batch 20, Loss: 0.5919904708862305\n",
            "Epoch 20, Batch 21, Loss: 0.7292484641075134\n",
            "Epoch 20, Batch 22, Loss: 0.6179642081260681\n",
            "Epoch 20, Batch 23, Loss: 0.6402645111083984\n",
            "Epoch 20, Batch 24, Loss: 0.8060500025749207\n",
            "Epoch 20, Batch 25, Loss: 0.5303765535354614\n",
            "Epoch 20, Batch 26, Loss: 0.658410906791687\n",
            "Epoch 20, Batch 27, Loss: 0.7284244298934937\n",
            "Epoch 20, Batch 28, Loss: 0.7375160455703735\n",
            "Epoch 20, Batch 29, Loss: 0.8186656832695007\n",
            "Epoch 20, Batch 30, Loss: 0.7464017868041992\n",
            "Epoch 20, Batch 31, Loss: 0.8023422360420227\n",
            "Epoch 20, Batch 32, Loss: 0.7082218527793884\n",
            "Epoch 20, Batch 33, Loss: 0.5354596376419067\n",
            "Epoch 1, Batch 1, Loss: 0.5089679956436157\n",
            "Epoch 1, Batch 2, Loss: 2.77596378326416\n",
            "Epoch 1, Batch 3, Loss: 0.3877628445625305\n",
            "Epoch 1, Batch 4, Loss: 1.314202070236206\n",
            "Epoch 1, Batch 5, Loss: 0.7452021837234497\n",
            "Epoch 1, Batch 6, Loss: 1.10542631149292\n",
            "Epoch 1, Batch 7, Loss: 0.7525902986526489\n",
            "Epoch 1, Batch 8, Loss: 0.7340208888053894\n",
            "Epoch 1, Batch 9, Loss: 1.0484206676483154\n",
            "Epoch 2, Batch 1, Loss: 1.5116205215454102\n",
            "Epoch 2, Batch 2, Loss: 1.2414460182189941\n",
            "Epoch 2, Batch 3, Loss: 0.41810929775238037\n",
            "Epoch 2, Batch 4, Loss: 1.0627729892730713\n",
            "Epoch 2, Batch 5, Loss: 0.5846824645996094\n",
            "Epoch 2, Batch 6, Loss: 1.0378963947296143\n",
            "Epoch 2, Batch 7, Loss: 0.8143744468688965\n",
            "Epoch 2, Batch 8, Loss: 0.8868983387947083\n",
            "Epoch 2, Batch 9, Loss: 1.2160840034484863\n",
            "Epoch 3, Batch 1, Loss: 0.5175129175186157\n",
            "Epoch 3, Batch 2, Loss: 0.9111311435699463\n",
            "Epoch 3, Batch 3, Loss: 0.4071083962917328\n",
            "Epoch 3, Batch 4, Loss: 1.2786681652069092\n",
            "Epoch 3, Batch 5, Loss: 0.8799059391021729\n",
            "Epoch 3, Batch 6, Loss: 0.7573525905609131\n",
            "Epoch 3, Batch 7, Loss: 0.664585530757904\n",
            "Epoch 3, Batch 8, Loss: 0.725864052772522\n",
            "Epoch 3, Batch 9, Loss: 0.8706725835800171\n",
            "Epoch 4, Batch 1, Loss: 0.5145654678344727\n",
            "Epoch 4, Batch 2, Loss: 0.8640598654747009\n",
            "Epoch 4, Batch 3, Loss: 0.5498558878898621\n",
            "Epoch 4, Batch 4, Loss: 1.0180189609527588\n",
            "Epoch 4, Batch 5, Loss: 0.5216032266616821\n",
            "Epoch 4, Batch 6, Loss: 0.6715063452720642\n",
            "Epoch 4, Batch 7, Loss: 0.5960009098052979\n",
            "Epoch 4, Batch 8, Loss: 0.7149261236190796\n",
            "Epoch 4, Batch 9, Loss: 0.509999692440033\n",
            "Epoch 5, Batch 1, Loss: 0.5829542875289917\n",
            "Epoch 5, Batch 2, Loss: 0.8950968980789185\n",
            "Epoch 5, Batch 3, Loss: 0.37097662687301636\n",
            "Epoch 5, Batch 4, Loss: 1.0511025190353394\n",
            "Epoch 5, Batch 5, Loss: 0.6468689441680908\n",
            "Epoch 5, Batch 6, Loss: 0.6719644069671631\n",
            "Epoch 5, Batch 7, Loss: 0.5655282735824585\n",
            "Epoch 5, Batch 8, Loss: 0.7133322954177856\n",
            "Epoch 5, Batch 9, Loss: 0.5362476706504822\n",
            "Epoch 6, Batch 1, Loss: 0.5235711336135864\n",
            "Epoch 6, Batch 2, Loss: 0.8444868922233582\n",
            "Epoch 6, Batch 3, Loss: 0.41543546319007874\n",
            "Epoch 6, Batch 4, Loss: 0.9403831958770752\n",
            "Epoch 6, Batch 5, Loss: 0.5512263774871826\n",
            "Epoch 6, Batch 6, Loss: 0.6740533709526062\n",
            "Epoch 6, Batch 7, Loss: 0.5819598436355591\n",
            "Epoch 6, Batch 8, Loss: 0.7020086050033569\n",
            "Epoch 6, Batch 9, Loss: 0.2867438793182373\n",
            "Epoch 7, Batch 1, Loss: 0.5630887150764465\n",
            "Epoch 7, Batch 2, Loss: 0.8487001657485962\n",
            "Epoch 7, Batch 3, Loss: 0.36774563789367676\n",
            "Epoch 7, Batch 4, Loss: 0.9333852529525757\n",
            "Epoch 7, Batch 5, Loss: 0.5561580061912537\n",
            "Epoch 7, Batch 6, Loss: 0.6376462578773499\n",
            "Epoch 7, Batch 7, Loss: 0.5579620599746704\n",
            "Epoch 7, Batch 8, Loss: 0.6696176528930664\n",
            "Epoch 7, Batch 9, Loss: 0.44722849130630493\n",
            "Epoch 8, Batch 1, Loss: 0.5426452159881592\n",
            "Epoch 8, Batch 2, Loss: 0.8566626310348511\n",
            "Epoch 8, Batch 3, Loss: 0.37129560112953186\n",
            "Epoch 8, Batch 4, Loss: 0.9635651111602783\n",
            "Epoch 8, Batch 5, Loss: 0.6197545528411865\n",
            "Epoch 8, Batch 6, Loss: 0.6317557096481323\n",
            "Epoch 8, Batch 7, Loss: 0.5377391576766968\n",
            "Epoch 8, Batch 8, Loss: 0.6433219909667969\n",
            "Epoch 8, Batch 9, Loss: 0.013489593751728535\n",
            "Epoch 9, Batch 1, Loss: 0.548533022403717\n",
            "Epoch 9, Batch 2, Loss: 0.8644095659255981\n",
            "Epoch 9, Batch 3, Loss: 0.4793025553226471\n",
            "Epoch 9, Batch 4, Loss: 0.948002278804779\n",
            "Epoch 9, Batch 5, Loss: 0.50571608543396\n",
            "Epoch 9, Batch 6, Loss: 0.6400681734085083\n",
            "Epoch 9, Batch 7, Loss: 0.49110889434814453\n",
            "Epoch 9, Batch 8, Loss: 0.6719504594802856\n",
            "Epoch 9, Batch 9, Loss: 0.02224959060549736\n",
            "Epoch 10, Batch 1, Loss: 0.5634545087814331\n",
            "Epoch 10, Batch 2, Loss: 0.804872989654541\n",
            "Epoch 10, Batch 3, Loss: 0.34099727869033813\n",
            "Epoch 10, Batch 4, Loss: 0.844489574432373\n",
            "Epoch 10, Batch 5, Loss: 0.5350393056869507\n",
            "Epoch 10, Batch 6, Loss: 0.5943790674209595\n",
            "Epoch 10, Batch 7, Loss: 0.5192370414733887\n",
            "Epoch 10, Batch 8, Loss: 0.6934727430343628\n",
            "Epoch 10, Batch 9, Loss: 0.0022578418720513582\n",
            "Epoch 11, Batch 1, Loss: 0.5708681344985962\n",
            "Epoch 11, Batch 2, Loss: 0.8010560274124146\n",
            "Epoch 11, Batch 3, Loss: 0.37021496891975403\n",
            "Epoch 11, Batch 4, Loss: 0.8445727229118347\n",
            "Epoch 11, Batch 5, Loss: 0.6096445322036743\n",
            "Epoch 11, Batch 6, Loss: 0.5174999237060547\n",
            "Epoch 11, Batch 7, Loss: 0.496886670589447\n",
            "Epoch 11, Batch 8, Loss: 0.688248872756958\n",
            "Epoch 11, Batch 9, Loss: 0.15729710459709167\n",
            "Epoch 12, Batch 1, Loss: 0.5275267362594604\n",
            "Epoch 12, Batch 2, Loss: 0.7832453846931458\n",
            "Epoch 12, Batch 3, Loss: 0.29796427488327026\n",
            "Epoch 12, Batch 4, Loss: 0.8956248760223389\n",
            "Epoch 12, Batch 5, Loss: 0.577984094619751\n",
            "Epoch 12, Batch 6, Loss: 0.4705882668495178\n",
            "Epoch 12, Batch 7, Loss: 0.43795841932296753\n",
            "Epoch 12, Batch 8, Loss: 0.6499544382095337\n",
            "Epoch 12, Batch 9, Loss: 0.24792855978012085\n",
            "Epoch 13, Batch 1, Loss: 0.5607205629348755\n",
            "Epoch 13, Batch 2, Loss: 0.8549821972846985\n",
            "Epoch 13, Batch 3, Loss: 0.5122827291488647\n",
            "Epoch 13, Batch 4, Loss: 0.9657858610153198\n",
            "Epoch 13, Batch 5, Loss: 0.507635235786438\n",
            "Epoch 13, Batch 6, Loss: 0.5357154607772827\n",
            "Epoch 13, Batch 7, Loss: 0.47628480195999146\n",
            "Epoch 13, Batch 8, Loss: 0.7800326347351074\n",
            "Epoch 13, Batch 9, Loss: 0.03582753613591194\n",
            "Epoch 14, Batch 1, Loss: 0.5753210783004761\n",
            "Epoch 14, Batch 2, Loss: 0.8637261390686035\n",
            "Epoch 14, Batch 3, Loss: 0.31751886010169983\n",
            "Epoch 14, Batch 4, Loss: 0.8414573669433594\n",
            "Epoch 14, Batch 5, Loss: 0.5253052711486816\n",
            "Epoch 14, Batch 6, Loss: 0.6178220510482788\n",
            "Epoch 14, Batch 7, Loss: 0.5059272050857544\n",
            "Epoch 14, Batch 8, Loss: 0.681489109992981\n",
            "Epoch 14, Batch 9, Loss: 0.14822857081890106\n",
            "Epoch 15, Batch 1, Loss: 0.519134521484375\n",
            "Epoch 15, Batch 2, Loss: 0.8284633159637451\n",
            "Epoch 15, Batch 3, Loss: 0.3344050645828247\n",
            "Epoch 15, Batch 4, Loss: 0.8116200566291809\n",
            "Epoch 15, Batch 5, Loss: 0.5865948796272278\n",
            "Epoch 15, Batch 6, Loss: 0.4796849191188812\n",
            "Epoch 15, Batch 7, Loss: 0.46486353874206543\n",
            "Epoch 15, Batch 8, Loss: 0.6839037537574768\n",
            "Epoch 15, Batch 9, Loss: 0.14842763543128967\n",
            "Epoch 16, Batch 1, Loss: 0.5556343793869019\n",
            "Epoch 16, Batch 2, Loss: 0.7249415516853333\n",
            "Epoch 16, Batch 3, Loss: 0.3432019352912903\n",
            "Epoch 16, Batch 4, Loss: 0.8387333154678345\n",
            "Epoch 16, Batch 5, Loss: 0.5497909784317017\n",
            "Epoch 16, Batch 6, Loss: 0.47392141819000244\n",
            "Epoch 16, Batch 7, Loss: 0.42795509099960327\n",
            "Epoch 16, Batch 8, Loss: 0.6516401767730713\n",
            "Epoch 16, Batch 9, Loss: 0.07388253509998322\n",
            "Epoch 17, Batch 1, Loss: 0.48292481899261475\n",
            "Epoch 17, Batch 2, Loss: 0.7976583242416382\n",
            "Epoch 17, Batch 3, Loss: 0.38974541425704956\n",
            "Epoch 17, Batch 4, Loss: 0.7887711524963379\n",
            "Epoch 17, Batch 5, Loss: 0.5396307110786438\n",
            "Epoch 17, Batch 6, Loss: 0.49230343103408813\n",
            "Epoch 17, Batch 7, Loss: 0.44266581535339355\n",
            "Epoch 17, Batch 8, Loss: 0.6542454957962036\n",
            "Epoch 17, Batch 9, Loss: 0.1786719262599945\n",
            "Epoch 18, Batch 1, Loss: 0.4412928819656372\n",
            "Epoch 18, Batch 2, Loss: 0.7170898914337158\n",
            "Epoch 18, Batch 3, Loss: 0.4083293676376343\n",
            "Epoch 18, Batch 4, Loss: 0.912980854511261\n",
            "Epoch 18, Batch 5, Loss: 0.5625234842300415\n",
            "Epoch 18, Batch 6, Loss: 0.4958793520927429\n",
            "Epoch 18, Batch 7, Loss: 0.4238004982471466\n",
            "Epoch 18, Batch 8, Loss: 0.6607760190963745\n",
            "Epoch 18, Batch 9, Loss: 0.07220295071601868\n",
            "Epoch 19, Batch 1, Loss: 0.5300840735435486\n",
            "Epoch 19, Batch 2, Loss: 0.756870448589325\n",
            "Epoch 19, Batch 3, Loss: 0.3197636008262634\n",
            "Epoch 19, Batch 4, Loss: 0.8352459669113159\n",
            "Epoch 19, Batch 5, Loss: 0.5175119638442993\n",
            "Epoch 19, Batch 6, Loss: 0.5899591445922852\n",
            "Epoch 19, Batch 7, Loss: 0.46660351753234863\n",
            "Epoch 19, Batch 8, Loss: 0.6275776624679565\n",
            "Epoch 19, Batch 9, Loss: 0.008735527284443378\n",
            "Epoch 20, Batch 1, Loss: 0.5467557907104492\n",
            "Epoch 20, Batch 2, Loss: 0.8209167718887329\n",
            "Epoch 20, Batch 3, Loss: 0.44880422949790955\n",
            "Epoch 20, Batch 4, Loss: 0.7943763732910156\n",
            "Epoch 20, Batch 5, Loss: 0.5623517036437988\n",
            "Epoch 20, Batch 6, Loss: 0.4966142177581787\n",
            "Epoch 20, Batch 7, Loss: 0.4794628322124481\n",
            "Epoch 20, Batch 8, Loss: 0.6759844422340393\n",
            "Epoch 20, Batch 9, Loss: 0.10757268220186234\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 700x300 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# train and eval the model\n",
        "\n",
        "model, r2list, rmselist = train_model(LSTM_NeuralModel, dataset, LSTM_Config(), 20)\n",
        "r2list_eval, rmselist_eval = eval_model(model, LSTM_Config())\n",
        "plot_training_results(r2list, rmselist, r2list_eval, rmselist_eval)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QvJz3cPPqJEX"
      },
      "outputs": [],
      "source": [
        "# baseline lstm model result\n",
        "\n",
        "def test_model(model, dataset, config):\n",
        "    test_data = dataset.get_subset('test')\n",
        "    test_loader = get_eval_loader('standard', test_data, batch_size=config.B)\n",
        "\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    for batch in test_loader:\n",
        "        x_batch, y_batch = batch\n",
        "        predictions = model(x_batch, training=False)\n",
        "        y_true.extend(y_batch.numpy().flatten())\n",
        "        y_pred.extend(predictions.numpy().flatten())\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    return r2, rmse\n",
        "\n",
        "r2, rmse = test_model(model, dataset, LSTM_Config())\n",
        "print(f'R¬≤: {r2:.4f}')\n",
        "print(f'RMSE: {rmse:.4f}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6Gb75q34hX3"
      },
      "source": [
        "## Fine-tunning the lstm model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1rz61jyBHwA"
      },
      "outputs": [],
      "source": [
        "# update the __init__ to update the hyper-parameters\n",
        "\n",
        "class LSTM_Config():\n",
        "    B, W, C = 32,32,9\n",
        "    H = 32 #all season lengths will be 32\n",
        "    loss_lambda = 0.75\n",
        "    lstm_layers = 1\n",
        "    lstm_H = 200\n",
        "    dense = 356\n",
        "    season_len = 32\n",
        "\n",
        "    train_step = 10000000\n",
        "    lr = 0.005\n",
        "    #keep probability\n",
        "    drop_out = 0.75\n",
        "\n",
        "    # update the hyper-parameters for grid-search\n",
        "    def __init__(self, season_frac=None, lstm_H=None, drop_out=None, lr=None): # update\n",
        "        if season_frac is not None:\n",
        "            self.H = int(season_frac *self.H)\n",
        "        if lstm_H is not None:\n",
        "            self.lstm_H = lstm_H\n",
        "        if drop_out is not None:\n",
        "            self.drop_out = drop_out\n",
        "        if lr is not None:\n",
        "            self.lr = lr\n",
        "\n",
        "def dense(input_data, H, N=None, name = \"dense\"):\n",
        "    if not N:\n",
        "        N = input_data.get_shape()[-1]\n",
        "    with tf.variable_scope(name):\n",
        "        W = tf.get_variable(\"W\", [N, H], initializer=tf.contrib.layers.variance_scaling_initializer())\n",
        "        b = tf.get_variable(\"b\", [1, H])\n",
        "        tf.summary.histogram(name + \".W\", W)\n",
        "        tf.summary.histogram(name + \".b\", b)\n",
        "        return tf.matmul(input_data, W, name=\"matmul\") + b\n",
        "\n",
        "def lstm_net(input_data,output_data,config,keep_prob = 1,name=\"lstm_net\"):\n",
        "    with tf.variable_scope(name):\n",
        "        lstm_cell = tf.nn.rnn_cell.LSTMCell(config.lstm_H,state_is_tuple=True)\n",
        "        lstm_cell = tf.nn.rnn_cell.DropoutWrapper(lstm_cell, output_keep_prob=keep_prob)\n",
        "        cell = tf.nn.rnn_cell.MultiRNNCell([lstm_cell] * config.lstm_layers,state_is_tuple=True)\n",
        "        state = cell.zero_state(config.B, tf.float32)\n",
        "        outputs, final_state = tf.nn.dynamic_rnn(cell, input_data,\n",
        "                       initial_state=state, time_major=True)\n",
        "        tf.summary.histogram(name + '.outputs', outputs)\n",
        "        output_final = tf.squeeze(tf.slice(outputs, [config.H-1,0,0] , [1,-1,-1]))\n",
        "        tf.summary.histogram(name + '.output_final', output_final)\n",
        "        fc1 = dense(output_final, config.dense, name=\"dense\")\n",
        "\n",
        "        logit = tf.squeeze(dense(fc1,1,name='logit'))\n",
        "        tf.summary.histogram(name + '.logit', logit)\n",
        "        loss_err = tf.nn.l2_loss(logit - output_data)\n",
        "        loss_reg = tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables()])\n",
        "        total_loss = config.loss_lambda * loss_err + (1 - config.loss_lambda) * loss_reg\n",
        "\n",
        "        tf.summary.scalar(name + '.loss_err', loss_err)\n",
        "        tf.summary.scalar(name + '.loss_reg', loss_reg)\n",
        "        tf.summary.scalar(name + '.loss_total', total_loss)\n",
        "\n",
        "        return logit,total_loss,fc1\n",
        "\n",
        "class LSTM_NeuralModel(tf.keras.Model):\n",
        "    def __init__(self, config, name=\"LSTM_Model\"):\n",
        "        super(LSTM_NeuralModel, self).__init__(name=name)\n",
        "        self.config = config\n",
        "        self.lstm = tf.keras.layers.LSTM(\n",
        "            units=config.lstm_H,\n",
        "            return_sequences=False,\n",
        "            return_state=False,\n",
        "            dropout=1 - config.drop_out,\n",
        "        )\n",
        "        self.dense1 = tf.keras.layers.Dense(config.dense, activation=\"relu\")\n",
        "        self.logit = tf.keras.layers.Dense(1, activation=None)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        x = tf.transpose(inputs, perm=[2, 0, 1, 3])\n",
        "        x = tf.reshape(x, [-1, self.config.W, self.config.H * self.config.C])\n",
        "\n",
        "        # LSTM layer\n",
        "        x = self.lstm(x, training=training)\n",
        "\n",
        "        # Dense layer\n",
        "        x = self.dense1(x)\n",
        "        output = self.logit(x)\n",
        "\n",
        "        return output\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3FU5eTX3hqc",
        "outputId": "f684f8e9-50d9-4d10-bfab-b0493672c4d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training with dropout=0.75, lr=0.1, lstm_H=200\n",
            "Epoch 1, Batch 1, Loss: 7.7069549560546875\n",
            "Epoch 1, Batch 2, Loss: 38538.1015625\n",
            "Epoch 1, Batch 3, Loss: 2786.58740234375\n",
            "Epoch 1, Batch 4, Loss: 14.428361892700195\n",
            "Epoch 1, Batch 5, Loss: 39.066932678222656\n",
            "Epoch 1, Batch 6, Loss: 337.958740234375\n",
            "Epoch 1, Batch 7, Loss: 10.643880844116211\n",
            "Epoch 1, Batch 8, Loss: 10.359560012817383\n",
            "Epoch 1, Batch 9, Loss: 10.487308502197266\n",
            "Epoch 1, Batch 10, Loss: 6.204424858093262\n",
            "Epoch 1, Batch 11, Loss: 3.012660026550293\n",
            "Epoch 1, Batch 12, Loss: 0.8946355581283569\n",
            "Epoch 1, Batch 13, Loss: 4.892642021179199\n",
            "Epoch 1, Batch 14, Loss: 2.6870572566986084\n",
            "Epoch 1, Batch 15, Loss: 1.1602532863616943\n",
            "Epoch 1, Batch 16, Loss: 0.939075767993927\n",
            "Epoch 1, Batch 17, Loss: 1.297715663909912\n",
            "Epoch 1, Batch 18, Loss: 1.8347899913787842\n",
            "Epoch 1, Batch 19, Loss: 2.895655632019043\n",
            "Epoch 1, Batch 20, Loss: 2.8737807273864746\n",
            "Epoch 1, Batch 21, Loss: 1.9828583002090454\n",
            "Epoch 1, Batch 22, Loss: 0.7930763959884644\n",
            "Epoch 1, Batch 23, Loss: 0.8374148011207581\n",
            "Epoch 1, Batch 24, Loss: 1.8682447671890259\n",
            "Epoch 1, Batch 25, Loss: 2.374563217163086\n",
            "Epoch 1, Batch 26, Loss: 1.2350281476974487\n",
            "Epoch 1, Batch 27, Loss: 2.0846424102783203\n",
            "Epoch 1, Batch 28, Loss: 1.9200596809387207\n",
            "Epoch 1, Batch 29, Loss: 0.9304431676864624\n",
            "Epoch 1, Batch 30, Loss: 1.4877619743347168\n",
            "Epoch 1, Batch 31, Loss: 1.5164704322814941\n",
            "Epoch 1, Batch 32, Loss: 0.7931114435195923\n",
            "Epoch 1, Batch 33, Loss: 0.6830270886421204\n",
            "Epoch 2, Batch 1, Loss: 2.0400266647338867\n",
            "Epoch 2, Batch 2, Loss: 3.034939765930176\n",
            "Epoch 2, Batch 3, Loss: 1.3545551300048828\n",
            "Epoch 2, Batch 4, Loss: 1.0928990840911865\n",
            "Epoch 2, Batch 5, Loss: 1.399086594581604\n",
            "Epoch 2, Batch 6, Loss: 2.66851806640625\n",
            "Epoch 2, Batch 7, Loss: 1.3916049003601074\n",
            "Epoch 2, Batch 8, Loss: 1.7604259252548218\n",
            "Epoch 2, Batch 9, Loss: 0.8382112979888916\n",
            "Epoch 2, Batch 10, Loss: 1.0838632583618164\n",
            "Epoch 2, Batch 11, Loss: 1.1947581768035889\n",
            "Epoch 2, Batch 12, Loss: 1.6357002258300781\n",
            "Epoch 2, Batch 13, Loss: 0.9745117425918579\n",
            "Epoch 2, Batch 14, Loss: 0.7422033548355103\n",
            "Epoch 2, Batch 15, Loss: 0.9222357869148254\n",
            "Epoch 2, Batch 16, Loss: 1.6682262420654297\n",
            "Epoch 2, Batch 17, Loss: 0.9709093570709229\n",
            "Epoch 2, Batch 18, Loss: 0.8020514249801636\n",
            "Epoch 2, Batch 19, Loss: 0.6477058529853821\n",
            "Epoch 2, Batch 20, Loss: 1.0322818756103516\n",
            "Epoch 2, Batch 21, Loss: 0.6391944885253906\n",
            "Epoch 2, Batch 22, Loss: 1.1467769145965576\n",
            "Epoch 2, Batch 23, Loss: 0.959044337272644\n",
            "Epoch 2, Batch 24, Loss: 0.7184493541717529\n",
            "Epoch 2, Batch 25, Loss: 0.7279741764068604\n",
            "Epoch 2, Batch 26, Loss: 0.8759151101112366\n",
            "Epoch 2, Batch 27, Loss: 1.1009743213653564\n",
            "Epoch 2, Batch 28, Loss: 0.5556058883666992\n",
            "Epoch 2, Batch 29, Loss: 0.7773145437240601\n",
            "Epoch 2, Batch 30, Loss: 0.7438251972198486\n",
            "Epoch 2, Batch 31, Loss: 0.8277825117111206\n",
            "Epoch 2, Batch 32, Loss: 0.36551010608673096\n",
            "Epoch 2, Batch 33, Loss: 1.0460383892059326\n",
            "Epoch 3, Batch 1, Loss: 1.2022082805633545\n",
            "Epoch 3, Batch 2, Loss: 1.0861480236053467\n",
            "Epoch 3, Batch 3, Loss: 1.0975964069366455\n",
            "Epoch 3, Batch 4, Loss: 1.072324275970459\n",
            "Epoch 3, Batch 5, Loss: 0.7644917964935303\n",
            "Epoch 3, Batch 6, Loss: 0.8917564749717712\n",
            "Epoch 3, Batch 7, Loss: 1.3100080490112305\n",
            "Epoch 3, Batch 8, Loss: 0.3371250033378601\n",
            "Epoch 3, Batch 9, Loss: 1.3306456804275513\n",
            "Epoch 3, Batch 10, Loss: 0.791417121887207\n",
            "Epoch 3, Batch 11, Loss: 0.6350695490837097\n",
            "Epoch 3, Batch 12, Loss: 1.1503396034240723\n",
            "Epoch 3, Batch 13, Loss: 1.3358291387557983\n",
            "Epoch 3, Batch 14, Loss: 1.2895146608352661\n",
            "Epoch 3, Batch 15, Loss: 1.196281909942627\n",
            "Epoch 3, Batch 16, Loss: 0.9633608460426331\n",
            "Epoch 3, Batch 17, Loss: 1.1745952367782593\n",
            "Epoch 3, Batch 18, Loss: 1.1603423357009888\n",
            "Epoch 3, Batch 19, Loss: 0.6066577434539795\n",
            "Epoch 3, Batch 20, Loss: 0.6972427368164062\n",
            "Epoch 3, Batch 21, Loss: 0.71296226978302\n",
            "Epoch 3, Batch 22, Loss: 0.8370264172554016\n",
            "Epoch 3, Batch 23, Loss: 0.866714358329773\n",
            "Epoch 3, Batch 24, Loss: 0.6779464483261108\n",
            "Epoch 3, Batch 25, Loss: 0.41526490449905396\n",
            "Epoch 3, Batch 26, Loss: 0.6026160717010498\n",
            "Epoch 3, Batch 27, Loss: 0.8283612728118896\n",
            "Epoch 3, Batch 28, Loss: 0.5928239822387695\n",
            "Epoch 3, Batch 29, Loss: 0.7014910578727722\n",
            "Epoch 3, Batch 30, Loss: 0.6092255115509033\n",
            "Epoch 3, Batch 31, Loss: 0.8372339010238647\n",
            "Epoch 3, Batch 32, Loss: 0.6669410467147827\n",
            "Epoch 3, Batch 33, Loss: 0.5682342052459717\n",
            "Epoch 4, Batch 1, Loss: 0.8840067386627197\n",
            "Epoch 4, Batch 2, Loss: 0.7439419031143188\n",
            "Epoch 4, Batch 3, Loss: 0.70921790599823\n",
            "Epoch 4, Batch 4, Loss: 0.558716893196106\n",
            "Epoch 4, Batch 5, Loss: 0.7221115827560425\n",
            "Epoch 4, Batch 6, Loss: 0.6365500688552856\n",
            "Epoch 4, Batch 7, Loss: 0.6672911047935486\n",
            "Epoch 4, Batch 8, Loss: 0.6638245582580566\n",
            "Epoch 4, Batch 9, Loss: 0.8707637190818787\n",
            "Epoch 4, Batch 10, Loss: 0.5829078555107117\n",
            "Epoch 4, Batch 11, Loss: 0.749033510684967\n",
            "Epoch 4, Batch 12, Loss: 0.9175416231155396\n",
            "Epoch 4, Batch 13, Loss: 0.8836349844932556\n",
            "Epoch 4, Batch 14, Loss: 0.7657551765441895\n",
            "Epoch 4, Batch 15, Loss: 0.9029940366744995\n",
            "Epoch 4, Batch 16, Loss: 1.1210675239562988\n",
            "Epoch 4, Batch 17, Loss: 1.1201539039611816\n",
            "Epoch 4, Batch 18, Loss: 0.6348667740821838\n",
            "Epoch 4, Batch 19, Loss: 1.105506181716919\n",
            "Epoch 4, Batch 20, Loss: 1.433729648590088\n",
            "Epoch 4, Batch 21, Loss: 0.8036858439445496\n",
            "Epoch 4, Batch 22, Loss: 0.9444065093994141\n",
            "Epoch 4, Batch 23, Loss: 0.8565888404846191\n",
            "Epoch 4, Batch 24, Loss: 1.2393689155578613\n",
            "Epoch 4, Batch 25, Loss: 0.9528132081031799\n",
            "Epoch 4, Batch 26, Loss: 0.7366759777069092\n",
            "Epoch 4, Batch 27, Loss: 0.8310302495956421\n",
            "Epoch 4, Batch 28, Loss: 1.1806937456130981\n",
            "Epoch 4, Batch 29, Loss: 1.000572919845581\n",
            "Epoch 4, Batch 30, Loss: 0.9005377292633057\n",
            "Epoch 4, Batch 31, Loss: 0.618622362613678\n",
            "Epoch 4, Batch 32, Loss: 0.7230803966522217\n",
            "Epoch 4, Batch 33, Loss: 1.1513317823410034\n",
            "Epoch 5, Batch 1, Loss: 0.6251752376556396\n",
            "Epoch 5, Batch 2, Loss: 0.5664420127868652\n",
            "Epoch 5, Batch 3, Loss: 0.5736579298973083\n",
            "Epoch 5, Batch 4, Loss: 0.7633258700370789\n",
            "Epoch 5, Batch 5, Loss: 0.8378894329071045\n",
            "Epoch 5, Batch 6, Loss: 0.6718010902404785\n",
            "Epoch 5, Batch 7, Loss: 0.639519453048706\n",
            "Epoch 5, Batch 8, Loss: 0.9738608002662659\n",
            "Epoch 5, Batch 9, Loss: 0.6378629803657532\n",
            "Epoch 5, Batch 10, Loss: 0.7840462923049927\n",
            "Epoch 5, Batch 11, Loss: 1.2261509895324707\n",
            "Epoch 5, Batch 12, Loss: 0.6336669921875\n",
            "Epoch 5, Batch 13, Loss: 0.5734524726867676\n",
            "Epoch 5, Batch 14, Loss: 0.8720316886901855\n",
            "Epoch 5, Batch 15, Loss: 0.7899438142776489\n",
            "Epoch 5, Batch 16, Loss: 1.0958425998687744\n",
            "Epoch 5, Batch 17, Loss: 0.8911795616149902\n",
            "Epoch 5, Batch 18, Loss: 0.6805981397628784\n",
            "Epoch 5, Batch 19, Loss: 0.9417484998703003\n",
            "Epoch 5, Batch 20, Loss: 0.8890048265457153\n",
            "Epoch 5, Batch 21, Loss: 0.7051171064376831\n",
            "Epoch 5, Batch 22, Loss: 0.8318237066268921\n",
            "Epoch 5, Batch 23, Loss: 0.5605604648590088\n",
            "Epoch 5, Batch 24, Loss: 0.5650684833526611\n",
            "Epoch 5, Batch 25, Loss: 0.24890129268169403\n",
            "Epoch 5, Batch 26, Loss: 0.5974540710449219\n",
            "Epoch 5, Batch 27, Loss: 0.844577968120575\n",
            "Epoch 5, Batch 28, Loss: 0.603570818901062\n",
            "Epoch 5, Batch 29, Loss: 0.503778874874115\n",
            "Epoch 5, Batch 30, Loss: 0.9855095148086548\n",
            "Epoch 5, Batch 31, Loss: 0.5702374577522278\n",
            "Epoch 5, Batch 32, Loss: 1.1060997247695923\n",
            "Epoch 5, Batch 33, Loss: 0.2490297257900238\n",
            "Epoch 6, Batch 1, Loss: 0.8434510231018066\n",
            "Epoch 6, Batch 2, Loss: 0.504665732383728\n",
            "Epoch 6, Batch 3, Loss: 0.745647668838501\n",
            "Epoch 6, Batch 4, Loss: 0.865249752998352\n",
            "Epoch 6, Batch 5, Loss: 0.933732271194458\n",
            "Epoch 6, Batch 6, Loss: 1.0503287315368652\n",
            "Epoch 6, Batch 7, Loss: 0.6770059466362\n",
            "Epoch 6, Batch 8, Loss: 0.7760497331619263\n",
            "Epoch 6, Batch 9, Loss: 0.7403780221939087\n",
            "Epoch 6, Batch 10, Loss: 0.77302485704422\n",
            "Epoch 6, Batch 11, Loss: 0.6842918395996094\n",
            "Epoch 6, Batch 12, Loss: 0.6994127035140991\n",
            "Epoch 6, Batch 13, Loss: 0.819735586643219\n",
            "Epoch 6, Batch 14, Loss: 0.9798994660377502\n",
            "Epoch 6, Batch 15, Loss: 0.7388613224029541\n",
            "Epoch 6, Batch 16, Loss: 0.75078284740448\n",
            "Epoch 6, Batch 17, Loss: 0.6284419894218445\n",
            "Epoch 6, Batch 18, Loss: 0.8877328634262085\n",
            "Epoch 6, Batch 19, Loss: 0.6265181303024292\n",
            "Epoch 6, Batch 20, Loss: 0.6159242987632751\n",
            "Epoch 6, Batch 21, Loss: 0.8938663005828857\n",
            "Epoch 6, Batch 22, Loss: 0.6608952879905701\n",
            "Epoch 6, Batch 23, Loss: 0.6944935917854309\n",
            "Epoch 6, Batch 24, Loss: 0.6470655202865601\n",
            "Epoch 6, Batch 25, Loss: 0.6395014524459839\n",
            "Epoch 6, Batch 26, Loss: 0.9787561893463135\n",
            "Epoch 6, Batch 27, Loss: 1.0036981105804443\n",
            "Epoch 6, Batch 28, Loss: 0.5351743102073669\n",
            "Epoch 6, Batch 29, Loss: 0.8559059500694275\n",
            "Epoch 6, Batch 30, Loss: 1.027944564819336\n",
            "Epoch 6, Batch 31, Loss: 0.8904435634613037\n",
            "Epoch 6, Batch 32, Loss: 0.6705090999603271\n",
            "Epoch 6, Batch 33, Loss: 0.3442024886608124\n",
            "Epoch 7, Batch 1, Loss: 0.9285778999328613\n",
            "Epoch 7, Batch 2, Loss: 0.679183840751648\n",
            "Epoch 7, Batch 3, Loss: 0.4579212963581085\n",
            "Epoch 7, Batch 4, Loss: 0.8966159820556641\n",
            "Epoch 7, Batch 5, Loss: 0.4912833273410797\n",
            "Epoch 7, Batch 6, Loss: 0.7268660068511963\n",
            "Epoch 7, Batch 7, Loss: 0.8028540015220642\n",
            "Epoch 7, Batch 8, Loss: 0.8780680298805237\n",
            "Epoch 7, Batch 9, Loss: 0.6948251724243164\n",
            "Epoch 7, Batch 10, Loss: 0.7979150414466858\n",
            "Epoch 7, Batch 11, Loss: 0.6940581798553467\n",
            "Epoch 7, Batch 12, Loss: 0.9246070981025696\n",
            "Epoch 7, Batch 13, Loss: 0.6072636246681213\n",
            "Epoch 7, Batch 14, Loss: 0.921026349067688\n",
            "Epoch 7, Batch 15, Loss: 0.4738342761993408\n",
            "Epoch 7, Batch 16, Loss: 0.8219632506370544\n",
            "Epoch 7, Batch 17, Loss: 0.7158256769180298\n",
            "Epoch 7, Batch 18, Loss: 0.9375770688056946\n",
            "Epoch 7, Batch 19, Loss: 1.0107243061065674\n",
            "Epoch 7, Batch 20, Loss: 0.7904154658317566\n",
            "Epoch 7, Batch 21, Loss: 0.9617413282394409\n",
            "Epoch 7, Batch 22, Loss: 0.8426812887191772\n",
            "Epoch 7, Batch 23, Loss: 0.7016562223434448\n",
            "Epoch 7, Batch 24, Loss: 0.7628082633018494\n",
            "Epoch 7, Batch 25, Loss: 0.4786320626735687\n",
            "Epoch 7, Batch 26, Loss: 0.6452831029891968\n",
            "Epoch 7, Batch 27, Loss: 0.5740063190460205\n",
            "Epoch 7, Batch 28, Loss: 0.7385877370834351\n",
            "Epoch 7, Batch 29, Loss: 0.6535754203796387\n",
            "Epoch 7, Batch 30, Loss: 0.42859798669815063\n",
            "Epoch 7, Batch 31, Loss: 0.6269248127937317\n",
            "Epoch 7, Batch 32, Loss: 0.8211387395858765\n",
            "Epoch 7, Batch 33, Loss: 0.725286066532135\n",
            "Epoch 8, Batch 1, Loss: 0.6464940309524536\n",
            "Epoch 8, Batch 2, Loss: 0.688775897026062\n",
            "Epoch 8, Batch 3, Loss: 0.6450181007385254\n",
            "Epoch 8, Batch 4, Loss: 0.7256943583488464\n",
            "Epoch 8, Batch 5, Loss: 0.8743976354598999\n",
            "Epoch 8, Batch 6, Loss: 0.5144323706626892\n",
            "Epoch 8, Batch 7, Loss: 0.834402859210968\n",
            "Epoch 8, Batch 8, Loss: 0.5080388188362122\n",
            "Epoch 8, Batch 9, Loss: 0.4418905973434448\n",
            "Epoch 8, Batch 10, Loss: 0.7836129069328308\n",
            "Epoch 8, Batch 11, Loss: 0.7255145311355591\n",
            "Epoch 8, Batch 12, Loss: 0.8056058287620544\n",
            "Epoch 8, Batch 13, Loss: 0.5965421199798584\n",
            "Epoch 8, Batch 14, Loss: 0.6845595240592957\n",
            "Epoch 8, Batch 15, Loss: 0.4779001772403717\n",
            "Epoch 8, Batch 16, Loss: 0.7517229914665222\n",
            "Epoch 8, Batch 17, Loss: 0.5822527408599854\n",
            "Epoch 8, Batch 18, Loss: 0.8524489402770996\n",
            "Epoch 8, Batch 19, Loss: 0.5455490350723267\n",
            "Epoch 8, Batch 20, Loss: 0.9660980105400085\n",
            "Epoch 8, Batch 21, Loss: 0.9107243418693542\n",
            "Epoch 8, Batch 22, Loss: 0.4878515899181366\n",
            "Epoch 8, Batch 23, Loss: 0.49088501930236816\n",
            "Epoch 8, Batch 24, Loss: 0.8019651174545288\n",
            "Epoch 8, Batch 25, Loss: 0.4422954320907593\n",
            "Epoch 8, Batch 26, Loss: 1.0332212448120117\n",
            "Epoch 8, Batch 27, Loss: 0.8378084897994995\n",
            "Epoch 8, Batch 28, Loss: 1.0039911270141602\n",
            "Epoch 8, Batch 29, Loss: 0.7385866641998291\n",
            "Epoch 8, Batch 30, Loss: 0.6774051785469055\n",
            "Epoch 8, Batch 31, Loss: 0.8367209434509277\n",
            "Epoch 8, Batch 32, Loss: 1.067603349685669\n",
            "Epoch 8, Batch 33, Loss: 0.4267502427101135\n",
            "Epoch 9, Batch 1, Loss: 0.9046026468276978\n",
            "Epoch 9, Batch 2, Loss: 0.938757598400116\n",
            "Epoch 9, Batch 3, Loss: 0.7665010690689087\n",
            "Epoch 9, Batch 4, Loss: 0.6718255281448364\n",
            "Epoch 9, Batch 5, Loss: 0.8169476985931396\n",
            "Epoch 9, Batch 6, Loss: 0.752723217010498\n",
            "Epoch 9, Batch 7, Loss: 0.7806986570358276\n",
            "Epoch 9, Batch 8, Loss: 0.8232032060623169\n",
            "Epoch 9, Batch 9, Loss: 0.7923758029937744\n",
            "Epoch 9, Batch 10, Loss: 0.8031186461448669\n",
            "Epoch 9, Batch 11, Loss: 0.7047796249389648\n",
            "Epoch 9, Batch 12, Loss: 1.1657590866088867\n",
            "Epoch 9, Batch 13, Loss: 0.7985364198684692\n",
            "Epoch 9, Batch 14, Loss: 0.8982176780700684\n",
            "Epoch 9, Batch 15, Loss: 0.8238677978515625\n",
            "Epoch 9, Batch 16, Loss: 0.8706406354904175\n",
            "Epoch 9, Batch 17, Loss: 0.9537806510925293\n",
            "Epoch 9, Batch 18, Loss: 0.3997141420841217\n",
            "Epoch 9, Batch 19, Loss: 1.169227957725525\n",
            "Epoch 9, Batch 20, Loss: 0.8129648566246033\n",
            "Epoch 9, Batch 21, Loss: 0.7283511161804199\n",
            "Epoch 9, Batch 22, Loss: 0.7583141326904297\n",
            "Epoch 9, Batch 23, Loss: 0.807900607585907\n",
            "Epoch 9, Batch 24, Loss: 0.7197628021240234\n",
            "Epoch 9, Batch 25, Loss: 0.7672901153564453\n",
            "Epoch 9, Batch 26, Loss: 0.7263302803039551\n",
            "Epoch 9, Batch 27, Loss: 0.5900733470916748\n",
            "Epoch 9, Batch 28, Loss: 0.6233347058296204\n",
            "Epoch 9, Batch 29, Loss: 0.49806275963783264\n",
            "Epoch 9, Batch 30, Loss: 0.7119582891464233\n",
            "Epoch 9, Batch 31, Loss: 0.84808748960495\n",
            "Epoch 9, Batch 32, Loss: 0.671344518661499\n",
            "Epoch 9, Batch 33, Loss: 0.8316361308097839\n",
            "Epoch 10, Batch 1, Loss: 0.5843996405601501\n",
            "Epoch 10, Batch 2, Loss: 0.5656958818435669\n",
            "Epoch 10, Batch 3, Loss: 0.9154089689254761\n",
            "Epoch 10, Batch 4, Loss: 0.5787038803100586\n",
            "Epoch 10, Batch 5, Loss: 0.8272038698196411\n",
            "Epoch 10, Batch 6, Loss: 0.6221321821212769\n",
            "Epoch 10, Batch 7, Loss: 0.759347140789032\n",
            "Epoch 10, Batch 8, Loss: 0.47632092237472534\n",
            "Epoch 10, Batch 9, Loss: 0.7227787375450134\n",
            "Epoch 10, Batch 10, Loss: 0.5506833791732788\n",
            "Epoch 10, Batch 11, Loss: 0.7978197932243347\n",
            "Epoch 10, Batch 12, Loss: 0.6436107158660889\n",
            "Epoch 10, Batch 13, Loss: 0.6081936359405518\n",
            "Epoch 10, Batch 14, Loss: 0.8808749914169312\n",
            "Epoch 10, Batch 15, Loss: 0.5129894018173218\n",
            "Epoch 10, Batch 16, Loss: 0.7460623979568481\n",
            "Epoch 10, Batch 17, Loss: 0.6898826360702515\n",
            "Epoch 10, Batch 18, Loss: 0.5901246070861816\n",
            "Epoch 10, Batch 19, Loss: 0.8271651268005371\n",
            "Epoch 10, Batch 20, Loss: 1.1014198064804077\n",
            "Epoch 10, Batch 21, Loss: 0.7681397795677185\n",
            "Epoch 10, Batch 22, Loss: 1.0191487073898315\n",
            "Epoch 10, Batch 23, Loss: 0.7284311652183533\n",
            "Epoch 10, Batch 24, Loss: 1.0750354528427124\n",
            "Epoch 10, Batch 25, Loss: 1.1073975563049316\n",
            "Epoch 10, Batch 26, Loss: 0.7962941527366638\n",
            "Epoch 10, Batch 27, Loss: 1.2203788757324219\n",
            "Epoch 10, Batch 28, Loss: 0.802098274230957\n",
            "Epoch 10, Batch 29, Loss: 0.6174135208129883\n",
            "Epoch 10, Batch 30, Loss: 0.7524096369743347\n",
            "Epoch 10, Batch 31, Loss: 0.867141604423523\n",
            "Epoch 10, Batch 32, Loss: 0.9213181734085083\n",
            "Epoch 10, Batch 33, Loss: 0.7544819712638855\n",
            "Epoch 11, Batch 1, Loss: 0.6253689527511597\n",
            "Epoch 11, Batch 2, Loss: 0.8662618398666382\n",
            "Epoch 11, Batch 3, Loss: 0.7453738451004028\n",
            "Epoch 11, Batch 4, Loss: 0.7087122797966003\n",
            "Epoch 11, Batch 5, Loss: 0.6621965169906616\n",
            "Epoch 11, Batch 6, Loss: 0.6942718029022217\n",
            "Epoch 11, Batch 7, Loss: 0.6918118596076965\n",
            "Epoch 11, Batch 8, Loss: 0.4752689301967621\n",
            "Epoch 11, Batch 9, Loss: 0.9342718124389648\n",
            "Epoch 11, Batch 10, Loss: 0.6208238005638123\n",
            "Epoch 11, Batch 11, Loss: 0.834114670753479\n",
            "Epoch 11, Batch 12, Loss: 0.7069326639175415\n",
            "Epoch 11, Batch 13, Loss: 0.49032843112945557\n",
            "Epoch 11, Batch 14, Loss: 0.5540065765380859\n",
            "Epoch 11, Batch 15, Loss: 0.7058226466178894\n",
            "Epoch 11, Batch 16, Loss: 0.7899441719055176\n",
            "Epoch 11, Batch 17, Loss: 0.7561297416687012\n",
            "Epoch 11, Batch 18, Loss: 0.9952341318130493\n",
            "Epoch 11, Batch 19, Loss: 0.9138974547386169\n",
            "Epoch 11, Batch 20, Loss: 1.0649168491363525\n",
            "Epoch 11, Batch 21, Loss: 0.7294411659240723\n",
            "Epoch 11, Batch 22, Loss: 0.8926438689231873\n",
            "Epoch 11, Batch 23, Loss: 0.7087163329124451\n",
            "Epoch 11, Batch 24, Loss: 0.6151628494262695\n",
            "Epoch 11, Batch 25, Loss: 0.5643880367279053\n",
            "Epoch 11, Batch 26, Loss: 0.7371764183044434\n",
            "Epoch 11, Batch 27, Loss: 0.6776294112205505\n",
            "Epoch 11, Batch 28, Loss: 0.6208078861236572\n",
            "Epoch 11, Batch 29, Loss: 0.7954661846160889\n",
            "Epoch 11, Batch 30, Loss: 0.7618395090103149\n",
            "Epoch 11, Batch 31, Loss: 0.8188958168029785\n",
            "Epoch 11, Batch 32, Loss: 0.6769857406616211\n",
            "Epoch 11, Batch 33, Loss: 1.1401164531707764\n",
            "Epoch 12, Batch 1, Loss: 0.636899471282959\n",
            "Epoch 12, Batch 2, Loss: 0.6244029998779297\n",
            "Epoch 12, Batch 3, Loss: 0.7042020559310913\n",
            "Epoch 12, Batch 4, Loss: 0.7684468030929565\n",
            "Epoch 12, Batch 5, Loss: 0.5850694179534912\n",
            "Epoch 12, Batch 6, Loss: 0.6040259599685669\n",
            "Epoch 12, Batch 7, Loss: 0.6081966161727905\n",
            "Epoch 12, Batch 8, Loss: 0.9100883603096008\n",
            "Epoch 12, Batch 9, Loss: 0.675828218460083\n",
            "Epoch 12, Batch 10, Loss: 0.9190895557403564\n",
            "Epoch 12, Batch 11, Loss: 0.6743947267532349\n",
            "Epoch 12, Batch 12, Loss: 0.5623084902763367\n",
            "Epoch 12, Batch 13, Loss: 0.8977255821228027\n",
            "Epoch 12, Batch 14, Loss: 0.7268396019935608\n",
            "Epoch 12, Batch 15, Loss: 1.0535094738006592\n",
            "Epoch 12, Batch 16, Loss: 0.8707144856452942\n",
            "Epoch 12, Batch 17, Loss: 1.0360703468322754\n",
            "Epoch 12, Batch 18, Loss: 1.0861921310424805\n",
            "Epoch 12, Batch 19, Loss: 0.580220103263855\n",
            "Epoch 12, Batch 20, Loss: 0.5840358138084412\n",
            "Epoch 12, Batch 21, Loss: 0.6812306642532349\n",
            "Epoch 12, Batch 22, Loss: 0.8663568496704102\n",
            "Epoch 12, Batch 23, Loss: 0.5645180344581604\n",
            "Epoch 12, Batch 24, Loss: 0.6561298370361328\n",
            "Epoch 12, Batch 25, Loss: 0.6719409823417664\n",
            "Epoch 12, Batch 26, Loss: 0.7294169664382935\n",
            "Epoch 12, Batch 27, Loss: 0.5703649520874023\n",
            "Epoch 12, Batch 28, Loss: 0.7682152986526489\n",
            "Epoch 12, Batch 29, Loss: 1.0195097923278809\n",
            "Epoch 12, Batch 30, Loss: 0.7761831879615784\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:tensorflow:==================================\n",
            "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
            "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7e15262af8b0>\n",
            "If you want to mark it as used call its \"mark_used()\" method.\n",
            "It was originally created here:\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/rnn.py\", line 418, in <genexpr>\n",
            "    output_ta_t = tuple(  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/tf_should_use.py\", line 288, in wrapped\n",
            "    return _add_should_use_warning(fn(*args, **kwargs),\n",
            "==================================\n",
            "ERROR:tensorflow:==================================\n",
            "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
            "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7e1529033160>\n",
            "If you want to mark it as used call its \"mark_used()\" method.\n",
            "It was originally created here:\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/rnn.py\", line 418, in <genexpr>\n",
            "    output_ta_t = tuple(  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/tf_should_use.py\", line 288, in wrapped\n",
            "    return _add_should_use_warning(fn(*args, **kwargs),\n",
            "==================================\n",
            "ERROR:tensorflow:==================================\n",
            "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
            "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7e151a89e560>\n",
            "If you want to mark it as used call its \"mark_used()\" method.\n",
            "It was originally created here:\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/rnn.py\", line 418, in <genexpr>\n",
            "    output_ta_t = tuple(  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/tf_should_use.py\", line 288, in wrapped\n",
            "    return _add_should_use_warning(fn(*args, **kwargs),\n",
            "==================================\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12, Batch 31, Loss: 0.5896695852279663\n",
            "Epoch 12, Batch 32, Loss: 1.0717189311981201\n",
            "Epoch 12, Batch 33, Loss: 1.1318480968475342\n",
            "Epoch 13, Batch 1, Loss: 0.6875051259994507\n",
            "Epoch 13, Batch 2, Loss: 0.7015473246574402\n",
            "Epoch 13, Batch 3, Loss: 0.7749155759811401\n",
            "Epoch 13, Batch 4, Loss: 1.0037440061569214\n",
            "Epoch 13, Batch 5, Loss: 0.6222079992294312\n",
            "Epoch 13, Batch 6, Loss: 0.5860673189163208\n",
            "Epoch 13, Batch 7, Loss: 0.6440155506134033\n",
            "Epoch 13, Batch 8, Loss: 0.6710919737815857\n",
            "Epoch 13, Batch 9, Loss: 0.6054389476776123\n",
            "Epoch 13, Batch 10, Loss: 0.6091039180755615\n",
            "Epoch 13, Batch 11, Loss: 0.6558399200439453\n",
            "Epoch 13, Batch 12, Loss: 0.714393138885498\n",
            "Epoch 13, Batch 13, Loss: 0.5591267347335815\n",
            "Epoch 13, Batch 14, Loss: 0.7454805970191956\n",
            "Epoch 13, Batch 15, Loss: 0.9250683784484863\n",
            "Epoch 13, Batch 16, Loss: 0.7648913264274597\n",
            "Epoch 13, Batch 17, Loss: 1.0305302143096924\n",
            "Epoch 13, Batch 18, Loss: 0.7169872522354126\n",
            "Epoch 13, Batch 19, Loss: 0.872818112373352\n",
            "Epoch 13, Batch 20, Loss: 0.7173112034797668\n",
            "Epoch 13, Batch 21, Loss: 0.7120212316513062\n",
            "Epoch 13, Batch 22, Loss: 0.5285742878913879\n",
            "Epoch 13, Batch 23, Loss: 1.1313167810440063\n",
            "Epoch 13, Batch 24, Loss: 0.5309926271438599\n",
            "Epoch 13, Batch 25, Loss: 0.5928076505661011\n",
            "Epoch 13, Batch 26, Loss: 1.0215401649475098\n",
            "Epoch 13, Batch 27, Loss: 0.685506284236908\n",
            "Epoch 13, Batch 28, Loss: 0.6205273866653442\n",
            "Epoch 13, Batch 29, Loss: 0.6282929182052612\n",
            "Epoch 13, Batch 30, Loss: 0.9714388251304626\n",
            "Epoch 13, Batch 31, Loss: 0.8140556216239929\n",
            "Epoch 13, Batch 32, Loss: 0.7782494425773621\n",
            "Epoch 13, Batch 33, Loss: 0.5401053428649902\n",
            "Epoch 14, Batch 1, Loss: 0.9515942335128784\n",
            "Epoch 14, Batch 2, Loss: 1.0159413814544678\n",
            "Epoch 14, Batch 3, Loss: 1.0368256568908691\n",
            "Epoch 14, Batch 4, Loss: 0.48342254757881165\n",
            "Epoch 14, Batch 5, Loss: 0.9252740144729614\n",
            "Epoch 14, Batch 6, Loss: 0.6707857251167297\n",
            "Epoch 14, Batch 7, Loss: 0.6077659726142883\n",
            "Epoch 14, Batch 8, Loss: 0.6325805187225342\n",
            "Epoch 14, Batch 9, Loss: 0.7545756101608276\n",
            "Epoch 14, Batch 10, Loss: 0.47628486156463623\n",
            "Epoch 14, Batch 11, Loss: 0.6042189598083496\n",
            "Epoch 14, Batch 12, Loss: 0.926034152507782\n",
            "Epoch 14, Batch 13, Loss: 0.5666732788085938\n",
            "Epoch 14, Batch 14, Loss: 0.8757940530776978\n",
            "Epoch 14, Batch 15, Loss: 0.4960046410560608\n",
            "Epoch 14, Batch 16, Loss: 0.5706685781478882\n",
            "Epoch 14, Batch 17, Loss: 0.9669152498245239\n",
            "Epoch 14, Batch 18, Loss: 0.8985189199447632\n",
            "Epoch 14, Batch 19, Loss: 0.657437801361084\n",
            "Epoch 14, Batch 20, Loss: 0.6060342788696289\n",
            "Epoch 14, Batch 21, Loss: 0.7549611330032349\n",
            "Epoch 14, Batch 22, Loss: 0.6889808177947998\n",
            "Epoch 14, Batch 23, Loss: 0.7713130712509155\n",
            "Epoch 14, Batch 24, Loss: 0.45111191272735596\n",
            "Epoch 14, Batch 25, Loss: 0.6958829760551453\n",
            "Epoch 14, Batch 26, Loss: 0.7823547124862671\n",
            "Epoch 14, Batch 27, Loss: 1.1388986110687256\n",
            "Epoch 14, Batch 28, Loss: 0.4446369409561157\n",
            "Epoch 14, Batch 29, Loss: 0.4834008514881134\n",
            "Epoch 14, Batch 30, Loss: 0.8463649749755859\n",
            "Epoch 14, Batch 31, Loss: 0.8171530365943909\n",
            "Epoch 14, Batch 32, Loss: 0.8084437847137451\n",
            "Epoch 14, Batch 33, Loss: 0.7998037338256836\n",
            "Epoch 15, Batch 1, Loss: 0.6900311708450317\n",
            "Epoch 15, Batch 2, Loss: 0.9405378103256226\n",
            "Epoch 15, Batch 3, Loss: 1.0361192226409912\n",
            "Epoch 15, Batch 4, Loss: 0.6900539398193359\n",
            "Epoch 15, Batch 5, Loss: 0.7771366834640503\n",
            "Epoch 15, Batch 6, Loss: 0.5192573070526123\n",
            "Epoch 15, Batch 7, Loss: 0.9087380170822144\n",
            "Epoch 15, Batch 8, Loss: 0.6081875562667847\n",
            "Epoch 15, Batch 9, Loss: 0.5197879672050476\n",
            "Epoch 15, Batch 10, Loss: 1.007157325744629\n",
            "Epoch 15, Batch 11, Loss: 0.6592875719070435\n",
            "Epoch 15, Batch 12, Loss: 0.7568803429603577\n",
            "Epoch 15, Batch 13, Loss: 1.014165997505188\n",
            "Epoch 15, Batch 14, Loss: 0.6588062047958374\n",
            "Epoch 15, Batch 15, Loss: 0.5183214545249939\n",
            "Epoch 15, Batch 16, Loss: 0.6027882695198059\n",
            "Epoch 15, Batch 17, Loss: 0.4634614884853363\n",
            "Epoch 15, Batch 18, Loss: 0.6146751642227173\n",
            "Epoch 15, Batch 19, Loss: 0.5292447805404663\n",
            "Epoch 15, Batch 20, Loss: 0.7186183333396912\n",
            "Epoch 15, Batch 21, Loss: 0.9131805300712585\n",
            "Epoch 15, Batch 22, Loss: 0.7640443444252014\n",
            "Epoch 15, Batch 23, Loss: 0.8277055025100708\n",
            "Epoch 15, Batch 24, Loss: 0.5761335492134094\n",
            "Epoch 15, Batch 25, Loss: 0.48695293068885803\n",
            "Epoch 15, Batch 26, Loss: 0.9321670532226562\n",
            "Epoch 15, Batch 27, Loss: 0.7148950099945068\n",
            "Epoch 15, Batch 28, Loss: 0.8122835755348206\n",
            "Epoch 15, Batch 29, Loss: 0.6594511270523071\n",
            "Epoch 15, Batch 30, Loss: 0.9487557411193848\n",
            "Epoch 15, Batch 31, Loss: 0.5584409236907959\n",
            "Epoch 15, Batch 32, Loss: 0.589827835559845\n",
            "Epoch 15, Batch 33, Loss: 0.7116453051567078\n",
            "RMSE for this set: 0.8072030931351818\n",
            "Training with dropout=0.75, lr=0.1, lstm_H=300\n",
            "Epoch 1, Batch 1, Loss: 7.115312576293945\n",
            "Epoch 1, Batch 2, Loss: 106402.5703125\n",
            "Epoch 1, Batch 3, Loss: 6.5338544845581055\n",
            "Epoch 1, Batch 4, Loss: 90.09210205078125\n",
            "Epoch 1, Batch 5, Loss: 27.81675910949707\n",
            "Epoch 1, Batch 6, Loss: 20.795923233032227\n",
            "Epoch 1, Batch 7, Loss: 11.661367416381836\n",
            "Epoch 1, Batch 8, Loss: 10.571614265441895\n",
            "Epoch 1, Batch 9, Loss: 9.610486030578613\n",
            "Epoch 1, Batch 10, Loss: 7.963628768920898\n",
            "Epoch 1, Batch 11, Loss: 9.978180885314941\n",
            "Epoch 1, Batch 12, Loss: 7.566947937011719\n",
            "Epoch 1, Batch 13, Loss: 9.38139820098877\n",
            "Epoch 1, Batch 14, Loss: 8.349689483642578\n",
            "Epoch 1, Batch 15, Loss: 8.85201644897461\n",
            "Epoch 1, Batch 16, Loss: 7.625512599945068\n",
            "Epoch 1, Batch 17, Loss: 9.62236213684082\n",
            "Epoch 1, Batch 18, Loss: 7.967476844787598\n",
            "Epoch 1, Batch 19, Loss: 9.230224609375\n",
            "Epoch 1, Batch 20, Loss: 9.135063171386719\n",
            "Epoch 1, Batch 21, Loss: 9.00702953338623\n",
            "Epoch 1, Batch 22, Loss: 9.164775848388672\n",
            "Epoch 1, Batch 23, Loss: 9.411373138427734\n",
            "Epoch 1, Batch 24, Loss: 10.356618881225586\n",
            "Epoch 1, Batch 25, Loss: 9.652219772338867\n",
            "Epoch 1, Batch 26, Loss: 8.521387100219727\n",
            "Epoch 1, Batch 27, Loss: 9.50735092163086\n",
            "Epoch 1, Batch 28, Loss: 10.309036254882812\n",
            "Epoch 1, Batch 29, Loss: 8.905630111694336\n",
            "Epoch 1, Batch 30, Loss: 9.771086692810059\n",
            "Epoch 1, Batch 31, Loss: 8.991764068603516\n",
            "Epoch 1, Batch 32, Loss: 10.121923446655273\n",
            "Epoch 1, Batch 33, Loss: 8.554634094238281\n",
            "Epoch 2, Batch 1, Loss: 11.544486045837402\n",
            "Epoch 2, Batch 2, Loss: 10.590272903442383\n",
            "Epoch 2, Batch 3, Loss: 9.285418510437012\n",
            "Epoch 2, Batch 4, Loss: 7.841971397399902\n",
            "Epoch 2, Batch 5, Loss: 7.741701126098633\n",
            "Epoch 2, Batch 6, Loss: 8.824607849121094\n",
            "Epoch 2, Batch 7, Loss: 9.192686080932617\n",
            "Epoch 2, Batch 8, Loss: 9.713747024536133\n",
            "Epoch 2, Batch 9, Loss: 9.347549438476562\n",
            "Epoch 2, Batch 10, Loss: 8.601249694824219\n",
            "Epoch 2, Batch 11, Loss: 8.917091369628906\n",
            "Epoch 2, Batch 12, Loss: 8.886604309082031\n",
            "Epoch 2, Batch 13, Loss: 8.113618850708008\n",
            "Epoch 2, Batch 14, Loss: 8.324399948120117\n",
            "Epoch 2, Batch 15, Loss: 9.537771224975586\n",
            "Epoch 2, Batch 16, Loss: 8.627469062805176\n",
            "Epoch 2, Batch 17, Loss: 8.108356475830078\n",
            "Epoch 2, Batch 18, Loss: 9.485681533813477\n",
            "Epoch 2, Batch 19, Loss: 7.824514865875244\n",
            "Epoch 2, Batch 20, Loss: 8.150026321411133\n",
            "Epoch 2, Batch 21, Loss: 10.717964172363281\n",
            "Epoch 2, Batch 22, Loss: 8.509082794189453\n",
            "Epoch 2, Batch 23, Loss: 8.6449556350708\n",
            "Epoch 2, Batch 24, Loss: 7.968960285186768\n",
            "Epoch 2, Batch 25, Loss: 7.937084674835205\n",
            "Epoch 2, Batch 26, Loss: 8.861265182495117\n",
            "Epoch 2, Batch 27, Loss: 6.968071937561035\n",
            "Epoch 2, Batch 28, Loss: 7.999109268188477\n",
            "Epoch 2, Batch 29, Loss: 6.566154479980469\n",
            "Epoch 2, Batch 30, Loss: 8.608057022094727\n",
            "Epoch 2, Batch 31, Loss: 9.65284252166748\n",
            "Epoch 2, Batch 32, Loss: 8.04815673828125\n",
            "Epoch 2, Batch 33, Loss: 8.562029838562012\n",
            "Epoch 3, Batch 1, Loss: 9.511124610900879\n",
            "Epoch 3, Batch 2, Loss: 7.947028636932373\n",
            "Epoch 3, Batch 3, Loss: 8.149989128112793\n",
            "Epoch 3, Batch 4, Loss: 8.0722074508667\n",
            "Epoch 3, Batch 5, Loss: 9.676807403564453\n",
            "Epoch 3, Batch 6, Loss: 7.761521339416504\n",
            "Epoch 3, Batch 7, Loss: 7.9604597091674805\n",
            "Epoch 3, Batch 8, Loss: 8.074899673461914\n",
            "Epoch 3, Batch 9, Loss: 7.575216770172119\n",
            "Epoch 3, Batch 10, Loss: 6.696544647216797\n",
            "Epoch 3, Batch 11, Loss: 7.671943664550781\n",
            "Epoch 3, Batch 12, Loss: 7.75194787979126\n",
            "Epoch 3, Batch 13, Loss: 7.171000957489014\n",
            "Epoch 3, Batch 14, Loss: 8.21735954284668\n",
            "Epoch 3, Batch 15, Loss: 7.135794639587402\n",
            "Epoch 3, Batch 16, Loss: 6.567556381225586\n",
            "Epoch 3, Batch 17, Loss: 6.765102386474609\n",
            "Epoch 3, Batch 18, Loss: 8.351537704467773\n",
            "Epoch 3, Batch 19, Loss: 6.029359340667725\n",
            "Epoch 3, Batch 20, Loss: 7.0582275390625\n",
            "Epoch 3, Batch 21, Loss: 6.539062976837158\n",
            "Epoch 3, Batch 22, Loss: 7.544184684753418\n",
            "Epoch 3, Batch 23, Loss: 7.474496364593506\n",
            "Epoch 3, Batch 24, Loss: 6.955299377441406\n",
            "Epoch 3, Batch 25, Loss: 7.478808879852295\n",
            "Epoch 3, Batch 26, Loss: 6.9223809242248535\n",
            "Epoch 3, Batch 27, Loss: 7.213449478149414\n",
            "Epoch 3, Batch 28, Loss: 7.108526706695557\n",
            "Epoch 3, Batch 29, Loss: 7.646426200866699\n",
            "Epoch 3, Batch 30, Loss: 7.400904178619385\n",
            "Epoch 3, Batch 31, Loss: 7.731398582458496\n",
            "Epoch 3, Batch 32, Loss: 5.96135139465332\n",
            "Epoch 3, Batch 33, Loss: 8.454726219177246\n",
            "Epoch 4, Batch 1, Loss: 7.356607437133789\n",
            "Epoch 4, Batch 2, Loss: 7.571208477020264\n",
            "Epoch 4, Batch 3, Loss: 6.675760269165039\n",
            "Epoch 4, Batch 4, Loss: 6.194404602050781\n",
            "Epoch 4, Batch 5, Loss: 7.6824750900268555\n",
            "Epoch 4, Batch 6, Loss: 7.682570457458496\n",
            "Epoch 4, Batch 7, Loss: 6.075438976287842\n",
            "Epoch 4, Batch 8, Loss: 7.293951034545898\n",
            "Epoch 4, Batch 9, Loss: 5.318204879760742\n",
            "Epoch 4, Batch 10, Loss: 5.030414581298828\n",
            "Epoch 4, Batch 11, Loss: 6.299271583557129\n",
            "Epoch 4, Batch 12, Loss: 5.9058942794799805\n",
            "Epoch 4, Batch 13, Loss: 6.491424560546875\n",
            "Epoch 4, Batch 14, Loss: 7.323367118835449\n",
            "Epoch 4, Batch 15, Loss: 6.10152006149292\n",
            "Epoch 4, Batch 16, Loss: 6.3653764724731445\n",
            "Epoch 4, Batch 17, Loss: 6.34535551071167\n",
            "Epoch 4, Batch 18, Loss: 5.407283782958984\n",
            "Epoch 4, Batch 19, Loss: 5.917163848876953\n",
            "Epoch 4, Batch 20, Loss: 4.8630170822143555\n",
            "Epoch 4, Batch 21, Loss: 7.120645046234131\n",
            "Epoch 4, Batch 22, Loss: 6.028026103973389\n",
            "Epoch 4, Batch 23, Loss: 5.7787766456604\n",
            "Epoch 4, Batch 24, Loss: 5.23403263092041\n",
            "Epoch 4, Batch 25, Loss: 5.638891220092773\n",
            "Epoch 4, Batch 26, Loss: 6.80048942565918\n",
            "Epoch 4, Batch 27, Loss: 5.7772111892700195\n",
            "Epoch 4, Batch 28, Loss: 5.509555339813232\n",
            "Epoch 4, Batch 29, Loss: 6.616222858428955\n",
            "Epoch 4, Batch 30, Loss: 6.122447967529297\n",
            "Epoch 4, Batch 31, Loss: 5.566273212432861\n",
            "Epoch 4, Batch 32, Loss: 5.718762397766113\n",
            "Epoch 4, Batch 33, Loss: 3.7623276710510254\n",
            "Epoch 5, Batch 1, Loss: 6.285142421722412\n",
            "Epoch 5, Batch 2, Loss: 5.477733612060547\n",
            "Epoch 5, Batch 3, Loss: 5.584263801574707\n",
            "Epoch 5, Batch 4, Loss: 6.382811546325684\n",
            "Epoch 5, Batch 5, Loss: 5.869693279266357\n",
            "Epoch 5, Batch 6, Loss: 5.559720993041992\n",
            "Epoch 5, Batch 7, Loss: 5.021766662597656\n",
            "Epoch 5, Batch 8, Loss: 4.651830196380615\n",
            "Epoch 5, Batch 9, Loss: 5.476149082183838\n",
            "Epoch 5, Batch 10, Loss: 4.8236083984375\n",
            "Epoch 5, Batch 11, Loss: 5.676374435424805\n",
            "Epoch 5, Batch 12, Loss: 4.685357093811035\n",
            "Epoch 5, Batch 13, Loss: 5.952152729034424\n",
            "Epoch 5, Batch 14, Loss: 5.492534637451172\n",
            "Epoch 5, Batch 15, Loss: 4.832786560058594\n",
            "Epoch 5, Batch 16, Loss: 5.765896320343018\n",
            "Epoch 5, Batch 17, Loss: 5.255390167236328\n",
            "Epoch 5, Batch 18, Loss: 5.0916924476623535\n",
            "Epoch 5, Batch 19, Loss: 4.639651298522949\n",
            "Epoch 5, Batch 20, Loss: 3.9492111206054688\n",
            "Epoch 5, Batch 21, Loss: 5.184203147888184\n",
            "Epoch 5, Batch 22, Loss: 5.212828636169434\n",
            "Epoch 5, Batch 23, Loss: 3.818588972091675\n",
            "Epoch 5, Batch 24, Loss: 5.007413387298584\n",
            "Epoch 5, Batch 25, Loss: 3.755794048309326\n",
            "Epoch 5, Batch 26, Loss: 4.8645172119140625\n",
            "Epoch 5, Batch 27, Loss: 4.846676826477051\n",
            "Epoch 5, Batch 28, Loss: 4.558839321136475\n",
            "Epoch 5, Batch 29, Loss: 4.860757350921631\n",
            "Epoch 5, Batch 30, Loss: 4.150734901428223\n",
            "Epoch 5, Batch 31, Loss: 4.823916435241699\n",
            "Epoch 5, Batch 32, Loss: 3.4692845344543457\n",
            "Epoch 5, Batch 33, Loss: 3.5993924140930176\n",
            "Epoch 6, Batch 1, Loss: 4.3054962158203125\n",
            "Epoch 6, Batch 2, Loss: 5.037014484405518\n",
            "Epoch 6, Batch 3, Loss: 4.440805912017822\n",
            "Epoch 6, Batch 4, Loss: 3.649251937866211\n",
            "Epoch 6, Batch 5, Loss: 4.384002208709717\n",
            "Epoch 6, Batch 6, Loss: 3.5370142459869385\n",
            "Epoch 6, Batch 7, Loss: 3.7311599254608154\n",
            "Epoch 6, Batch 8, Loss: 4.305444240570068\n",
            "Epoch 6, Batch 9, Loss: 4.125101566314697\n",
            "Epoch 6, Batch 10, Loss: 4.2431464195251465\n",
            "Epoch 6, Batch 11, Loss: 3.79083251953125\n",
            "Epoch 6, Batch 12, Loss: 3.8000049591064453\n",
            "Epoch 6, Batch 13, Loss: 4.0546979904174805\n",
            "Epoch 6, Batch 14, Loss: 4.712862014770508\n",
            "Epoch 6, Batch 15, Loss: 3.6270785331726074\n",
            "Epoch 6, Batch 16, Loss: 4.546117782592773\n",
            "Epoch 6, Batch 17, Loss: 2.8196663856506348\n",
            "Epoch 6, Batch 18, Loss: 4.407688140869141\n",
            "Epoch 6, Batch 19, Loss: 3.7217774391174316\n",
            "Epoch 6, Batch 20, Loss: 3.900700569152832\n",
            "Epoch 6, Batch 21, Loss: 3.6868739128112793\n",
            "Epoch 6, Batch 22, Loss: 4.114025592803955\n",
            "Epoch 6, Batch 23, Loss: 3.997013568878174\n",
            "Epoch 6, Batch 24, Loss: 3.4137938022613525\n",
            "Epoch 6, Batch 25, Loss: 3.2618203163146973\n",
            "Epoch 6, Batch 26, Loss: 4.396814346313477\n",
            "Epoch 6, Batch 27, Loss: 4.603512763977051\n",
            "Epoch 6, Batch 28, Loss: 3.5569515228271484\n",
            "Epoch 6, Batch 29, Loss: 4.2902374267578125\n",
            "Epoch 6, Batch 30, Loss: 2.790243148803711\n",
            "Epoch 6, Batch 31, Loss: 3.129685640335083\n",
            "Epoch 6, Batch 32, Loss: 4.044381141662598\n",
            "Epoch 6, Batch 33, Loss: 5.168685436248779\n",
            "Epoch 7, Batch 1, Loss: 4.01310920715332\n",
            "Epoch 7, Batch 2, Loss: 3.613252878189087\n",
            "Epoch 7, Batch 3, Loss: 3.457608222961426\n",
            "Epoch 7, Batch 4, Loss: 3.1124589443206787\n",
            "Epoch 7, Batch 5, Loss: 4.298269271850586\n",
            "Epoch 7, Batch 6, Loss: 2.781245231628418\n",
            "Epoch 7, Batch 7, Loss: 3.00985050201416\n",
            "Epoch 7, Batch 8, Loss: 3.3259382247924805\n",
            "Epoch 7, Batch 9, Loss: 3.838029146194458\n",
            "Epoch 7, Batch 10, Loss: 3.5348291397094727\n",
            "Epoch 7, Batch 11, Loss: 2.578240394592285\n",
            "Epoch 7, Batch 12, Loss: 3.1691088676452637\n",
            "Epoch 7, Batch 13, Loss: 3.115061044692993\n",
            "Epoch 7, Batch 14, Loss: 3.3526878356933594\n",
            "Epoch 7, Batch 15, Loss: 2.62552809715271\n",
            "Epoch 7, Batch 16, Loss: 3.36917781829834\n",
            "Epoch 7, Batch 17, Loss: 3.1312711238861084\n",
            "Epoch 7, Batch 18, Loss: 2.6159231662750244\n",
            "Epoch 7, Batch 19, Loss: 2.6375463008880615\n",
            "Epoch 7, Batch 20, Loss: 3.1583099365234375\n",
            "Epoch 7, Batch 21, Loss: 3.256568431854248\n",
            "Epoch 7, Batch 22, Loss: 2.980725049972534\n",
            "Epoch 7, Batch 23, Loss: 3.135622978210449\n",
            "Epoch 7, Batch 24, Loss: 2.7554054260253906\n",
            "Epoch 7, Batch 25, Loss: 2.659531354904175\n",
            "Epoch 7, Batch 26, Loss: 3.355823040008545\n",
            "Epoch 7, Batch 27, Loss: 2.9284839630126953\n",
            "Epoch 7, Batch 28, Loss: 2.8376333713531494\n",
            "Epoch 7, Batch 29, Loss: 2.970047950744629\n",
            "Epoch 7, Batch 30, Loss: 2.2175705432891846\n",
            "Epoch 7, Batch 31, Loss: 2.451493263244629\n",
            "Epoch 7, Batch 32, Loss: 3.0771069526672363\n",
            "Epoch 7, Batch 33, Loss: 1.7695229053497314\n",
            "Epoch 8, Batch 1, Loss: 3.248990058898926\n",
            "Epoch 8, Batch 2, Loss: 2.3012499809265137\n",
            "Epoch 8, Batch 3, Loss: 2.7856078147888184\n",
            "Epoch 8, Batch 4, Loss: 2.2712535858154297\n",
            "Epoch 8, Batch 5, Loss: 2.039292335510254\n",
            "Epoch 8, Batch 6, Loss: 2.456387996673584\n",
            "Epoch 8, Batch 7, Loss: 2.140109062194824\n",
            "Epoch 8, Batch 8, Loss: 2.706730842590332\n",
            "Epoch 8, Batch 9, Loss: 2.117671012878418\n",
            "Epoch 8, Batch 10, Loss: 3.031503677368164\n",
            "Epoch 8, Batch 11, Loss: 2.6623098850250244\n",
            "Epoch 8, Batch 12, Loss: 2.4366049766540527\n",
            "Epoch 8, Batch 13, Loss: 2.7330548763275146\n",
            "Epoch 8, Batch 14, Loss: 2.6706748008728027\n",
            "Epoch 8, Batch 15, Loss: 2.829592704772949\n",
            "Epoch 8, Batch 16, Loss: 2.0346546173095703\n",
            "Epoch 8, Batch 17, Loss: 2.185270309448242\n",
            "Epoch 8, Batch 18, Loss: 2.9765145778656006\n",
            "Epoch 8, Batch 19, Loss: 1.8932983875274658\n",
            "Epoch 8, Batch 20, Loss: 2.677201986312866\n",
            "Epoch 8, Batch 21, Loss: 2.965920925140381\n",
            "Epoch 8, Batch 22, Loss: 2.373931884765625\n",
            "Epoch 8, Batch 23, Loss: 2.6538138389587402\n",
            "Epoch 8, Batch 24, Loss: 3.2211318016052246\n",
            "Epoch 8, Batch 25, Loss: 2.3828625679016113\n",
            "Epoch 8, Batch 26, Loss: 2.152602195739746\n",
            "Epoch 8, Batch 27, Loss: 1.3278062343597412\n",
            "Epoch 8, Batch 28, Loss: 2.1293540000915527\n",
            "Epoch 8, Batch 29, Loss: 1.4492725133895874\n",
            "Epoch 8, Batch 30, Loss: 1.9478187561035156\n",
            "Epoch 8, Batch 31, Loss: 2.1250133514404297\n",
            "Epoch 8, Batch 32, Loss: 2.1971969604492188\n",
            "Epoch 8, Batch 33, Loss: 1.4971249103546143\n",
            "Epoch 9, Batch 1, Loss: 1.8678255081176758\n",
            "Epoch 9, Batch 2, Loss: 2.591153621673584\n",
            "Epoch 9, Batch 3, Loss: 1.6650266647338867\n",
            "Epoch 9, Batch 4, Loss: 2.8416810035705566\n",
            "Epoch 9, Batch 5, Loss: 1.8209282159805298\n",
            "Epoch 9, Batch 6, Loss: 2.2462260723114014\n",
            "Epoch 9, Batch 7, Loss: 2.238034725189209\n",
            "Epoch 9, Batch 8, Loss: 1.8603575229644775\n",
            "Epoch 9, Batch 9, Loss: 1.9936974048614502\n",
            "Epoch 9, Batch 10, Loss: 1.8274314403533936\n",
            "Epoch 9, Batch 11, Loss: 2.0686819553375244\n",
            "Epoch 9, Batch 12, Loss: 1.8042845726013184\n",
            "Epoch 9, Batch 13, Loss: 1.6342295408248901\n",
            "Epoch 9, Batch 14, Loss: 1.7462430000305176\n",
            "Epoch 9, Batch 15, Loss: 1.391768455505371\n",
            "Epoch 9, Batch 16, Loss: 1.826674222946167\n",
            "Epoch 9, Batch 17, Loss: 1.7659138441085815\n",
            "Epoch 9, Batch 18, Loss: 1.6514875888824463\n",
            "Epoch 9, Batch 19, Loss: 1.8014668226242065\n",
            "Epoch 9, Batch 20, Loss: 1.571885585784912\n",
            "Epoch 9, Batch 21, Loss: 2.0320730209350586\n",
            "Epoch 9, Batch 22, Loss: 2.502324342727661\n",
            "Epoch 9, Batch 23, Loss: 1.4876909255981445\n",
            "Epoch 9, Batch 24, Loss: 2.1453351974487305\n",
            "Epoch 9, Batch 25, Loss: 1.5408077239990234\n",
            "Epoch 9, Batch 26, Loss: 1.6117477416992188\n",
            "Epoch 9, Batch 27, Loss: 1.4568755626678467\n",
            "Epoch 9, Batch 28, Loss: 1.700135350227356\n",
            "Epoch 9, Batch 29, Loss: 2.364367961883545\n",
            "Epoch 9, Batch 30, Loss: 1.9252649545669556\n",
            "Epoch 9, Batch 31, Loss: 1.533865213394165\n",
            "Epoch 9, Batch 32, Loss: 1.481834888458252\n",
            "Epoch 9, Batch 33, Loss: 1.8841516971588135\n",
            "Epoch 10, Batch 1, Loss: 1.5600961446762085\n",
            "Epoch 10, Batch 2, Loss: 1.6249873638153076\n",
            "Epoch 10, Batch 3, Loss: 1.662011742591858\n",
            "Epoch 10, Batch 4, Loss: 1.5219146013259888\n",
            "Epoch 10, Batch 5, Loss: 2.126817226409912\n",
            "Epoch 10, Batch 6, Loss: 1.462324857711792\n",
            "Epoch 10, Batch 7, Loss: 1.6399012804031372\n",
            "Epoch 10, Batch 8, Loss: 1.5674216747283936\n",
            "Epoch 10, Batch 9, Loss: 1.3829905986785889\n",
            "Epoch 10, Batch 10, Loss: 1.8186873197555542\n",
            "Epoch 10, Batch 11, Loss: 2.054532527923584\n",
            "Epoch 10, Batch 12, Loss: 1.650617003440857\n",
            "Epoch 10, Batch 13, Loss: 0.9906170964241028\n",
            "Epoch 10, Batch 14, Loss: 1.3597230911254883\n",
            "Epoch 10, Batch 15, Loss: 1.5654544830322266\n",
            "Epoch 10, Batch 16, Loss: 1.1597485542297363\n",
            "Epoch 10, Batch 17, Loss: 1.7265572547912598\n",
            "Epoch 10, Batch 18, Loss: 1.6967339515686035\n",
            "Epoch 10, Batch 19, Loss: 1.4543958902359009\n",
            "Epoch 10, Batch 20, Loss: 1.217052698135376\n",
            "Epoch 10, Batch 21, Loss: 1.3816357851028442\n",
            "Epoch 10, Batch 22, Loss: 0.9433172345161438\n",
            "Epoch 10, Batch 23, Loss: 1.385472059249878\n",
            "Epoch 10, Batch 24, Loss: 0.9210262298583984\n",
            "Epoch 10, Batch 25, Loss: 1.9876446723937988\n",
            "Epoch 10, Batch 26, Loss: 1.3461920022964478\n",
            "Epoch 10, Batch 27, Loss: 1.133425235748291\n",
            "Epoch 10, Batch 28, Loss: 1.2430222034454346\n",
            "Epoch 10, Batch 29, Loss: 2.045391082763672\n",
            "Epoch 10, Batch 30, Loss: 1.0939819812774658\n",
            "Epoch 10, Batch 31, Loss: 1.4939203262329102\n",
            "Epoch 10, Batch 32, Loss: 1.3498733043670654\n",
            "Epoch 10, Batch 33, Loss: 1.8006651401519775\n",
            "Epoch 11, Batch 1, Loss: 1.1753170490264893\n",
            "Epoch 11, Batch 2, Loss: 1.4440525770187378\n",
            "Epoch 11, Batch 3, Loss: 1.3687937259674072\n",
            "Epoch 11, Batch 4, Loss: 1.86789071559906\n",
            "Epoch 11, Batch 5, Loss: 1.2679414749145508\n",
            "Epoch 11, Batch 6, Loss: 1.2125433683395386\n",
            "Epoch 11, Batch 7, Loss: 1.0382180213928223\n",
            "Epoch 11, Batch 8, Loss: 1.5770924091339111\n",
            "Epoch 11, Batch 9, Loss: 1.414729118347168\n",
            "Epoch 11, Batch 10, Loss: 1.1439234018325806\n",
            "Epoch 11, Batch 11, Loss: 1.1719268560409546\n",
            "Epoch 11, Batch 12, Loss: 1.1474924087524414\n",
            "Epoch 11, Batch 13, Loss: 0.9969690442085266\n",
            "Epoch 11, Batch 14, Loss: 1.1785868406295776\n",
            "Epoch 11, Batch 15, Loss: 1.16765558719635\n",
            "Epoch 11, Batch 16, Loss: 1.359220027923584\n",
            "Epoch 11, Batch 17, Loss: 1.0633652210235596\n",
            "Epoch 11, Batch 18, Loss: 1.0085828304290771\n",
            "Epoch 11, Batch 19, Loss: 1.2581337690353394\n",
            "Epoch 11, Batch 20, Loss: 1.2504525184631348\n",
            "Epoch 11, Batch 21, Loss: 1.277527928352356\n",
            "Epoch 11, Batch 22, Loss: 0.8276013135910034\n",
            "Epoch 11, Batch 23, Loss: 1.217475414276123\n",
            "Epoch 11, Batch 24, Loss: 1.1955127716064453\n",
            "Epoch 11, Batch 25, Loss: 1.6165388822555542\n",
            "Epoch 11, Batch 26, Loss: 1.0960657596588135\n",
            "Epoch 11, Batch 27, Loss: 1.6406816244125366\n",
            "Epoch 11, Batch 28, Loss: 0.9056848883628845\n",
            "Epoch 11, Batch 29, Loss: 1.0730018615722656\n",
            "Epoch 11, Batch 30, Loss: 1.0625989437103271\n",
            "Epoch 11, Batch 31, Loss: 0.9634556174278259\n",
            "Epoch 11, Batch 32, Loss: 1.0058513879776\n",
            "Epoch 11, Batch 33, Loss: 0.6776749491691589\n",
            "Epoch 12, Batch 1, Loss: 1.089760422706604\n",
            "Epoch 12, Batch 2, Loss: 1.0991252660751343\n",
            "Epoch 12, Batch 3, Loss: 1.1268908977508545\n",
            "Epoch 12, Batch 4, Loss: 1.1339142322540283\n",
            "Epoch 12, Batch 5, Loss: 1.0996897220611572\n",
            "Epoch 12, Batch 6, Loss: 0.913627028465271\n",
            "Epoch 12, Batch 7, Loss: 0.8802704811096191\n",
            "Epoch 12, Batch 8, Loss: 0.9087280035018921\n",
            "Epoch 12, Batch 9, Loss: 1.2777891159057617\n",
            "Epoch 12, Batch 10, Loss: 0.7706084251403809\n",
            "Epoch 12, Batch 11, Loss: 1.3215043544769287\n",
            "Epoch 12, Batch 12, Loss: 0.9123957753181458\n",
            "Epoch 12, Batch 13, Loss: 1.2145874500274658\n",
            "Epoch 12, Batch 14, Loss: 1.2090662717819214\n",
            "Epoch 12, Batch 15, Loss: 1.160160779953003\n",
            "Epoch 12, Batch 16, Loss: 0.7133223414421082\n",
            "Epoch 12, Batch 17, Loss: 0.848584771156311\n",
            "Epoch 12, Batch 18, Loss: 1.3343099355697632\n",
            "Epoch 12, Batch 19, Loss: 1.2228991985321045\n",
            "Epoch 12, Batch 20, Loss: 0.7513637542724609\n",
            "Epoch 12, Batch 21, Loss: 0.8635115027427673\n",
            "Epoch 12, Batch 22, Loss: 1.0027941465377808\n",
            "Epoch 12, Batch 23, Loss: 0.9232138991355896\n",
            "Epoch 12, Batch 24, Loss: 1.2309894561767578\n",
            "Epoch 12, Batch 25, Loss: 0.9461727738380432\n",
            "Epoch 12, Batch 26, Loss: 0.776414692401886\n",
            "Epoch 12, Batch 27, Loss: 0.8895562887191772\n",
            "Epoch 12, Batch 28, Loss: 1.0175915956497192\n",
            "Epoch 12, Batch 29, Loss: 1.0151973962783813\n",
            "Epoch 12, Batch 30, Loss: 1.0651533603668213\n",
            "Epoch 12, Batch 31, Loss: 0.9814122319221497\n",
            "Epoch 12, Batch 32, Loss: 0.9488939046859741\n",
            "Epoch 12, Batch 33, Loss: 1.3245118856430054\n",
            "Epoch 13, Batch 1, Loss: 1.3276759386062622\n",
            "Epoch 13, Batch 2, Loss: 0.9577982425689697\n",
            "Epoch 13, Batch 3, Loss: 0.9473137855529785\n",
            "Epoch 13, Batch 4, Loss: 0.8933144807815552\n",
            "Epoch 13, Batch 5, Loss: 0.8651162385940552\n",
            "Epoch 13, Batch 6, Loss: 0.7451223134994507\n",
            "Epoch 13, Batch 7, Loss: 0.697243332862854\n",
            "Epoch 13, Batch 8, Loss: 1.3664531707763672\n",
            "Epoch 13, Batch 9, Loss: 0.9089372158050537\n",
            "Epoch 13, Batch 10, Loss: 1.076990008354187\n",
            "Epoch 13, Batch 11, Loss: 0.6829545497894287\n",
            "Epoch 13, Batch 12, Loss: 1.034571647644043\n",
            "Epoch 13, Batch 13, Loss: 1.007124423980713\n",
            "Epoch 13, Batch 14, Loss: 0.9621860384941101\n",
            "Epoch 13, Batch 15, Loss: 0.7053109407424927\n",
            "Epoch 13, Batch 16, Loss: 0.7406035661697388\n",
            "Epoch 13, Batch 17, Loss: 0.7105456590652466\n",
            "Epoch 13, Batch 18, Loss: 0.9295998811721802\n",
            "Epoch 13, Batch 19, Loss: 0.8710131049156189\n",
            "Epoch 13, Batch 20, Loss: 0.956602931022644\n",
            "Epoch 13, Batch 21, Loss: 0.7738440632820129\n",
            "Epoch 13, Batch 22, Loss: 0.792199969291687\n",
            "Epoch 13, Batch 23, Loss: 1.1243011951446533\n",
            "Epoch 13, Batch 24, Loss: 0.38392773270606995\n",
            "Epoch 13, Batch 25, Loss: 1.0515429973602295\n",
            "Epoch 13, Batch 26, Loss: 1.1420631408691406\n",
            "Epoch 13, Batch 27, Loss: 0.649982213973999\n",
            "Epoch 13, Batch 28, Loss: 0.9695266485214233\n",
            "Epoch 13, Batch 29, Loss: 0.649681806564331\n",
            "Epoch 13, Batch 30, Loss: 0.983452558517456\n",
            "Epoch 13, Batch 31, Loss: 0.8154163360595703\n",
            "Epoch 13, Batch 32, Loss: 0.975113034248352\n",
            "Epoch 13, Batch 33, Loss: 0.8915383219718933\n",
            "Epoch 14, Batch 1, Loss: 0.9920466542243958\n",
            "Epoch 14, Batch 2, Loss: 0.7403507232666016\n",
            "Epoch 14, Batch 3, Loss: 0.7914870977401733\n",
            "Epoch 14, Batch 4, Loss: 0.9440422058105469\n",
            "Epoch 14, Batch 5, Loss: 0.6112486124038696\n",
            "Epoch 14, Batch 6, Loss: 0.5600084662437439\n",
            "Epoch 14, Batch 7, Loss: 0.961267352104187\n",
            "Epoch 14, Batch 8, Loss: 0.6602871417999268\n",
            "Epoch 14, Batch 9, Loss: 0.8169928789138794\n",
            "Epoch 14, Batch 10, Loss: 0.6523171663284302\n",
            "Epoch 14, Batch 11, Loss: 0.8541189432144165\n",
            "Epoch 14, Batch 12, Loss: 0.881690502166748\n",
            "Epoch 14, Batch 13, Loss: 0.8226820826530457\n",
            "Epoch 14, Batch 14, Loss: 0.7094526290893555\n",
            "Epoch 14, Batch 15, Loss: 0.7382099628448486\n",
            "Epoch 14, Batch 16, Loss: 0.7033161520957947\n",
            "Epoch 14, Batch 17, Loss: 0.9617851972579956\n",
            "Epoch 14, Batch 18, Loss: 0.8253166675567627\n",
            "Epoch 14, Batch 19, Loss: 1.0835914611816406\n",
            "Epoch 14, Batch 20, Loss: 0.8465374708175659\n",
            "Epoch 14, Batch 21, Loss: 0.9349427223205566\n",
            "Epoch 14, Batch 22, Loss: 0.9108657836914062\n",
            "Epoch 14, Batch 23, Loss: 0.857123613357544\n",
            "Epoch 14, Batch 24, Loss: 0.8277132511138916\n",
            "Epoch 14, Batch 25, Loss: 0.7640390396118164\n",
            "Epoch 14, Batch 26, Loss: 0.8366922736167908\n",
            "Epoch 14, Batch 27, Loss: 0.6195390820503235\n",
            "Epoch 14, Batch 28, Loss: 0.9884719848632812\n",
            "Epoch 14, Batch 29, Loss: 0.9470497369766235\n",
            "Epoch 14, Batch 30, Loss: 0.690550684928894\n",
            "Epoch 14, Batch 31, Loss: 0.8373150825500488\n",
            "Epoch 14, Batch 32, Loss: 0.7789930105209351\n",
            "Epoch 14, Batch 33, Loss: 0.4823857247829437\n",
            "Epoch 15, Batch 1, Loss: 0.5527211427688599\n",
            "Epoch 15, Batch 2, Loss: 0.6224003434181213\n",
            "Epoch 15, Batch 3, Loss: 1.2260946035385132\n",
            "Epoch 15, Batch 4, Loss: 0.6360185742378235\n",
            "Epoch 15, Batch 5, Loss: 0.8928477764129639\n",
            "Epoch 15, Batch 6, Loss: 1.0987672805786133\n",
            "Epoch 15, Batch 7, Loss: 0.6787430047988892\n",
            "Epoch 15, Batch 8, Loss: 0.7735809087753296\n",
            "Epoch 15, Batch 9, Loss: 0.7852995991706848\n",
            "Epoch 15, Batch 10, Loss: 0.7415774464607239\n",
            "Epoch 15, Batch 11, Loss: 0.8503692150115967\n",
            "Epoch 15, Batch 12, Loss: 0.7683464288711548\n",
            "Epoch 15, Batch 13, Loss: 0.8096101880073547\n",
            "Epoch 15, Batch 14, Loss: 0.7667996883392334\n",
            "Epoch 15, Batch 15, Loss: 0.9734362959861755\n",
            "Epoch 15, Batch 16, Loss: 0.46670761704444885\n",
            "Epoch 15, Batch 17, Loss: 0.572941243648529\n",
            "Epoch 15, Batch 18, Loss: 0.9991410970687866\n",
            "Epoch 15, Batch 19, Loss: 0.6778843402862549\n",
            "Epoch 15, Batch 20, Loss: 0.5752843618392944\n",
            "Epoch 15, Batch 21, Loss: 0.5787189602851868\n",
            "Epoch 15, Batch 22, Loss: 0.7416902780532837\n",
            "Epoch 15, Batch 23, Loss: 0.904860258102417\n",
            "Epoch 15, Batch 24, Loss: 0.4493575990200043\n",
            "Epoch 15, Batch 25, Loss: 1.0399354696273804\n",
            "Epoch 15, Batch 26, Loss: 0.705754280090332\n",
            "Epoch 15, Batch 27, Loss: 0.6739383935928345\n",
            "Epoch 15, Batch 28, Loss: 0.7043991684913635\n",
            "Epoch 15, Batch 29, Loss: 0.7488465309143066\n",
            "Epoch 15, Batch 30, Loss: 0.9294132590293884\n",
            "Epoch 15, Batch 31, Loss: 0.8811383247375488\n",
            "Epoch 15, Batch 32, Loss: 0.4629751443862915\n",
            "Epoch 15, Batch 33, Loss: 1.1662551164627075\n",
            "RMSE for this set: 0.8011980683522617\n",
            "Training with dropout=0.75, lr=0.01, lstm_H=200\n",
            "Epoch 1, Batch 1, Loss: 6.256880760192871\n",
            "Epoch 1, Batch 2, Loss: 0.8601998686790466\n",
            "Epoch 1, Batch 3, Loss: 12.349846839904785\n",
            "Epoch 1, Batch 4, Loss: 6.339310169219971\n",
            "Epoch 1, Batch 5, Loss: 1.9677821397781372\n",
            "Epoch 1, Batch 6, Loss: 1.3926339149475098\n",
            "Epoch 1, Batch 7, Loss: 1.8871228694915771\n",
            "Epoch 1, Batch 8, Loss: 0.6008596420288086\n",
            "Epoch 1, Batch 9, Loss: 2.1116647720336914\n",
            "Epoch 1, Batch 10, Loss: 0.8113042116165161\n",
            "Epoch 1, Batch 11, Loss: 0.675026535987854\n",
            "Epoch 1, Batch 12, Loss: 1.2943315505981445\n",
            "Epoch 1, Batch 13, Loss: 1.2407119274139404\n",
            "Epoch 1, Batch 14, Loss: 0.7157171368598938\n",
            "Epoch 1, Batch 15, Loss: 0.6715837717056274\n",
            "Epoch 1, Batch 16, Loss: 1.1182429790496826\n",
            "Epoch 1, Batch 17, Loss: 0.9791494607925415\n",
            "Epoch 1, Batch 18, Loss: 0.8727194666862488\n",
            "Epoch 1, Batch 19, Loss: 0.5377624034881592\n",
            "Epoch 1, Batch 20, Loss: 0.6421957612037659\n",
            "Epoch 1, Batch 21, Loss: 0.63227379322052\n",
            "Epoch 1, Batch 22, Loss: 0.8042938709259033\n",
            "Epoch 1, Batch 23, Loss: 0.8291805982589722\n",
            "Epoch 1, Batch 24, Loss: 0.892078161239624\n",
            "Epoch 1, Batch 25, Loss: 0.642661452293396\n",
            "Epoch 1, Batch 26, Loss: 0.8666141629219055\n",
            "Epoch 1, Batch 27, Loss: 0.8353676795959473\n",
            "Epoch 1, Batch 28, Loss: 0.6774324774742126\n",
            "Epoch 1, Batch 29, Loss: 0.5695194005966187\n",
            "Epoch 1, Batch 30, Loss: 0.6847688555717468\n",
            "Epoch 1, Batch 31, Loss: 0.9294824600219727\n",
            "Epoch 1, Batch 32, Loss: 0.6586095094680786\n",
            "Epoch 1, Batch 33, Loss: 0.8265867233276367\n",
            "Epoch 2, Batch 1, Loss: 0.9314231276512146\n",
            "Epoch 2, Batch 2, Loss: 0.5203644037246704\n",
            "Epoch 2, Batch 3, Loss: 0.7910798192024231\n",
            "Epoch 2, Batch 4, Loss: 1.0230540037155151\n",
            "Epoch 2, Batch 5, Loss: 0.6501408815383911\n",
            "Epoch 2, Batch 6, Loss: 0.587741494178772\n",
            "Epoch 2, Batch 7, Loss: 0.666512131690979\n",
            "Epoch 2, Batch 8, Loss: 0.7350764274597168\n",
            "Epoch 2, Batch 9, Loss: 1.0556879043579102\n",
            "Epoch 2, Batch 10, Loss: 0.552436113357544\n",
            "Epoch 2, Batch 11, Loss: 0.5016531944274902\n",
            "Epoch 2, Batch 12, Loss: 0.8208370208740234\n",
            "Epoch 2, Batch 13, Loss: 0.7194288969039917\n",
            "Epoch 2, Batch 14, Loss: 0.7403725385665894\n",
            "Epoch 2, Batch 15, Loss: 0.6655985713005066\n",
            "Epoch 2, Batch 16, Loss: 0.6645301580429077\n",
            "Epoch 2, Batch 17, Loss: 0.37829115986824036\n",
            "Epoch 2, Batch 18, Loss: 0.36040109395980835\n",
            "Epoch 2, Batch 19, Loss: 0.7086992263793945\n",
            "Epoch 2, Batch 20, Loss: 0.7657459378242493\n",
            "Epoch 2, Batch 21, Loss: 0.6502165794372559\n",
            "Epoch 2, Batch 22, Loss: 0.8045170903205872\n",
            "Epoch 2, Batch 23, Loss: 0.5835128426551819\n",
            "Epoch 2, Batch 24, Loss: 0.5279880166053772\n",
            "Epoch 2, Batch 25, Loss: 0.7009484767913818\n",
            "Epoch 2, Batch 26, Loss: 0.9138703346252441\n",
            "Epoch 2, Batch 27, Loss: 0.603867769241333\n",
            "Epoch 2, Batch 28, Loss: 0.8263466358184814\n",
            "Epoch 2, Batch 29, Loss: 0.8243358731269836\n",
            "Epoch 2, Batch 30, Loss: 0.7908207178115845\n",
            "Epoch 2, Batch 31, Loss: 0.6369180679321289\n",
            "Epoch 2, Batch 32, Loss: 0.6798732876777649\n",
            "Epoch 2, Batch 33, Loss: 1.2095903158187866\n",
            "Epoch 3, Batch 1, Loss: 0.9743801355361938\n",
            "Epoch 3, Batch 2, Loss: 0.6071147322654724\n",
            "Epoch 3, Batch 3, Loss: 0.4896208643913269\n",
            "Epoch 3, Batch 4, Loss: 0.5811682939529419\n",
            "Epoch 3, Batch 5, Loss: 0.7158175110816956\n",
            "Epoch 3, Batch 6, Loss: 0.7831034660339355\n",
            "Epoch 3, Batch 7, Loss: 0.7378603219985962\n",
            "Epoch 3, Batch 8, Loss: 0.7953338623046875\n",
            "Epoch 3, Batch 9, Loss: 0.8644542098045349\n",
            "Epoch 3, Batch 10, Loss: 0.6852113008499146\n",
            "Epoch 3, Batch 11, Loss: 0.6602756977081299\n",
            "Epoch 3, Batch 12, Loss: 0.47975072264671326\n",
            "Epoch 3, Batch 13, Loss: 0.772208571434021\n",
            "Epoch 3, Batch 14, Loss: 0.701608419418335\n",
            "Epoch 3, Batch 15, Loss: 0.9107651114463806\n",
            "Epoch 3, Batch 16, Loss: 0.6064835786819458\n",
            "Epoch 3, Batch 17, Loss: 0.6322892904281616\n",
            "Epoch 3, Batch 18, Loss: 0.5360699892044067\n",
            "Epoch 3, Batch 19, Loss: 0.6728207468986511\n",
            "Epoch 3, Batch 20, Loss: 0.8077808618545532\n",
            "Epoch 3, Batch 21, Loss: 0.7138429284095764\n",
            "Epoch 3, Batch 22, Loss: 0.8931492567062378\n",
            "Epoch 3, Batch 23, Loss: 0.8475239872932434\n",
            "Epoch 3, Batch 24, Loss: 0.6733564138412476\n",
            "Epoch 3, Batch 25, Loss: 0.6688271164894104\n",
            "Epoch 3, Batch 26, Loss: 0.7424523830413818\n",
            "Epoch 3, Batch 27, Loss: 0.738335132598877\n",
            "Epoch 3, Batch 28, Loss: 0.6446860432624817\n",
            "Epoch 3, Batch 29, Loss: 0.5955156087875366\n",
            "Epoch 3, Batch 30, Loss: 0.8482552170753479\n",
            "Epoch 3, Batch 31, Loss: 0.8802556991577148\n",
            "Epoch 3, Batch 32, Loss: 0.7388805747032166\n",
            "Epoch 3, Batch 33, Loss: 1.004632592201233\n",
            "Epoch 4, Batch 1, Loss: 0.4157077372074127\n",
            "Epoch 4, Batch 2, Loss: 0.676455020904541\n",
            "Epoch 4, Batch 3, Loss: 0.5599564909934998\n",
            "Epoch 4, Batch 4, Loss: 0.52239990234375\n",
            "Epoch 4, Batch 5, Loss: 0.8591374158859253\n",
            "Epoch 4, Batch 6, Loss: 0.8956082463264465\n",
            "Epoch 4, Batch 7, Loss: 0.6875358819961548\n",
            "Epoch 4, Batch 8, Loss: 0.6955515146255493\n",
            "Epoch 4, Batch 9, Loss: 0.6260040998458862\n",
            "Epoch 4, Batch 10, Loss: 0.8410047888755798\n",
            "Epoch 4, Batch 11, Loss: 0.7531427145004272\n",
            "Epoch 4, Batch 12, Loss: 0.6020289063453674\n",
            "Epoch 4, Batch 13, Loss: 0.7288630604743958\n",
            "Epoch 4, Batch 14, Loss: 0.7276790142059326\n",
            "Epoch 4, Batch 15, Loss: 0.453385591506958\n",
            "Epoch 4, Batch 16, Loss: 0.859065592288971\n",
            "Epoch 4, Batch 17, Loss: 0.552898645401001\n",
            "Epoch 4, Batch 18, Loss: 0.6708357334136963\n",
            "Epoch 4, Batch 19, Loss: 0.6254982948303223\n",
            "Epoch 4, Batch 20, Loss: 0.9150716066360474\n",
            "Epoch 4, Batch 21, Loss: 0.9129765033721924\n",
            "Epoch 4, Batch 22, Loss: 0.5913305282592773\n",
            "Epoch 4, Batch 23, Loss: 0.6098486185073853\n",
            "Epoch 4, Batch 24, Loss: 0.7433176040649414\n",
            "Epoch 4, Batch 25, Loss: 0.7988618612289429\n",
            "Epoch 4, Batch 26, Loss: 0.812725305557251\n",
            "Epoch 4, Batch 27, Loss: 0.7822675704956055\n",
            "Epoch 4, Batch 28, Loss: 0.5924665331840515\n",
            "Epoch 4, Batch 29, Loss: 0.7353581786155701\n",
            "Epoch 4, Batch 30, Loss: 0.653731107711792\n",
            "Epoch 4, Batch 31, Loss: 0.8147879838943481\n",
            "Epoch 4, Batch 32, Loss: 0.7267094254493713\n",
            "Epoch 4, Batch 33, Loss: 0.9243587255477905\n",
            "Epoch 5, Batch 1, Loss: 1.0045146942138672\n",
            "Epoch 5, Batch 2, Loss: 0.6781718730926514\n",
            "Epoch 5, Batch 3, Loss: 0.48077476024627686\n",
            "Epoch 5, Batch 4, Loss: 0.645359992980957\n",
            "Epoch 5, Batch 5, Loss: 0.6022522449493408\n",
            "Epoch 5, Batch 6, Loss: 0.7701042294502258\n",
            "Epoch 5, Batch 7, Loss: 0.4809930622577667\n",
            "Epoch 5, Batch 8, Loss: 0.7566928863525391\n",
            "Epoch 5, Batch 9, Loss: 0.6032894849777222\n",
            "Epoch 5, Batch 10, Loss: 0.6678407192230225\n",
            "Epoch 5, Batch 11, Loss: 0.9313794374465942\n",
            "Epoch 5, Batch 12, Loss: 0.5660351514816284\n",
            "Epoch 5, Batch 13, Loss: 0.4839942753314972\n",
            "Epoch 5, Batch 14, Loss: 0.5433295965194702\n",
            "Epoch 5, Batch 15, Loss: 0.47615814208984375\n",
            "Epoch 5, Batch 16, Loss: 0.652271032333374\n",
            "Epoch 5, Batch 17, Loss: 0.7353076934814453\n",
            "Epoch 5, Batch 18, Loss: 1.000152826309204\n",
            "Epoch 5, Batch 19, Loss: 0.7769527435302734\n",
            "Epoch 5, Batch 20, Loss: 0.9580494165420532\n",
            "Epoch 5, Batch 21, Loss: 0.9784574508666992\n",
            "Epoch 5, Batch 22, Loss: 0.7527255415916443\n",
            "Epoch 5, Batch 23, Loss: 0.8489806652069092\n",
            "Epoch 5, Batch 24, Loss: 0.5874902009963989\n",
            "Epoch 5, Batch 25, Loss: 0.8571469783782959\n",
            "Epoch 5, Batch 26, Loss: 0.7002274394035339\n",
            "Epoch 5, Batch 27, Loss: 0.6059386134147644\n",
            "Epoch 5, Batch 28, Loss: 0.8071781992912292\n",
            "Epoch 5, Batch 29, Loss: 0.5702782869338989\n",
            "Epoch 5, Batch 30, Loss: 0.6502485275268555\n",
            "Epoch 5, Batch 31, Loss: 0.4367080628871918\n",
            "Epoch 5, Batch 32, Loss: 0.5851921439170837\n",
            "Epoch 5, Batch 33, Loss: 1.0512824058532715\n",
            "Epoch 6, Batch 1, Loss: 0.6724528074264526\n",
            "Epoch 6, Batch 2, Loss: 1.01283597946167\n",
            "Epoch 6, Batch 3, Loss: 0.9287526607513428\n",
            "Epoch 6, Batch 4, Loss: 0.6474332809448242\n",
            "Epoch 6, Batch 5, Loss: 0.7261382341384888\n",
            "Epoch 6, Batch 6, Loss: 0.5933810472488403\n",
            "Epoch 6, Batch 7, Loss: 0.8959932327270508\n",
            "Epoch 6, Batch 8, Loss: 0.8730751276016235\n",
            "Epoch 6, Batch 9, Loss: 0.6753909587860107\n",
            "Epoch 6, Batch 10, Loss: 0.7916579842567444\n",
            "Epoch 6, Batch 11, Loss: 0.813795804977417\n",
            "Epoch 6, Batch 12, Loss: 0.5633431673049927\n",
            "Epoch 6, Batch 13, Loss: 0.5542090535163879\n",
            "Epoch 6, Batch 14, Loss: 0.5098984837532043\n",
            "Epoch 6, Batch 15, Loss: 0.6984058618545532\n",
            "Epoch 6, Batch 16, Loss: 0.9906232953071594\n",
            "Epoch 6, Batch 17, Loss: 0.7662659287452698\n",
            "Epoch 6, Batch 18, Loss: 0.5766235589981079\n",
            "Epoch 6, Batch 19, Loss: 0.7784773111343384\n",
            "Epoch 6, Batch 20, Loss: 0.6732403039932251\n",
            "Epoch 6, Batch 21, Loss: 1.0164129734039307\n",
            "Epoch 6, Batch 22, Loss: 0.5872206687927246\n",
            "Epoch 6, Batch 23, Loss: 0.8745485544204712\n",
            "Epoch 6, Batch 24, Loss: 0.5903513431549072\n",
            "Epoch 6, Batch 25, Loss: 0.7456454038619995\n",
            "Epoch 6, Batch 26, Loss: 0.9634933471679688\n",
            "Epoch 6, Batch 27, Loss: 0.9394724369049072\n",
            "Epoch 6, Batch 28, Loss: 0.6992000341415405\n",
            "Epoch 6, Batch 29, Loss: 0.7827290296554565\n",
            "Epoch 6, Batch 30, Loss: 0.5793225765228271\n",
            "Epoch 6, Batch 31, Loss: 0.5084671974182129\n",
            "Epoch 6, Batch 32, Loss: 0.6111075282096863\n",
            "Epoch 6, Batch 33, Loss: 0.5343176126480103\n",
            "Epoch 7, Batch 1, Loss: 0.5534747838973999\n",
            "Epoch 7, Batch 2, Loss: 0.4911222457885742\n",
            "Epoch 7, Batch 3, Loss: 0.6755449175834656\n",
            "Epoch 7, Batch 4, Loss: 0.9505330324172974\n",
            "Epoch 7, Batch 5, Loss: 0.9605274200439453\n",
            "Epoch 7, Batch 6, Loss: 1.2491506338119507\n",
            "Epoch 7, Batch 7, Loss: 0.7948617339134216\n",
            "Epoch 7, Batch 8, Loss: 0.5836793780326843\n",
            "Epoch 7, Batch 9, Loss: 0.7137683629989624\n",
            "Epoch 7, Batch 10, Loss: 0.5784112811088562\n",
            "Epoch 7, Batch 11, Loss: 1.0760482549667358\n",
            "Epoch 7, Batch 12, Loss: 0.7927804589271545\n",
            "Epoch 7, Batch 13, Loss: 0.958631694316864\n",
            "Epoch 7, Batch 14, Loss: 0.8669809103012085\n",
            "Epoch 7, Batch 15, Loss: 0.843207836151123\n",
            "Epoch 7, Batch 16, Loss: 0.8278822302818298\n",
            "Epoch 7, Batch 17, Loss: 0.5483556985855103\n",
            "Epoch 7, Batch 18, Loss: 0.7266391515731812\n",
            "Epoch 7, Batch 19, Loss: 0.523970365524292\n",
            "Epoch 7, Batch 20, Loss: 0.6963533163070679\n",
            "Epoch 7, Batch 21, Loss: 0.709065318107605\n",
            "Epoch 7, Batch 22, Loss: 0.750579833984375\n",
            "Epoch 7, Batch 23, Loss: 0.6935688257217407\n",
            "Epoch 7, Batch 24, Loss: 0.7589944005012512\n",
            "Epoch 7, Batch 25, Loss: 0.49651390314102173\n",
            "Epoch 7, Batch 26, Loss: 0.7573301792144775\n",
            "Epoch 7, Batch 27, Loss: 0.6777191758155823\n",
            "Epoch 7, Batch 28, Loss: 0.9169033169746399\n",
            "Epoch 7, Batch 29, Loss: 0.6060106754302979\n",
            "Epoch 7, Batch 30, Loss: 0.8505497574806213\n",
            "Epoch 7, Batch 31, Loss: 0.5996044874191284\n",
            "Epoch 7, Batch 32, Loss: 0.7221571207046509\n",
            "Epoch 7, Batch 33, Loss: 0.7372373342514038\n",
            "Epoch 8, Batch 1, Loss: 0.7782200574874878\n",
            "Epoch 8, Batch 2, Loss: 0.34554678201675415\n",
            "Epoch 8, Batch 3, Loss: 0.7696536183357239\n",
            "Epoch 8, Batch 4, Loss: 0.6379954218864441\n",
            "Epoch 8, Batch 5, Loss: 0.6614239811897278\n",
            "Epoch 8, Batch 6, Loss: 0.5678555965423584\n",
            "Epoch 8, Batch 7, Loss: 0.531119704246521\n",
            "Epoch 8, Batch 8, Loss: 0.5891457200050354\n",
            "Epoch 8, Batch 9, Loss: 0.5934933423995972\n",
            "Epoch 8, Batch 10, Loss: 0.6672157049179077\n",
            "Epoch 8, Batch 11, Loss: 0.865635871887207\n",
            "Epoch 8, Batch 12, Loss: 0.8822751641273499\n",
            "Epoch 8, Batch 13, Loss: 0.9941232800483704\n",
            "Epoch 8, Batch 14, Loss: 0.8497059345245361\n",
            "Epoch 8, Batch 15, Loss: 0.8356648683547974\n",
            "Epoch 8, Batch 16, Loss: 0.852507472038269\n",
            "Epoch 8, Batch 17, Loss: 0.6106714010238647\n",
            "Epoch 8, Batch 18, Loss: 0.8009802103042603\n",
            "Epoch 8, Batch 19, Loss: 1.1507160663604736\n",
            "Epoch 8, Batch 20, Loss: 1.0747113227844238\n",
            "Epoch 8, Batch 21, Loss: 0.5509220361709595\n",
            "Epoch 8, Batch 22, Loss: 0.6041418313980103\n",
            "Epoch 8, Batch 23, Loss: 0.7586240768432617\n",
            "Epoch 8, Batch 24, Loss: 0.596705436706543\n",
            "Epoch 8, Batch 25, Loss: 0.6967732310295105\n",
            "Epoch 8, Batch 26, Loss: 0.5880861878395081\n",
            "Epoch 8, Batch 27, Loss: 0.7948106527328491\n",
            "Epoch 8, Batch 28, Loss: 0.5552138090133667\n",
            "Epoch 8, Batch 29, Loss: 0.5272694826126099\n",
            "Epoch 8, Batch 30, Loss: 1.03561270236969\n",
            "Epoch 8, Batch 31, Loss: 0.7193277478218079\n",
            "Epoch 8, Batch 32, Loss: 0.45122769474983215\n",
            "Epoch 8, Batch 33, Loss: 0.6227343678474426\n",
            "Epoch 9, Batch 1, Loss: 0.5676877498626709\n",
            "Epoch 9, Batch 2, Loss: 0.6454982757568359\n",
            "Epoch 9, Batch 3, Loss: 0.47236567735671997\n",
            "Epoch 9, Batch 4, Loss: 0.9420493841171265\n",
            "Epoch 9, Batch 5, Loss: 0.7119315266609192\n",
            "Epoch 9, Batch 6, Loss: 0.9476619958877563\n",
            "Epoch 9, Batch 7, Loss: 0.6725565195083618\n",
            "Epoch 9, Batch 8, Loss: 0.5280700922012329\n",
            "Epoch 9, Batch 9, Loss: 1.0149997472763062\n",
            "Epoch 9, Batch 10, Loss: 0.9538426995277405\n",
            "Epoch 9, Batch 11, Loss: 0.6149130463600159\n",
            "Epoch 9, Batch 12, Loss: 0.45990151166915894\n",
            "Epoch 9, Batch 13, Loss: 0.6676597595214844\n",
            "Epoch 9, Batch 14, Loss: 0.8405981063842773\n",
            "Epoch 9, Batch 15, Loss: 0.780823826789856\n",
            "Epoch 9, Batch 16, Loss: 0.6851439476013184\n",
            "Epoch 9, Batch 17, Loss: 0.6342252492904663\n",
            "Epoch 9, Batch 18, Loss: 0.8807916045188904\n",
            "Epoch 9, Batch 19, Loss: 0.676749050617218\n",
            "Epoch 9, Batch 20, Loss: 0.7328312993049622\n",
            "Epoch 9, Batch 21, Loss: 0.882799506187439\n",
            "Epoch 9, Batch 22, Loss: 0.7142348289489746\n",
            "Epoch 9, Batch 23, Loss: 0.5760745406150818\n",
            "Epoch 9, Batch 24, Loss: 0.6888242959976196\n",
            "Epoch 9, Batch 25, Loss: 0.6315411925315857\n",
            "Epoch 9, Batch 26, Loss: 0.7177512645721436\n",
            "Epoch 9, Batch 27, Loss: 0.881316065788269\n",
            "Epoch 9, Batch 28, Loss: 0.6532325148582458\n",
            "Epoch 9, Batch 29, Loss: 1.2187596559524536\n",
            "Epoch 9, Batch 30, Loss: 0.7579019069671631\n",
            "Epoch 9, Batch 31, Loss: 0.6291182637214661\n",
            "Epoch 9, Batch 32, Loss: 0.8670850992202759\n",
            "Epoch 9, Batch 33, Loss: 0.7403965592384338\n",
            "Epoch 10, Batch 1, Loss: 0.7825513482093811\n",
            "Epoch 10, Batch 2, Loss: 0.6386407017707825\n",
            "Epoch 10, Batch 3, Loss: 0.7464722394943237\n",
            "Epoch 10, Batch 4, Loss: 0.9268629550933838\n",
            "Epoch 10, Batch 5, Loss: 0.8536546230316162\n",
            "Epoch 10, Batch 6, Loss: 0.7534053921699524\n",
            "Epoch 10, Batch 7, Loss: 1.457585334777832\n",
            "Epoch 10, Batch 8, Loss: 0.7138036489486694\n",
            "Epoch 10, Batch 9, Loss: 1.032742977142334\n",
            "Epoch 10, Batch 10, Loss: 0.7825171947479248\n",
            "Epoch 10, Batch 11, Loss: 0.7926160097122192\n",
            "Epoch 10, Batch 12, Loss: 0.8950487375259399\n",
            "Epoch 10, Batch 13, Loss: 0.6548238396644592\n",
            "Epoch 10, Batch 14, Loss: 0.5611871480941772\n",
            "Epoch 10, Batch 15, Loss: 0.6089767217636108\n",
            "Epoch 10, Batch 16, Loss: 0.9584664106369019\n",
            "Epoch 10, Batch 17, Loss: 0.8356146812438965\n",
            "Epoch 10, Batch 18, Loss: 0.5165244340896606\n",
            "Epoch 10, Batch 19, Loss: 0.7444888353347778\n",
            "Epoch 10, Batch 20, Loss: 0.8010752201080322\n",
            "Epoch 10, Batch 21, Loss: 0.5882425308227539\n",
            "Epoch 10, Batch 22, Loss: 0.83194899559021\n",
            "Epoch 10, Batch 23, Loss: 0.7667564749717712\n",
            "Epoch 10, Batch 24, Loss: 0.6068063378334045\n",
            "Epoch 10, Batch 25, Loss: 0.5653814077377319\n",
            "Epoch 10, Batch 26, Loss: 0.9848524332046509\n",
            "Epoch 10, Batch 27, Loss: 0.8518041372299194\n",
            "Epoch 10, Batch 28, Loss: 0.9541501998901367\n",
            "Epoch 10, Batch 29, Loss: 0.45709314942359924\n",
            "Epoch 10, Batch 30, Loss: 0.7662276029586792\n",
            "Epoch 10, Batch 31, Loss: 0.583766758441925\n",
            "Epoch 10, Batch 32, Loss: 0.8192457556724548\n",
            "Epoch 10, Batch 33, Loss: 0.9404686093330383\n",
            "Epoch 11, Batch 1, Loss: 0.6655822992324829\n",
            "Epoch 11, Batch 2, Loss: 0.6559456586837769\n",
            "Epoch 11, Batch 3, Loss: 0.6006474494934082\n",
            "Epoch 11, Batch 4, Loss: 0.5921584367752075\n",
            "Epoch 11, Batch 5, Loss: 0.6436905860900879\n",
            "Epoch 11, Batch 6, Loss: 0.40802693367004395\n",
            "Epoch 11, Batch 7, Loss: 0.7922619581222534\n",
            "Epoch 11, Batch 8, Loss: 0.8185378313064575\n",
            "Epoch 11, Batch 9, Loss: 0.6869456768035889\n",
            "Epoch 11, Batch 10, Loss: 0.7899647951126099\n",
            "Epoch 11, Batch 11, Loss: 0.7784958481788635\n",
            "Epoch 11, Batch 12, Loss: 0.6500025391578674\n",
            "Epoch 11, Batch 13, Loss: 0.9178823232650757\n",
            "Epoch 11, Batch 14, Loss: 0.6601725220680237\n",
            "Epoch 11, Batch 15, Loss: 0.8354024887084961\n",
            "Epoch 11, Batch 16, Loss: 0.7765719294548035\n",
            "Epoch 11, Batch 17, Loss: 0.6135928630828857\n",
            "Epoch 11, Batch 18, Loss: 0.792281448841095\n",
            "Epoch 11, Batch 19, Loss: 0.8025020360946655\n",
            "Epoch 11, Batch 20, Loss: 0.8492720723152161\n",
            "Epoch 11, Batch 21, Loss: 0.3127731382846832\n",
            "Epoch 11, Batch 22, Loss: 0.8130383491516113\n",
            "Epoch 11, Batch 23, Loss: 0.9276905059814453\n",
            "Epoch 11, Batch 24, Loss: 0.7787871956825256\n",
            "Epoch 11, Batch 25, Loss: 0.47067511081695557\n",
            "Epoch 11, Batch 26, Loss: 0.7876529693603516\n",
            "Epoch 11, Batch 27, Loss: 0.8526877760887146\n",
            "Epoch 11, Batch 28, Loss: 0.942999005317688\n",
            "Epoch 11, Batch 29, Loss: 0.8949555158615112\n",
            "Epoch 11, Batch 30, Loss: 0.6637617945671082\n",
            "Epoch 11, Batch 31, Loss: 0.601874053478241\n",
            "Epoch 11, Batch 32, Loss: 0.5732110738754272\n",
            "Epoch 11, Batch 33, Loss: 0.6659013628959656\n",
            "Epoch 12, Batch 1, Loss: 0.7395345568656921\n",
            "Epoch 12, Batch 2, Loss: 0.6244179010391235\n",
            "Epoch 12, Batch 3, Loss: 0.6289756298065186\n",
            "Epoch 12, Batch 4, Loss: 0.6685999631881714\n",
            "Epoch 12, Batch 5, Loss: 0.6102969646453857\n",
            "Epoch 12, Batch 6, Loss: 0.7168086767196655\n",
            "Epoch 12, Batch 7, Loss: 0.6759552359580994\n",
            "Epoch 12, Batch 8, Loss: 0.5859397649765015\n",
            "Epoch 12, Batch 9, Loss: 0.8772833347320557\n",
            "Epoch 12, Batch 10, Loss: 0.8138099908828735\n",
            "Epoch 12, Batch 11, Loss: 0.7073283791542053\n",
            "Epoch 12, Batch 12, Loss: 0.958225667476654\n",
            "Epoch 12, Batch 13, Loss: 0.7833707928657532\n",
            "Epoch 12, Batch 14, Loss: 0.6656676530838013\n",
            "Epoch 12, Batch 15, Loss: 0.4025726318359375\n",
            "Epoch 12, Batch 16, Loss: 1.057557463645935\n",
            "Epoch 12, Batch 17, Loss: 0.8738744258880615\n",
            "Epoch 12, Batch 18, Loss: 0.5187934637069702\n",
            "Epoch 12, Batch 19, Loss: 0.6599209308624268\n",
            "Epoch 12, Batch 20, Loss: 0.6027919054031372\n",
            "Epoch 12, Batch 21, Loss: 0.7685079574584961\n",
            "Epoch 12, Batch 22, Loss: 0.41922885179519653\n",
            "Epoch 12, Batch 23, Loss: 0.6696092486381531\n",
            "Epoch 12, Batch 24, Loss: 0.597398042678833\n",
            "Epoch 12, Batch 25, Loss: 0.5104743242263794\n",
            "Epoch 12, Batch 26, Loss: 0.7534317374229431\n",
            "Epoch 12, Batch 27, Loss: 0.6454533338546753\n",
            "Epoch 12, Batch 28, Loss: 1.0392463207244873\n",
            "Epoch 12, Batch 29, Loss: 0.6980035305023193\n",
            "Epoch 12, Batch 30, Loss: 0.9031210541725159\n",
            "Epoch 12, Batch 31, Loss: 0.9910411834716797\n",
            "Epoch 12, Batch 32, Loss: 0.49149763584136963\n",
            "Epoch 12, Batch 33, Loss: 0.6212981939315796\n",
            "Epoch 13, Batch 1, Loss: 0.9403483867645264\n",
            "Epoch 13, Batch 2, Loss: 0.7746076583862305\n",
            "Epoch 13, Batch 3, Loss: 0.5236146450042725\n",
            "Epoch 13, Batch 4, Loss: 0.4329485595226288\n",
            "Epoch 13, Batch 5, Loss: 0.8738476037979126\n",
            "Epoch 13, Batch 6, Loss: 0.8400851488113403\n",
            "Epoch 13, Batch 7, Loss: 0.8659064173698425\n",
            "Epoch 13, Batch 8, Loss: 0.45094364881515503\n",
            "Epoch 13, Batch 9, Loss: 0.7787690758705139\n",
            "Epoch 13, Batch 10, Loss: 0.732120156288147\n",
            "Epoch 13, Batch 11, Loss: 0.43061548471450806\n",
            "Epoch 13, Batch 12, Loss: 0.4339200258255005\n",
            "Epoch 13, Batch 13, Loss: 0.5015795230865479\n",
            "Epoch 13, Batch 14, Loss: 0.8075025081634521\n",
            "Epoch 13, Batch 15, Loss: 0.9776872992515564\n",
            "Epoch 13, Batch 16, Loss: 0.7592334747314453\n",
            "Epoch 13, Batch 17, Loss: 0.607798159122467\n",
            "Epoch 13, Batch 18, Loss: 1.0477195978164673\n",
            "Epoch 13, Batch 19, Loss: 0.610998272895813\n",
            "Epoch 13, Batch 20, Loss: 1.0335497856140137\n",
            "Epoch 13, Batch 21, Loss: 1.0150914192199707\n",
            "Epoch 13, Batch 22, Loss: 0.9405803084373474\n",
            "Epoch 13, Batch 23, Loss: 0.6667357087135315\n",
            "Epoch 13, Batch 24, Loss: 0.8136700391769409\n",
            "Epoch 13, Batch 25, Loss: 1.2502639293670654\n",
            "Epoch 13, Batch 26, Loss: 0.9198993444442749\n",
            "Epoch 13, Batch 27, Loss: 0.6213207244873047\n",
            "Epoch 13, Batch 28, Loss: 0.6578106880187988\n",
            "Epoch 13, Batch 29, Loss: 0.7048090696334839\n",
            "Epoch 13, Batch 30, Loss: 0.7007582783699036\n",
            "Epoch 13, Batch 31, Loss: 0.9135737419128418\n",
            "Epoch 13, Batch 32, Loss: 0.5585205554962158\n",
            "Epoch 13, Batch 33, Loss: 0.6659849882125854\n",
            "Epoch 14, Batch 1, Loss: 0.5508855581283569\n",
            "Epoch 14, Batch 2, Loss: 0.6154639720916748\n",
            "Epoch 14, Batch 3, Loss: 0.722404420375824\n",
            "Epoch 14, Batch 4, Loss: 0.7556261420249939\n",
            "Epoch 14, Batch 5, Loss: 0.7252142429351807\n",
            "Epoch 14, Batch 6, Loss: 1.041308879852295\n",
            "Epoch 14, Batch 7, Loss: 0.6652274131774902\n",
            "Epoch 14, Batch 8, Loss: 0.5210609436035156\n",
            "Epoch 14, Batch 9, Loss: 0.6735578775405884\n",
            "Epoch 14, Batch 10, Loss: 0.5404459238052368\n",
            "Epoch 14, Batch 11, Loss: 0.6170094609260559\n",
            "Epoch 14, Batch 12, Loss: 0.6990967988967896\n",
            "Epoch 14, Batch 13, Loss: 0.6681007146835327\n",
            "Epoch 14, Batch 14, Loss: 0.8036798238754272\n",
            "Epoch 14, Batch 15, Loss: 0.6050212979316711\n",
            "Epoch 14, Batch 16, Loss: 0.7699742317199707\n",
            "Epoch 14, Batch 17, Loss: 0.5026738047599792\n",
            "Epoch 14, Batch 18, Loss: 0.9173544049263\n",
            "Epoch 14, Batch 19, Loss: 0.6923525929450989\n",
            "Epoch 14, Batch 20, Loss: 0.5415319800376892\n",
            "Epoch 14, Batch 21, Loss: 0.677980899810791\n",
            "Epoch 14, Batch 22, Loss: 0.6042202711105347\n",
            "Epoch 14, Batch 23, Loss: 0.7546165585517883\n",
            "Epoch 14, Batch 24, Loss: 0.6496580243110657\n",
            "Epoch 14, Batch 25, Loss: 0.4549412727355957\n",
            "Epoch 14, Batch 26, Loss: 0.7978189587593079\n",
            "Epoch 14, Batch 27, Loss: 0.7855104207992554\n",
            "Epoch 14, Batch 28, Loss: 1.097632884979248\n",
            "Epoch 14, Batch 29, Loss: 0.6797444820404053\n",
            "Epoch 14, Batch 30, Loss: 0.6872467994689941\n",
            "Epoch 14, Batch 31, Loss: 0.8064241409301758\n",
            "Epoch 14, Batch 32, Loss: 0.6363416910171509\n",
            "Epoch 14, Batch 33, Loss: 1.216710090637207\n",
            "Epoch 15, Batch 1, Loss: 0.5494683384895325\n",
            "Epoch 15, Batch 2, Loss: 0.7759518623352051\n",
            "Epoch 15, Batch 3, Loss: 1.277048945426941\n",
            "Epoch 15, Batch 4, Loss: 0.889470100402832\n",
            "Epoch 15, Batch 5, Loss: 0.5347623229026794\n",
            "Epoch 15, Batch 6, Loss: 0.8978188037872314\n",
            "Epoch 15, Batch 7, Loss: 0.7866391539573669\n",
            "Epoch 15, Batch 8, Loss: 0.9077868461608887\n",
            "Epoch 15, Batch 9, Loss: 0.6096117496490479\n",
            "Epoch 15, Batch 10, Loss: 0.5509035587310791\n",
            "Epoch 15, Batch 11, Loss: 0.7848994731903076\n",
            "Epoch 15, Batch 12, Loss: 0.8595836162567139\n",
            "Epoch 15, Batch 13, Loss: 0.6512138247489929\n",
            "Epoch 15, Batch 14, Loss: 0.7465232610702515\n",
            "Epoch 15, Batch 15, Loss: 0.6861906051635742\n",
            "Epoch 15, Batch 16, Loss: 0.6226177215576172\n",
            "Epoch 15, Batch 17, Loss: 0.4923754036426544\n",
            "Epoch 15, Batch 18, Loss: 0.6440213918685913\n",
            "Epoch 15, Batch 19, Loss: 0.7166086435317993\n",
            "Epoch 15, Batch 20, Loss: 0.6027700304985046\n",
            "Epoch 15, Batch 21, Loss: 0.5638702511787415\n",
            "Epoch 15, Batch 22, Loss: 0.6970377564430237\n",
            "Epoch 15, Batch 23, Loss: 0.715570330619812\n",
            "Epoch 15, Batch 24, Loss: 0.8154807090759277\n",
            "Epoch 15, Batch 25, Loss: 0.6778279542922974\n",
            "Epoch 15, Batch 26, Loss: 0.9170973896980286\n",
            "Epoch 15, Batch 27, Loss: 0.8527863621711731\n",
            "Epoch 15, Batch 28, Loss: 1.077877402305603\n",
            "Epoch 15, Batch 29, Loss: 0.639683723449707\n",
            "Epoch 15, Batch 30, Loss: 0.5839505791664124\n",
            "Epoch 15, Batch 31, Loss: 0.6718012690544128\n",
            "Epoch 15, Batch 32, Loss: 0.6289029121398926\n",
            "Epoch 15, Batch 33, Loss: 0.3100528419017792\n",
            "RMSE for this set: 0.7785940308011187\n",
            "Training with dropout=0.75, lr=0.01, lstm_H=300\n",
            "Epoch 1, Batch 1, Loss: 7.294580459594727\n",
            "Epoch 1, Batch 2, Loss: 9.838838577270508\n",
            "Epoch 1, Batch 3, Loss: 13.587125778198242\n",
            "Epoch 1, Batch 4, Loss: 148.0201416015625\n",
            "Epoch 1, Batch 5, Loss: 5.69886589050293\n",
            "Epoch 1, Batch 6, Loss: 0.6650582551956177\n",
            "Epoch 1, Batch 7, Loss: 0.8918095231056213\n",
            "Epoch 1, Batch 8, Loss: 1.2653559446334839\n",
            "Epoch 1, Batch 9, Loss: 0.5320514440536499\n",
            "Epoch 1, Batch 10, Loss: 1.047379970550537\n",
            "Epoch 1, Batch 11, Loss: 1.0930747985839844\n",
            "Epoch 1, Batch 12, Loss: 0.9545238018035889\n",
            "Epoch 1, Batch 13, Loss: 1.089848518371582\n",
            "Epoch 1, Batch 14, Loss: 0.9793528318405151\n",
            "Epoch 1, Batch 15, Loss: 1.399306297302246\n",
            "Epoch 1, Batch 16, Loss: 1.1129567623138428\n",
            "Epoch 1, Batch 17, Loss: 1.147470235824585\n",
            "Epoch 1, Batch 18, Loss: 0.8318183422088623\n",
            "Epoch 1, Batch 19, Loss: 0.6542943716049194\n",
            "Epoch 1, Batch 20, Loss: 1.6398576498031616\n",
            "Epoch 1, Batch 21, Loss: 1.805988073348999\n",
            "Epoch 1, Batch 22, Loss: 1.0213706493377686\n",
            "Epoch 1, Batch 23, Loss: 0.7831315994262695\n",
            "Epoch 1, Batch 24, Loss: 1.5328192710876465\n",
            "Epoch 1, Batch 25, Loss: 0.8230313658714294\n",
            "Epoch 1, Batch 26, Loss: 0.813584566116333\n",
            "Epoch 1, Batch 27, Loss: 0.658622145652771\n",
            "Epoch 1, Batch 28, Loss: 0.5791255235671997\n",
            "Epoch 1, Batch 29, Loss: 0.9336363673210144\n",
            "Epoch 1, Batch 30, Loss: 0.874498188495636\n",
            "Epoch 1, Batch 31, Loss: 0.859764814376831\n",
            "Epoch 1, Batch 32, Loss: 0.9654729962348938\n",
            "Epoch 1, Batch 33, Loss: 1.0561292171478271\n",
            "Epoch 2, Batch 1, Loss: 0.9114155769348145\n",
            "Epoch 2, Batch 2, Loss: 0.8580049276351929\n",
            "Epoch 2, Batch 3, Loss: 0.7090499997138977\n",
            "Epoch 2, Batch 4, Loss: 0.9296737909317017\n",
            "Epoch 2, Batch 5, Loss: 0.8205671310424805\n",
            "Epoch 2, Batch 6, Loss: 0.7001680731773376\n",
            "Epoch 2, Batch 7, Loss: 0.8280056715011597\n",
            "Epoch 2, Batch 8, Loss: 0.850170373916626\n",
            "Epoch 2, Batch 9, Loss: 0.8964300155639648\n",
            "Epoch 2, Batch 10, Loss: 0.5934336185455322\n",
            "Epoch 2, Batch 11, Loss: 0.6999001502990723\n",
            "Epoch 2, Batch 12, Loss: 0.6707320213317871\n",
            "Epoch 2, Batch 13, Loss: 0.683344841003418\n",
            "Epoch 2, Batch 14, Loss: 0.5256383419036865\n",
            "Epoch 2, Batch 15, Loss: 1.2662408351898193\n",
            "Epoch 2, Batch 16, Loss: 1.0098695755004883\n",
            "Epoch 2, Batch 17, Loss: 0.7200764417648315\n",
            "Epoch 2, Batch 18, Loss: 0.5989684462547302\n",
            "Epoch 2, Batch 19, Loss: 0.7582812309265137\n",
            "Epoch 2, Batch 20, Loss: 0.5140954256057739\n",
            "Epoch 2, Batch 21, Loss: 1.4943654537200928\n",
            "Epoch 2, Batch 22, Loss: 1.2005183696746826\n",
            "Epoch 2, Batch 23, Loss: 0.5555307865142822\n",
            "Epoch 2, Batch 24, Loss: 0.4714367687702179\n",
            "Epoch 2, Batch 25, Loss: 0.815700888633728\n",
            "Epoch 2, Batch 26, Loss: 0.9237470030784607\n",
            "Epoch 2, Batch 27, Loss: 0.7643417716026306\n",
            "Epoch 2, Batch 28, Loss: 0.8405765295028687\n",
            "Epoch 2, Batch 29, Loss: 0.39285916090011597\n",
            "Epoch 2, Batch 30, Loss: 0.8962652683258057\n",
            "Epoch 2, Batch 31, Loss: 0.515455424785614\n",
            "Epoch 2, Batch 32, Loss: 1.146268367767334\n",
            "Epoch 2, Batch 33, Loss: 0.734225869178772\n",
            "Epoch 3, Batch 1, Loss: 0.7210444211959839\n",
            "Epoch 3, Batch 2, Loss: 0.7376985549926758\n",
            "Epoch 3, Batch 3, Loss: 0.5747417211532593\n",
            "Epoch 3, Batch 4, Loss: 0.8033046126365662\n",
            "Epoch 3, Batch 5, Loss: 0.7636909484863281\n",
            "Epoch 3, Batch 6, Loss: 0.8046014308929443\n",
            "Epoch 3, Batch 7, Loss: 0.8003039360046387\n",
            "Epoch 3, Batch 8, Loss: 0.9672509431838989\n",
            "Epoch 3, Batch 9, Loss: 0.4301934242248535\n",
            "Epoch 3, Batch 10, Loss: 0.5685653686523438\n",
            "Epoch 3, Batch 11, Loss: 0.4727986752986908\n",
            "Epoch 3, Batch 12, Loss: 1.1010369062423706\n",
            "Epoch 3, Batch 13, Loss: 0.6812597513198853\n",
            "Epoch 3, Batch 14, Loss: 0.644951581954956\n",
            "Epoch 3, Batch 15, Loss: 0.761796236038208\n",
            "Epoch 3, Batch 16, Loss: 1.0051894187927246\n",
            "Epoch 3, Batch 17, Loss: 1.286193609237671\n",
            "Epoch 3, Batch 18, Loss: 0.6921128034591675\n",
            "Epoch 3, Batch 19, Loss: 0.7949899435043335\n",
            "Epoch 3, Batch 20, Loss: 0.6811876893043518\n",
            "Epoch 3, Batch 21, Loss: 0.8409326672554016\n",
            "Epoch 3, Batch 22, Loss: 0.5713837146759033\n",
            "Epoch 3, Batch 23, Loss: 0.8491712808609009\n",
            "Epoch 3, Batch 24, Loss: 0.6124826073646545\n",
            "Epoch 3, Batch 25, Loss: 0.4768199324607849\n",
            "Epoch 3, Batch 26, Loss: 0.6872406005859375\n",
            "Epoch 3, Batch 27, Loss: 0.9302961230278015\n",
            "Epoch 3, Batch 28, Loss: 0.941817045211792\n",
            "Epoch 3, Batch 29, Loss: 0.4553060531616211\n",
            "Epoch 3, Batch 30, Loss: 0.6506819725036621\n",
            "Epoch 3, Batch 31, Loss: 0.6471778154373169\n",
            "Epoch 3, Batch 32, Loss: 0.5804577469825745\n",
            "Epoch 3, Batch 33, Loss: 0.8122297525405884\n",
            "Epoch 4, Batch 1, Loss: 0.4760415852069855\n",
            "Epoch 4, Batch 2, Loss: 0.9038182497024536\n",
            "Epoch 4, Batch 3, Loss: 0.6877404451370239\n",
            "Epoch 4, Batch 4, Loss: 1.0381476879119873\n",
            "Epoch 4, Batch 5, Loss: 0.6654120087623596\n",
            "Epoch 4, Batch 6, Loss: 0.8991583585739136\n",
            "Epoch 4, Batch 7, Loss: 0.8123128414154053\n",
            "Epoch 4, Batch 8, Loss: 0.9597761631011963\n",
            "Epoch 4, Batch 9, Loss: 0.8887462615966797\n",
            "Epoch 4, Batch 10, Loss: 0.7108997106552124\n",
            "Epoch 4, Batch 11, Loss: 0.5480376482009888\n",
            "Epoch 4, Batch 12, Loss: 0.7111449241638184\n",
            "Epoch 4, Batch 13, Loss: 0.6400899291038513\n",
            "Epoch 4, Batch 14, Loss: 0.6892836689949036\n",
            "Epoch 4, Batch 15, Loss: 0.8676298260688782\n",
            "Epoch 4, Batch 16, Loss: 0.39225703477859497\n",
            "Epoch 4, Batch 17, Loss: 0.8352252840995789\n",
            "Epoch 4, Batch 18, Loss: 0.6778578758239746\n",
            "Epoch 4, Batch 19, Loss: 0.7398982644081116\n",
            "Epoch 4, Batch 20, Loss: 0.9350611567497253\n",
            "Epoch 4, Batch 21, Loss: 0.6169496774673462\n",
            "Epoch 4, Batch 22, Loss: 0.7077997922897339\n",
            "Epoch 4, Batch 23, Loss: 0.7009547352790833\n",
            "Epoch 4, Batch 24, Loss: 0.6654757857322693\n",
            "Epoch 4, Batch 25, Loss: 0.6073828935623169\n",
            "Epoch 4, Batch 26, Loss: 0.5774693489074707\n",
            "Epoch 4, Batch 27, Loss: 0.658855676651001\n",
            "Epoch 4, Batch 28, Loss: 0.5094775557518005\n",
            "Epoch 4, Batch 29, Loss: 0.5354386568069458\n",
            "Epoch 4, Batch 30, Loss: 0.5870614051818848\n",
            "Epoch 4, Batch 31, Loss: 0.6359223127365112\n",
            "Epoch 4, Batch 32, Loss: 0.830364465713501\n",
            "Epoch 4, Batch 33, Loss: 0.6553070545196533\n",
            "Epoch 5, Batch 1, Loss: 0.671934962272644\n",
            "Epoch 5, Batch 2, Loss: 0.690510094165802\n",
            "Epoch 5, Batch 3, Loss: 0.7858989238739014\n",
            "Epoch 5, Batch 4, Loss: 0.9091912508010864\n",
            "Epoch 5, Batch 5, Loss: 0.6478241682052612\n",
            "Epoch 5, Batch 6, Loss: 0.6817169785499573\n",
            "Epoch 5, Batch 7, Loss: 0.9567966461181641\n",
            "Epoch 5, Batch 8, Loss: 1.179247498512268\n",
            "Epoch 5, Batch 9, Loss: 0.7595087885856628\n",
            "Epoch 5, Batch 10, Loss: 0.8698220252990723\n",
            "Epoch 5, Batch 11, Loss: 0.4769664406776428\n",
            "Epoch 5, Batch 12, Loss: 0.6170390844345093\n",
            "Epoch 5, Batch 13, Loss: 0.990600049495697\n",
            "Epoch 5, Batch 14, Loss: 0.7092140316963196\n",
            "Epoch 5, Batch 15, Loss: 0.6001865863800049\n",
            "Epoch 5, Batch 16, Loss: 0.562454104423523\n",
            "Epoch 5, Batch 17, Loss: 0.9585816264152527\n",
            "Epoch 5, Batch 18, Loss: 1.0149893760681152\n",
            "Epoch 5, Batch 19, Loss: 0.9103432893753052\n",
            "Epoch 5, Batch 20, Loss: 0.6733033657073975\n",
            "Epoch 5, Batch 21, Loss: 0.8247510194778442\n",
            "Epoch 5, Batch 22, Loss: 0.6350427865982056\n",
            "Epoch 5, Batch 23, Loss: 0.6099115610122681\n",
            "Epoch 5, Batch 24, Loss: 0.7697489261627197\n",
            "Epoch 5, Batch 25, Loss: 0.8960765600204468\n",
            "Epoch 5, Batch 26, Loss: 0.6449615359306335\n",
            "Epoch 5, Batch 27, Loss: 0.8032735586166382\n",
            "Epoch 5, Batch 28, Loss: 0.6501102447509766\n",
            "Epoch 5, Batch 29, Loss: 0.5542272329330444\n",
            "Epoch 5, Batch 30, Loss: 0.6140435338020325\n",
            "Epoch 5, Batch 31, Loss: 0.5201358199119568\n",
            "Epoch 5, Batch 32, Loss: 0.9069678783416748\n",
            "Epoch 5, Batch 33, Loss: 0.8041833639144897\n",
            "Epoch 6, Batch 1, Loss: 0.9216692447662354\n",
            "Epoch 6, Batch 2, Loss: 0.7105292081832886\n",
            "Epoch 6, Batch 3, Loss: 0.7940152883529663\n",
            "Epoch 6, Batch 4, Loss: 0.6748955249786377\n",
            "Epoch 6, Batch 5, Loss: 0.7702291011810303\n",
            "Epoch 6, Batch 6, Loss: 0.9639005661010742\n",
            "Epoch 6, Batch 7, Loss: 0.7283688187599182\n",
            "Epoch 6, Batch 8, Loss: 0.43782269954681396\n",
            "Epoch 6, Batch 9, Loss: 0.6026378273963928\n",
            "Epoch 6, Batch 10, Loss: 0.7035729289054871\n",
            "Epoch 6, Batch 11, Loss: 0.868227481842041\n",
            "Epoch 6, Batch 12, Loss: 0.8379960060119629\n",
            "Epoch 6, Batch 13, Loss: 0.7329467535018921\n",
            "Epoch 6, Batch 14, Loss: 0.8369200229644775\n",
            "Epoch 6, Batch 15, Loss: 0.64093017578125\n",
            "Epoch 6, Batch 16, Loss: 0.8263642191886902\n",
            "Epoch 6, Batch 17, Loss: 0.627011239528656\n",
            "Epoch 6, Batch 18, Loss: 1.1085327863693237\n",
            "Epoch 6, Batch 19, Loss: 0.6900696754455566\n",
            "Epoch 6, Batch 20, Loss: 0.6898019313812256\n",
            "Epoch 6, Batch 21, Loss: 0.6693228483200073\n",
            "Epoch 6, Batch 22, Loss: 0.555944561958313\n",
            "Epoch 6, Batch 23, Loss: 0.7447617053985596\n",
            "Epoch 6, Batch 24, Loss: 0.7056056261062622\n",
            "Epoch 6, Batch 25, Loss: 0.6288455724716187\n",
            "Epoch 6, Batch 26, Loss: 0.8009656071662903\n",
            "Epoch 6, Batch 27, Loss: 0.4963071048259735\n",
            "Epoch 6, Batch 28, Loss: 0.7077642679214478\n",
            "Epoch 6, Batch 29, Loss: 0.6040076017379761\n",
            "Epoch 6, Batch 30, Loss: 0.551550567150116\n",
            "Epoch 6, Batch 31, Loss: 0.6300549507141113\n",
            "Epoch 6, Batch 32, Loss: 0.8116822838783264\n",
            "Epoch 6, Batch 33, Loss: 0.8085819482803345\n",
            "Epoch 7, Batch 1, Loss: 0.9278748035430908\n",
            "Epoch 7, Batch 2, Loss: 0.6235536932945251\n",
            "Epoch 7, Batch 3, Loss: 0.72011399269104\n",
            "Epoch 7, Batch 4, Loss: 0.7310188412666321\n",
            "Epoch 7, Batch 5, Loss: 0.5761393308639526\n",
            "Epoch 7, Batch 6, Loss: 0.8051811456680298\n",
            "Epoch 7, Batch 7, Loss: 0.6391702890396118\n",
            "Epoch 7, Batch 8, Loss: 1.0111078023910522\n",
            "Epoch 7, Batch 9, Loss: 0.9889179468154907\n",
            "Epoch 7, Batch 10, Loss: 0.5166783928871155\n",
            "Epoch 7, Batch 11, Loss: 0.45745933055877686\n",
            "Epoch 7, Batch 12, Loss: 0.8837951421737671\n",
            "Epoch 7, Batch 13, Loss: 0.6337449550628662\n",
            "Epoch 7, Batch 14, Loss: 0.7331426739692688\n",
            "Epoch 7, Batch 15, Loss: 0.8383376598358154\n",
            "Epoch 7, Batch 16, Loss: 0.6139858365058899\n",
            "Epoch 7, Batch 17, Loss: 0.7226223349571228\n",
            "Epoch 7, Batch 18, Loss: 0.9355279803276062\n",
            "Epoch 7, Batch 19, Loss: 0.5617778897285461\n",
            "Epoch 7, Batch 20, Loss: 0.5899211168289185\n",
            "Epoch 7, Batch 21, Loss: 0.7112517952919006\n",
            "Epoch 7, Batch 22, Loss: 0.8404186367988586\n",
            "Epoch 7, Batch 23, Loss: 0.6493825912475586\n",
            "Epoch 7, Batch 24, Loss: 0.5490365028381348\n",
            "Epoch 7, Batch 25, Loss: 0.7457396388053894\n",
            "Epoch 7, Batch 26, Loss: 0.8782341480255127\n",
            "Epoch 7, Batch 27, Loss: 0.5115295648574829\n",
            "Epoch 7, Batch 28, Loss: 1.022254228591919\n",
            "Epoch 7, Batch 29, Loss: 0.6407796144485474\n",
            "Epoch 7, Batch 30, Loss: 0.5165753364562988\n",
            "Epoch 7, Batch 31, Loss: 0.46828949451446533\n",
            "Epoch 7, Batch 32, Loss: 0.5623786449432373\n",
            "Epoch 7, Batch 33, Loss: 0.4631311297416687\n",
            "Epoch 8, Batch 1, Loss: 0.8163691759109497\n",
            "Epoch 8, Batch 2, Loss: 0.6463807821273804\n",
            "Epoch 8, Batch 3, Loss: 0.8168048858642578\n",
            "Epoch 8, Batch 4, Loss: 0.7359688878059387\n",
            "Epoch 8, Batch 5, Loss: 0.6221343874931335\n",
            "Epoch 8, Batch 6, Loss: 0.852259635925293\n",
            "Epoch 8, Batch 7, Loss: 0.4514463245868683\n",
            "Epoch 8, Batch 8, Loss: 0.6022616624832153\n",
            "Epoch 8, Batch 9, Loss: 0.746256947517395\n",
            "Epoch 8, Batch 10, Loss: 0.687854528427124\n",
            "Epoch 8, Batch 11, Loss: 0.6818669438362122\n",
            "Epoch 8, Batch 12, Loss: 0.9678157567977905\n",
            "Epoch 8, Batch 13, Loss: 0.5103943347930908\n",
            "Epoch 8, Batch 14, Loss: 0.434673547744751\n",
            "Epoch 8, Batch 15, Loss: 0.6089141368865967\n",
            "Epoch 8, Batch 16, Loss: 0.8388133645057678\n",
            "Epoch 8, Batch 17, Loss: 0.6699286103248596\n",
            "Epoch 8, Batch 18, Loss: 0.6336364150047302\n",
            "Epoch 8, Batch 19, Loss: 0.7746115922927856\n",
            "Epoch 8, Batch 20, Loss: 0.6402782201766968\n",
            "Epoch 8, Batch 21, Loss: 1.0653102397918701\n",
            "Epoch 8, Batch 22, Loss: 0.7614315748214722\n",
            "Epoch 8, Batch 23, Loss: 0.7419896125793457\n",
            "Epoch 8, Batch 24, Loss: 0.6009534597396851\n",
            "Epoch 8, Batch 25, Loss: 0.5721719861030579\n",
            "Epoch 8, Batch 26, Loss: 0.6082605123519897\n",
            "Epoch 8, Batch 27, Loss: 0.6958001852035522\n",
            "Epoch 8, Batch 28, Loss: 0.5494624376296997\n",
            "Epoch 8, Batch 29, Loss: 0.8756049275398254\n",
            "Epoch 8, Batch 30, Loss: 0.7303662300109863\n",
            "Epoch 8, Batch 31, Loss: 0.8056875467300415\n",
            "Epoch 8, Batch 32, Loss: 0.6561962366104126\n",
            "Epoch 8, Batch 33, Loss: 0.7386466264724731\n",
            "Epoch 9, Batch 1, Loss: 0.7195477485656738\n",
            "Epoch 9, Batch 2, Loss: 0.795929491519928\n",
            "Epoch 9, Batch 3, Loss: 0.80367112159729\n",
            "Epoch 9, Batch 4, Loss: 0.8100098371505737\n",
            "Epoch 9, Batch 5, Loss: 0.6333743333816528\n",
            "Epoch 9, Batch 6, Loss: 0.8097105026245117\n",
            "Epoch 9, Batch 7, Loss: 0.6880970001220703\n",
            "Epoch 9, Batch 8, Loss: 0.6077008247375488\n",
            "Epoch 9, Batch 9, Loss: 0.9556782245635986\n",
            "Epoch 9, Batch 10, Loss: 0.7017480134963989\n",
            "Epoch 9, Batch 11, Loss: 0.711086630821228\n",
            "Epoch 9, Batch 12, Loss: 0.5993252992630005\n",
            "Epoch 9, Batch 13, Loss: 0.49360787868499756\n",
            "Epoch 9, Batch 14, Loss: 0.8285436630249023\n",
            "Epoch 9, Batch 15, Loss: 0.7439674735069275\n",
            "Epoch 9, Batch 16, Loss: 0.7338265180587769\n",
            "Epoch 9, Batch 17, Loss: 0.7033492922782898\n",
            "Epoch 9, Batch 18, Loss: 0.591730535030365\n",
            "Epoch 9, Batch 19, Loss: 0.799784243106842\n",
            "Epoch 9, Batch 20, Loss: 0.8294634819030762\n",
            "Epoch 9, Batch 21, Loss: 0.7032204866409302\n",
            "Epoch 9, Batch 22, Loss: 0.8233181238174438\n",
            "Epoch 9, Batch 23, Loss: 0.7009333372116089\n",
            "Epoch 9, Batch 24, Loss: 0.8622679710388184\n",
            "Epoch 9, Batch 25, Loss: 0.651930570602417\n",
            "Epoch 9, Batch 26, Loss: 0.6957602500915527\n",
            "Epoch 9, Batch 27, Loss: 0.8043705224990845\n",
            "Epoch 9, Batch 28, Loss: 0.6995715498924255\n",
            "Epoch 9, Batch 29, Loss: 0.5356625914573669\n",
            "Epoch 9, Batch 30, Loss: 0.669219970703125\n",
            "Epoch 9, Batch 31, Loss: 0.6227939128875732\n",
            "Epoch 9, Batch 32, Loss: 0.7641860246658325\n",
            "Epoch 9, Batch 33, Loss: 0.6722677946090698\n",
            "Epoch 10, Batch 1, Loss: 0.9982004165649414\n",
            "Epoch 10, Batch 2, Loss: 0.7613718509674072\n",
            "Epoch 10, Batch 3, Loss: 0.5702753067016602\n",
            "Epoch 10, Batch 4, Loss: 0.7848556041717529\n",
            "Epoch 10, Batch 5, Loss: 0.8292216062545776\n",
            "Epoch 10, Batch 6, Loss: 0.7313731908798218\n",
            "Epoch 10, Batch 7, Loss: 0.6702820658683777\n",
            "Epoch 10, Batch 8, Loss: 0.5082950592041016\n",
            "Epoch 10, Batch 9, Loss: 0.8100070357322693\n",
            "Epoch 10, Batch 10, Loss: 0.7956515550613403\n",
            "Epoch 10, Batch 11, Loss: 0.8097106218338013\n",
            "Epoch 10, Batch 12, Loss: 0.4897526502609253\n",
            "Epoch 10, Batch 13, Loss: 0.7864079475402832\n",
            "Epoch 10, Batch 14, Loss: 0.5516294240951538\n",
            "Epoch 10, Batch 15, Loss: 0.8156936168670654\n",
            "Epoch 10, Batch 16, Loss: 0.6446703672409058\n",
            "Epoch 10, Batch 17, Loss: 0.7121204137802124\n",
            "Epoch 10, Batch 18, Loss: 0.6506198644638062\n",
            "Epoch 10, Batch 19, Loss: 0.5491873025894165\n",
            "Epoch 10, Batch 20, Loss: 0.5836256742477417\n",
            "Epoch 10, Batch 21, Loss: 0.8430428504943848\n",
            "Epoch 10, Batch 22, Loss: 0.9242691993713379\n",
            "Epoch 10, Batch 23, Loss: 1.0121326446533203\n",
            "Epoch 10, Batch 24, Loss: 0.6398031115531921\n",
            "Epoch 10, Batch 25, Loss: 0.3894100487232208\n",
            "Epoch 10, Batch 26, Loss: 0.8024141788482666\n",
            "Epoch 10, Batch 27, Loss: 0.5880371332168579\n",
            "Epoch 10, Batch 28, Loss: 0.604499876499176\n",
            "Epoch 10, Batch 29, Loss: 0.7561591863632202\n",
            "Epoch 10, Batch 30, Loss: 0.5581386089324951\n",
            "Epoch 10, Batch 31, Loss: 0.7212177515029907\n",
            "Epoch 10, Batch 32, Loss: 0.47968342900276184\n",
            "Epoch 10, Batch 33, Loss: 0.7525559663772583\n",
            "Epoch 11, Batch 1, Loss: 0.586807906627655\n",
            "Epoch 11, Batch 2, Loss: 0.7030863761901855\n",
            "Epoch 11, Batch 3, Loss: 0.7418432235717773\n",
            "Epoch 11, Batch 4, Loss: 0.5630605220794678\n",
            "Epoch 11, Batch 5, Loss: 0.5722317099571228\n",
            "Epoch 11, Batch 6, Loss: 0.6235544681549072\n",
            "Epoch 11, Batch 7, Loss: 0.3254384398460388\n",
            "Epoch 11, Batch 8, Loss: 0.63231360912323\n",
            "Epoch 11, Batch 9, Loss: 0.7526428699493408\n",
            "Epoch 11, Batch 10, Loss: 0.5282713174819946\n",
            "Epoch 11, Batch 11, Loss: 0.6851412057876587\n",
            "Epoch 11, Batch 12, Loss: 0.7907080054283142\n",
            "Epoch 11, Batch 13, Loss: 0.6337606906890869\n",
            "Epoch 11, Batch 14, Loss: 0.7344673871994019\n",
            "Epoch 11, Batch 15, Loss: 0.8065283298492432\n",
            "Epoch 11, Batch 16, Loss: 0.6086512207984924\n",
            "Epoch 11, Batch 17, Loss: 0.9245684146881104\n",
            "Epoch 11, Batch 18, Loss: 0.6026109457015991\n",
            "Epoch 11, Batch 19, Loss: 0.6269998550415039\n",
            "Epoch 11, Batch 20, Loss: 0.873920738697052\n",
            "Epoch 11, Batch 21, Loss: 0.5969831347465515\n",
            "Epoch 11, Batch 22, Loss: 0.9491389989852905\n",
            "Epoch 11, Batch 23, Loss: 0.8497674465179443\n",
            "Epoch 11, Batch 24, Loss: 0.9718506336212158\n",
            "Epoch 11, Batch 25, Loss: 0.8166587352752686\n",
            "Epoch 11, Batch 26, Loss: 0.5616293549537659\n",
            "Epoch 11, Batch 27, Loss: 0.6858069896697998\n",
            "Epoch 11, Batch 28, Loss: 0.458835631608963\n",
            "Epoch 11, Batch 29, Loss: 0.6827805042266846\n",
            "Epoch 11, Batch 30, Loss: 0.6973838806152344\n",
            "Epoch 11, Batch 31, Loss: 0.7867324352264404\n",
            "Epoch 11, Batch 32, Loss: 1.0070382356643677\n",
            "Epoch 11, Batch 33, Loss: 0.7786086797714233\n",
            "Epoch 12, Batch 1, Loss: 0.7485606074333191\n",
            "Epoch 12, Batch 2, Loss: 0.75804603099823\n",
            "Epoch 12, Batch 3, Loss: 0.5975297689437866\n",
            "Epoch 12, Batch 4, Loss: 0.6627561450004578\n",
            "Epoch 12, Batch 5, Loss: 0.8766310214996338\n",
            "Epoch 12, Batch 6, Loss: 0.7264498472213745\n",
            "Epoch 12, Batch 7, Loss: 0.6297357082366943\n",
            "Epoch 12, Batch 8, Loss: 0.7701226472854614\n",
            "Epoch 12, Batch 9, Loss: 0.5908619165420532\n",
            "Epoch 12, Batch 10, Loss: 0.7944118976593018\n",
            "Epoch 12, Batch 11, Loss: 0.7573719024658203\n",
            "Epoch 12, Batch 12, Loss: 0.985382080078125\n",
            "Epoch 12, Batch 13, Loss: 0.9124900698661804\n",
            "Epoch 12, Batch 14, Loss: 0.8953249454498291\n",
            "Epoch 12, Batch 15, Loss: 0.6155809760093689\n",
            "Epoch 12, Batch 16, Loss: 0.6451340317726135\n",
            "Epoch 12, Batch 17, Loss: 0.5422989130020142\n",
            "Epoch 12, Batch 18, Loss: 0.7315933704376221\n",
            "Epoch 12, Batch 19, Loss: 0.5377980470657349\n",
            "Epoch 12, Batch 20, Loss: 0.6838518381118774\n",
            "Epoch 12, Batch 21, Loss: 0.7127445936203003\n",
            "Epoch 12, Batch 22, Loss: 0.5614941120147705\n",
            "Epoch 12, Batch 23, Loss: 0.5096321105957031\n",
            "Epoch 12, Batch 24, Loss: 1.008279800415039\n",
            "Epoch 12, Batch 25, Loss: 0.8687765002250671\n",
            "Epoch 12, Batch 26, Loss: 0.5339249968528748\n",
            "Epoch 12, Batch 27, Loss: 0.6042813062667847\n",
            "Epoch 12, Batch 28, Loss: 0.7358986735343933\n",
            "Epoch 12, Batch 29, Loss: 0.5866551399230957\n",
            "Epoch 12, Batch 30, Loss: 1.0852137804031372\n",
            "Epoch 12, Batch 31, Loss: 0.9163477420806885\n",
            "Epoch 12, Batch 32, Loss: 0.5367318391799927\n",
            "Epoch 12, Batch 33, Loss: 0.76689612865448\n",
            "Epoch 13, Batch 1, Loss: 0.7925057411193848\n",
            "Epoch 13, Batch 2, Loss: 1.2596828937530518\n",
            "Epoch 13, Batch 3, Loss: 0.6019254922866821\n",
            "Epoch 13, Batch 4, Loss: 0.788259744644165\n",
            "Epoch 13, Batch 5, Loss: 0.6759002208709717\n",
            "Epoch 13, Batch 6, Loss: 0.5471400618553162\n",
            "Epoch 13, Batch 7, Loss: 0.7354214191436768\n",
            "Epoch 13, Batch 8, Loss: 0.7361462712287903\n",
            "Epoch 13, Batch 9, Loss: 0.6689778566360474\n",
            "Epoch 13, Batch 10, Loss: 0.8039358854293823\n",
            "Epoch 13, Batch 11, Loss: 0.5701808929443359\n",
            "Epoch 13, Batch 12, Loss: 0.7548664808273315\n",
            "Epoch 13, Batch 13, Loss: 0.4359641671180725\n",
            "Epoch 13, Batch 14, Loss: 0.9593711495399475\n",
            "Epoch 13, Batch 15, Loss: 0.5963848829269409\n",
            "Epoch 13, Batch 16, Loss: 0.6496877074241638\n",
            "Epoch 13, Batch 17, Loss: 0.6782258749008179\n",
            "Epoch 13, Batch 18, Loss: 0.7838254570960999\n",
            "Epoch 13, Batch 19, Loss: 0.5895097255706787\n",
            "Epoch 13, Batch 20, Loss: 0.6266842484474182\n",
            "Epoch 13, Batch 21, Loss: 0.7828059196472168\n",
            "Epoch 13, Batch 22, Loss: 0.5139806866645813\n",
            "Epoch 13, Batch 23, Loss: 0.7254247069358826\n",
            "Epoch 13, Batch 24, Loss: 0.6257029175758362\n",
            "Epoch 13, Batch 25, Loss: 0.5701976418495178\n",
            "Epoch 13, Batch 26, Loss: 0.9387033581733704\n",
            "Epoch 13, Batch 27, Loss: 0.99947190284729\n",
            "Epoch 13, Batch 28, Loss: 0.7407417297363281\n",
            "Epoch 13, Batch 29, Loss: 0.5312736630439758\n",
            "Epoch 13, Batch 30, Loss: 0.6237017512321472\n",
            "Epoch 13, Batch 31, Loss: 0.8174204230308533\n",
            "Epoch 13, Batch 32, Loss: 0.8746985197067261\n",
            "Epoch 13, Batch 33, Loss: 0.5641820430755615\n",
            "Epoch 14, Batch 1, Loss: 0.5814098119735718\n",
            "Epoch 14, Batch 2, Loss: 0.8021615743637085\n",
            "Epoch 14, Batch 3, Loss: 0.9134083390235901\n",
            "Epoch 14, Batch 4, Loss: 0.8644185662269592\n",
            "Epoch 14, Batch 5, Loss: 0.9238181710243225\n",
            "Epoch 14, Batch 6, Loss: 0.8014137744903564\n",
            "Epoch 14, Batch 7, Loss: 0.5186364650726318\n",
            "Epoch 14, Batch 8, Loss: 0.45704910159111023\n",
            "Epoch 14, Batch 9, Loss: 0.6504281163215637\n",
            "Epoch 14, Batch 10, Loss: 0.6419030427932739\n",
            "Epoch 14, Batch 11, Loss: 0.6459966897964478\n",
            "Epoch 14, Batch 12, Loss: 0.7165611386299133\n",
            "Epoch 14, Batch 13, Loss: 0.5278890132904053\n",
            "Epoch 14, Batch 14, Loss: 0.47287803888320923\n",
            "Epoch 14, Batch 15, Loss: 0.5011472702026367\n",
            "Epoch 14, Batch 16, Loss: 1.1481322050094604\n",
            "Epoch 14, Batch 17, Loss: 0.7479022741317749\n",
            "Epoch 14, Batch 18, Loss: 0.549162745475769\n",
            "Epoch 14, Batch 19, Loss: 0.7025924921035767\n",
            "Epoch 14, Batch 20, Loss: 0.7181324362754822\n",
            "Epoch 14, Batch 21, Loss: 0.8369430899620056\n",
            "Epoch 14, Batch 22, Loss: 0.485202431678772\n",
            "Epoch 14, Batch 23, Loss: 0.48883509635925293\n",
            "Epoch 14, Batch 24, Loss: 0.9284942746162415\n",
            "Epoch 14, Batch 25, Loss: 0.623261570930481\n",
            "Epoch 14, Batch 26, Loss: 0.7428527474403381\n",
            "Epoch 14, Batch 27, Loss: 1.0462638139724731\n",
            "Epoch 14, Batch 28, Loss: 0.6185844540596008\n",
            "Epoch 14, Batch 29, Loss: 0.5727097392082214\n",
            "Epoch 14, Batch 30, Loss: 0.6575813293457031\n",
            "Epoch 14, Batch 31, Loss: 0.6240995526313782\n",
            "Epoch 14, Batch 32, Loss: 0.811448872089386\n",
            "Epoch 14, Batch 33, Loss: 0.8325093388557434\n",
            "Epoch 15, Batch 1, Loss: 0.6903442740440369\n",
            "Epoch 15, Batch 2, Loss: 0.749780535697937\n",
            "Epoch 15, Batch 3, Loss: 0.8085707426071167\n",
            "Epoch 15, Batch 4, Loss: 0.5749375820159912\n",
            "Epoch 15, Batch 5, Loss: 0.7651208639144897\n",
            "Epoch 15, Batch 6, Loss: 0.5452518463134766\n",
            "Epoch 15, Batch 7, Loss: 0.7182285785675049\n",
            "Epoch 15, Batch 8, Loss: 0.8793569803237915\n",
            "Epoch 15, Batch 9, Loss: 0.6353757381439209\n",
            "Epoch 15, Batch 10, Loss: 0.5719380378723145\n",
            "Epoch 15, Batch 11, Loss: 0.6749016046524048\n",
            "Epoch 15, Batch 12, Loss: 0.9314262270927429\n",
            "Epoch 15, Batch 13, Loss: 0.6954692602157593\n",
            "Epoch 15, Batch 14, Loss: 0.732627272605896\n",
            "Epoch 15, Batch 15, Loss: 0.6014946699142456\n",
            "Epoch 15, Batch 16, Loss: 0.7605754137039185\n",
            "Epoch 15, Batch 17, Loss: 1.0999480485916138\n",
            "Epoch 15, Batch 18, Loss: 0.8955411314964294\n",
            "Epoch 15, Batch 19, Loss: 0.5215665102005005\n",
            "Epoch 15, Batch 20, Loss: 0.5582374930381775\n",
            "Epoch 15, Batch 21, Loss: 0.5267190933227539\n",
            "Epoch 15, Batch 22, Loss: 0.596409797668457\n",
            "Epoch 15, Batch 23, Loss: 0.808708131313324\n",
            "Epoch 15, Batch 24, Loss: 0.6025934219360352\n",
            "Epoch 15, Batch 25, Loss: 0.792234480381012\n",
            "Epoch 15, Batch 26, Loss: 0.7869173884391785\n",
            "Epoch 15, Batch 27, Loss: 0.7393172979354858\n",
            "Epoch 15, Batch 28, Loss: 0.5974859595298767\n",
            "Epoch 15, Batch 29, Loss: 0.6302788853645325\n",
            "Epoch 15, Batch 30, Loss: 0.7369718551635742\n",
            "Epoch 15, Batch 31, Loss: 0.6613339185714722\n",
            "Epoch 15, Batch 32, Loss: 0.7958883047103882\n",
            "Epoch 15, Batch 33, Loss: 1.2230195999145508\n",
            "RMSE for this set: 0.7793673591240545\n",
            "Training with dropout=0.75, lr=0.005, lstm_H=200\n",
            "Epoch 1, Batch 1, Loss: 6.801549911499023\n",
            "Epoch 1, Batch 2, Loss: 4.071389198303223\n",
            "Epoch 1, Batch 3, Loss: 26.754348754882812\n",
            "Epoch 1, Batch 4, Loss: 0.9972050189971924\n",
            "Epoch 1, Batch 5, Loss: 1.9635368585586548\n",
            "Epoch 1, Batch 6, Loss: 2.865190029144287\n",
            "Epoch 1, Batch 7, Loss: 1.8415828943252563\n",
            "Epoch 1, Batch 8, Loss: 1.739945411682129\n",
            "Epoch 1, Batch 9, Loss: 0.6264245510101318\n",
            "Epoch 1, Batch 10, Loss: 0.9265943765640259\n",
            "Epoch 1, Batch 11, Loss: 0.9568432569503784\n",
            "Epoch 1, Batch 12, Loss: 0.763291597366333\n",
            "Epoch 1, Batch 13, Loss: 0.6501816511154175\n",
            "Epoch 1, Batch 14, Loss: 0.8151019811630249\n",
            "Epoch 1, Batch 15, Loss: 0.8590539693832397\n",
            "Epoch 1, Batch 16, Loss: 0.903152346611023\n",
            "Epoch 1, Batch 17, Loss: 0.6531873941421509\n",
            "Epoch 1, Batch 18, Loss: 0.5146677494049072\n",
            "Epoch 1, Batch 19, Loss: 0.9078861474990845\n",
            "Epoch 1, Batch 20, Loss: 0.9069334268569946\n",
            "Epoch 1, Batch 21, Loss: 0.4357762634754181\n",
            "Epoch 1, Batch 22, Loss: 0.6346793174743652\n",
            "Epoch 1, Batch 23, Loss: 0.4174680709838867\n",
            "Epoch 1, Batch 24, Loss: 0.8846558928489685\n",
            "Epoch 1, Batch 25, Loss: 0.6770331263542175\n",
            "Epoch 1, Batch 26, Loss: 0.6432005167007446\n",
            "Epoch 1, Batch 27, Loss: 0.825756311416626\n",
            "Epoch 1, Batch 28, Loss: 0.6475811004638672\n",
            "Epoch 1, Batch 29, Loss: 0.5704066753387451\n",
            "Epoch 1, Batch 30, Loss: 0.7200949192047119\n",
            "Epoch 1, Batch 31, Loss: 0.5392745733261108\n",
            "Epoch 1, Batch 32, Loss: 0.8252186179161072\n",
            "Epoch 1, Batch 33, Loss: 0.613677978515625\n",
            "Epoch 2, Batch 1, Loss: 0.4253254532814026\n",
            "Epoch 2, Batch 2, Loss: 1.166806936264038\n",
            "Epoch 2, Batch 3, Loss: 0.7314002513885498\n",
            "Epoch 2, Batch 4, Loss: 0.5896953344345093\n",
            "Epoch 2, Batch 5, Loss: 0.7977946400642395\n",
            "Epoch 2, Batch 6, Loss: 0.56300950050354\n",
            "Epoch 2, Batch 7, Loss: 0.8131656646728516\n",
            "Epoch 2, Batch 8, Loss: 0.6495094299316406\n",
            "Epoch 2, Batch 9, Loss: 0.46959224343299866\n",
            "Epoch 2, Batch 10, Loss: 0.5603883266448975\n",
            "Epoch 2, Batch 11, Loss: 0.5618295669555664\n",
            "Epoch 2, Batch 12, Loss: 0.6159287691116333\n",
            "Epoch 2, Batch 13, Loss: 1.1030631065368652\n",
            "Epoch 2, Batch 14, Loss: 0.7136571407318115\n",
            "Epoch 2, Batch 15, Loss: 0.5545227527618408\n",
            "Epoch 2, Batch 16, Loss: 0.5626619458198547\n",
            "Epoch 2, Batch 17, Loss: 0.9221137762069702\n",
            "Epoch 2, Batch 18, Loss: 0.7566181421279907\n",
            "Epoch 2, Batch 19, Loss: 0.4607446789741516\n",
            "Epoch 2, Batch 20, Loss: 0.7646631002426147\n",
            "Epoch 2, Batch 21, Loss: 0.643206775188446\n",
            "Epoch 2, Batch 22, Loss: 0.643854022026062\n",
            "Epoch 2, Batch 23, Loss: 0.7120362520217896\n",
            "Epoch 2, Batch 24, Loss: 0.7196409702301025\n",
            "Epoch 2, Batch 25, Loss: 0.8403868079185486\n",
            "Epoch 2, Batch 26, Loss: 0.4583165645599365\n",
            "Epoch 2, Batch 27, Loss: 0.8480991125106812\n",
            "Epoch 2, Batch 28, Loss: 0.9436763525009155\n",
            "Epoch 2, Batch 29, Loss: 0.7058916091918945\n",
            "Epoch 2, Batch 30, Loss: 0.8212108016014099\n",
            "Epoch 2, Batch 31, Loss: 0.765953540802002\n",
            "Epoch 2, Batch 32, Loss: 0.8029277324676514\n",
            "Epoch 2, Batch 33, Loss: 0.9126863479614258\n",
            "Epoch 3, Batch 1, Loss: 0.5870965719223022\n",
            "Epoch 3, Batch 2, Loss: 0.46879270672798157\n",
            "Epoch 3, Batch 3, Loss: 0.7169378995895386\n",
            "Epoch 3, Batch 4, Loss: 0.5287467837333679\n",
            "Epoch 3, Batch 5, Loss: 1.1180777549743652\n",
            "Epoch 3, Batch 6, Loss: 0.6715428829193115\n",
            "Epoch 3, Batch 7, Loss: 0.6794677972793579\n",
            "Epoch 3, Batch 8, Loss: 0.7701760530471802\n",
            "Epoch 3, Batch 9, Loss: 0.5686674118041992\n",
            "Epoch 3, Batch 10, Loss: 0.703528106212616\n",
            "Epoch 3, Batch 11, Loss: 0.6128090620040894\n",
            "Epoch 3, Batch 12, Loss: 0.6212517023086548\n",
            "Epoch 3, Batch 13, Loss: 0.6384141445159912\n",
            "Epoch 3, Batch 14, Loss: 0.7626183032989502\n",
            "Epoch 3, Batch 15, Loss: 0.6734142303466797\n",
            "Epoch 3, Batch 16, Loss: 0.7424931526184082\n",
            "Epoch 3, Batch 17, Loss: 0.4187292456626892\n",
            "Epoch 3, Batch 18, Loss: 0.4976382851600647\n",
            "Epoch 3, Batch 19, Loss: 0.843530535697937\n",
            "Epoch 3, Batch 20, Loss: 0.7602049708366394\n",
            "Epoch 3, Batch 21, Loss: 0.9075719118118286\n",
            "Epoch 3, Batch 22, Loss: 0.6995235681533813\n",
            "Epoch 3, Batch 23, Loss: 0.6257952451705933\n",
            "Epoch 3, Batch 24, Loss: 0.7201526165008545\n",
            "Epoch 3, Batch 25, Loss: 0.9525976777076721\n",
            "Epoch 3, Batch 26, Loss: 0.7857375144958496\n",
            "Epoch 3, Batch 27, Loss: 0.5861282348632812\n",
            "Epoch 3, Batch 28, Loss: 0.6543176174163818\n",
            "Epoch 3, Batch 29, Loss: 0.9239953756332397\n",
            "Epoch 3, Batch 30, Loss: 0.9169149398803711\n",
            "Epoch 3, Batch 31, Loss: 0.5853979587554932\n",
            "Epoch 3, Batch 32, Loss: 0.8959020376205444\n",
            "Epoch 3, Batch 33, Loss: 0.7013188004493713\n",
            "Epoch 4, Batch 1, Loss: 0.7058225274085999\n",
            "Epoch 4, Batch 2, Loss: 0.6692731380462646\n",
            "Epoch 4, Batch 3, Loss: 0.7616357803344727\n",
            "Epoch 4, Batch 4, Loss: 1.08681321144104\n",
            "Epoch 4, Batch 5, Loss: 0.8295369148254395\n",
            "Epoch 4, Batch 6, Loss: 0.6605582237243652\n",
            "Epoch 4, Batch 7, Loss: 0.6993434429168701\n",
            "Epoch 4, Batch 8, Loss: 0.776910126209259\n",
            "Epoch 4, Batch 9, Loss: 1.0438988208770752\n",
            "Epoch 4, Batch 10, Loss: 0.735744833946228\n",
            "Epoch 4, Batch 11, Loss: 0.913711428642273\n",
            "Epoch 4, Batch 12, Loss: 0.7446469068527222\n",
            "Epoch 4, Batch 13, Loss: 0.7238448858261108\n",
            "Epoch 4, Batch 14, Loss: 0.6755075454711914\n",
            "Epoch 4, Batch 15, Loss: 0.4391041696071625\n",
            "Epoch 4, Batch 16, Loss: 1.0417450666427612\n",
            "Epoch 4, Batch 17, Loss: 0.7452651858329773\n",
            "Epoch 4, Batch 18, Loss: 0.8506050705909729\n",
            "Epoch 4, Batch 19, Loss: 0.4894312024116516\n",
            "Epoch 4, Batch 20, Loss: 0.7460099458694458\n",
            "Epoch 4, Batch 21, Loss: 0.5195045471191406\n",
            "Epoch 4, Batch 22, Loss: 0.5496752262115479\n",
            "Epoch 4, Batch 23, Loss: 0.9676233530044556\n",
            "Epoch 4, Batch 24, Loss: 0.664745569229126\n",
            "Epoch 4, Batch 25, Loss: 0.5995599031448364\n",
            "Epoch 4, Batch 26, Loss: 0.57828289270401\n",
            "Epoch 4, Batch 27, Loss: 0.698898196220398\n",
            "Epoch 4, Batch 28, Loss: 0.45067834854125977\n",
            "Epoch 4, Batch 29, Loss: 0.7622113227844238\n",
            "Epoch 4, Batch 30, Loss: 0.6740546226501465\n",
            "Epoch 4, Batch 31, Loss: 0.7280728816986084\n",
            "Epoch 4, Batch 32, Loss: 0.6494771838188171\n",
            "Epoch 4, Batch 33, Loss: 1.32742178440094\n",
            "Epoch 5, Batch 1, Loss: 0.6520999670028687\n",
            "Epoch 5, Batch 2, Loss: 0.7679216265678406\n",
            "Epoch 5, Batch 3, Loss: 0.6283756494522095\n",
            "Epoch 5, Batch 4, Loss: 0.8980000615119934\n",
            "Epoch 5, Batch 5, Loss: 0.5387111902236938\n",
            "Epoch 5, Batch 6, Loss: 0.7806119918823242\n",
            "Epoch 5, Batch 7, Loss: 0.6521416306495667\n",
            "Epoch 5, Batch 8, Loss: 0.6295591592788696\n",
            "Epoch 5, Batch 9, Loss: 0.8235310912132263\n",
            "Epoch 5, Batch 10, Loss: 0.4992895722389221\n",
            "Epoch 5, Batch 11, Loss: 0.44495004415512085\n",
            "Epoch 5, Batch 12, Loss: 0.5579126477241516\n",
            "Epoch 5, Batch 13, Loss: 0.5685812830924988\n",
            "Epoch 5, Batch 14, Loss: 0.6477905511856079\n",
            "Epoch 5, Batch 15, Loss: 0.860823392868042\n",
            "Epoch 5, Batch 16, Loss: 0.6852517127990723\n",
            "Epoch 5, Batch 17, Loss: 0.6574769020080566\n",
            "Epoch 5, Batch 18, Loss: 0.7450783252716064\n",
            "Epoch 5, Batch 19, Loss: 0.7730220556259155\n",
            "Epoch 5, Batch 20, Loss: 0.9583793878555298\n",
            "Epoch 5, Batch 21, Loss: 0.6949774026870728\n",
            "Epoch 5, Batch 22, Loss: 0.6200940608978271\n",
            "Epoch 5, Batch 23, Loss: 0.8412671089172363\n",
            "Epoch 5, Batch 24, Loss: 0.7311050891876221\n",
            "Epoch 5, Batch 25, Loss: 0.7236820459365845\n",
            "Epoch 5, Batch 26, Loss: 0.6369462609291077\n",
            "Epoch 5, Batch 27, Loss: 0.8158083558082581\n",
            "Epoch 5, Batch 28, Loss: 0.6824520230293274\n",
            "Epoch 5, Batch 29, Loss: 0.7442605495452881\n",
            "Epoch 5, Batch 30, Loss: 0.5593632459640503\n",
            "Epoch 5, Batch 31, Loss: 1.01473069190979\n",
            "Epoch 5, Batch 32, Loss: 0.564140796661377\n",
            "Epoch 5, Batch 33, Loss: 0.7602062225341797\n",
            "Epoch 6, Batch 1, Loss: 0.4777412414550781\n",
            "Epoch 6, Batch 2, Loss: 0.6983799934387207\n",
            "Epoch 6, Batch 3, Loss: 0.966338038444519\n",
            "Epoch 6, Batch 4, Loss: 0.7188177108764648\n",
            "Epoch 6, Batch 5, Loss: 0.8095459938049316\n",
            "Epoch 6, Batch 6, Loss: 0.4984934329986572\n",
            "Epoch 6, Batch 7, Loss: 0.644102931022644\n",
            "Epoch 6, Batch 8, Loss: 0.6492003202438354\n",
            "Epoch 6, Batch 9, Loss: 0.48078495264053345\n",
            "Epoch 6, Batch 10, Loss: 0.6372110843658447\n",
            "Epoch 6, Batch 11, Loss: 0.8512283563613892\n",
            "Epoch 6, Batch 12, Loss: 0.5763469338417053\n",
            "Epoch 6, Batch 13, Loss: 0.8980047702789307\n",
            "Epoch 6, Batch 14, Loss: 0.43686407804489136\n",
            "Epoch 6, Batch 15, Loss: 0.5657788515090942\n",
            "Epoch 6, Batch 16, Loss: 0.7881520986557007\n",
            "Epoch 6, Batch 17, Loss: 0.6070927977561951\n",
            "Epoch 6, Batch 18, Loss: 0.7866659164428711\n",
            "Epoch 6, Batch 19, Loss: 1.1326543092727661\n",
            "Epoch 6, Batch 20, Loss: 0.6383241415023804\n",
            "Epoch 6, Batch 21, Loss: 0.6954013109207153\n",
            "Epoch 6, Batch 22, Loss: 0.7035530805587769\n",
            "Epoch 6, Batch 23, Loss: 0.6501659750938416\n",
            "Epoch 6, Batch 24, Loss: 0.711510181427002\n",
            "Epoch 6, Batch 25, Loss: 0.5127050876617432\n",
            "Epoch 6, Batch 26, Loss: 0.8611786365509033\n",
            "Epoch 6, Batch 27, Loss: 0.9843837022781372\n",
            "Epoch 6, Batch 28, Loss: 0.736289918422699\n",
            "Epoch 6, Batch 29, Loss: 0.8502868413925171\n",
            "Epoch 6, Batch 30, Loss: 0.8633815050125122\n",
            "Epoch 6, Batch 31, Loss: 0.9039846062660217\n",
            "Epoch 6, Batch 32, Loss: 0.6735092997550964\n",
            "Epoch 6, Batch 33, Loss: 0.7834897637367249\n",
            "Epoch 7, Batch 1, Loss: 0.9273427724838257\n",
            "Epoch 7, Batch 2, Loss: 0.6427978277206421\n",
            "Epoch 7, Batch 3, Loss: 0.8367434740066528\n",
            "Epoch 7, Batch 4, Loss: 0.7399214506149292\n",
            "Epoch 7, Batch 5, Loss: 0.7560105323791504\n",
            "Epoch 7, Batch 6, Loss: 0.9620416760444641\n",
            "Epoch 7, Batch 7, Loss: 0.7257831692695618\n",
            "Epoch 7, Batch 8, Loss: 0.618718147277832\n",
            "Epoch 7, Batch 9, Loss: 0.7646076679229736\n",
            "Epoch 7, Batch 10, Loss: 0.7740622758865356\n",
            "Epoch 7, Batch 11, Loss: 0.7298045754432678\n",
            "Epoch 7, Batch 12, Loss: 0.5636925101280212\n",
            "Epoch 7, Batch 13, Loss: 0.6867009997367859\n",
            "Epoch 7, Batch 14, Loss: 0.66985023021698\n",
            "Epoch 7, Batch 15, Loss: 0.4760925769805908\n",
            "Epoch 7, Batch 16, Loss: 0.7091245651245117\n",
            "Epoch 7, Batch 17, Loss: 0.538885235786438\n",
            "Epoch 7, Batch 18, Loss: 0.7866525650024414\n",
            "Epoch 7, Batch 19, Loss: 0.8180087208747864\n",
            "Epoch 7, Batch 20, Loss: 0.542458176612854\n",
            "Epoch 7, Batch 21, Loss: 0.5624672174453735\n",
            "Epoch 7, Batch 22, Loss: 0.6496911644935608\n",
            "Epoch 7, Batch 23, Loss: 0.6216577291488647\n",
            "Epoch 7, Batch 24, Loss: 0.500508189201355\n",
            "Epoch 7, Batch 25, Loss: 0.7534860372543335\n",
            "Epoch 7, Batch 26, Loss: 0.7038825750350952\n",
            "Epoch 7, Batch 27, Loss: 0.6162883043289185\n",
            "Epoch 7, Batch 28, Loss: 0.8997206687927246\n",
            "Epoch 7, Batch 29, Loss: 0.5105389356613159\n",
            "Epoch 7, Batch 30, Loss: 0.752143383026123\n",
            "Epoch 7, Batch 31, Loss: 0.8766154646873474\n",
            "Epoch 7, Batch 32, Loss: 0.8985440135002136\n",
            "Epoch 7, Batch 33, Loss: 1.326011061668396\n",
            "Epoch 8, Batch 1, Loss: 0.5663504600524902\n",
            "Epoch 8, Batch 2, Loss: 0.6317510604858398\n",
            "Epoch 8, Batch 3, Loss: 0.45092400908470154\n",
            "Epoch 8, Batch 4, Loss: 1.1265308856964111\n",
            "Epoch 8, Batch 5, Loss: 0.6285179853439331\n",
            "Epoch 8, Batch 6, Loss: 1.1049306392669678\n",
            "Epoch 8, Batch 7, Loss: 0.3758028447628021\n",
            "Epoch 8, Batch 8, Loss: 0.9379321336746216\n",
            "Epoch 8, Batch 9, Loss: 0.7372193932533264\n",
            "Epoch 8, Batch 10, Loss: 0.6495386362075806\n",
            "Epoch 8, Batch 11, Loss: 0.6506770849227905\n",
            "Epoch 8, Batch 12, Loss: 0.5741493105888367\n",
            "Epoch 8, Batch 13, Loss: 0.689916729927063\n",
            "Epoch 8, Batch 14, Loss: 0.6241660118103027\n",
            "Epoch 8, Batch 15, Loss: 0.8637643456459045\n",
            "Epoch 8, Batch 16, Loss: 0.5933433771133423\n",
            "Epoch 8, Batch 17, Loss: 0.7961385250091553\n",
            "Epoch 8, Batch 18, Loss: 0.6585763692855835\n",
            "Epoch 8, Batch 19, Loss: 0.8366413116455078\n",
            "Epoch 8, Batch 20, Loss: 1.0355238914489746\n",
            "Epoch 8, Batch 21, Loss: 0.6058946847915649\n",
            "Epoch 8, Batch 22, Loss: 0.4791860282421112\n",
            "Epoch 8, Batch 23, Loss: 0.8451457023620605\n",
            "Epoch 8, Batch 24, Loss: 0.6778569221496582\n",
            "Epoch 8, Batch 25, Loss: 0.5371303558349609\n",
            "Epoch 8, Batch 26, Loss: 0.7991840839385986\n",
            "Epoch 8, Batch 27, Loss: 0.5757755041122437\n",
            "Epoch 8, Batch 28, Loss: 0.6314391493797302\n",
            "Epoch 8, Batch 29, Loss: 0.692092776298523\n",
            "Epoch 8, Batch 30, Loss: 0.6098757386207581\n",
            "Epoch 8, Batch 31, Loss: 1.021993637084961\n",
            "Epoch 8, Batch 32, Loss: 0.7975085973739624\n",
            "Epoch 8, Batch 33, Loss: 0.6488003730773926\n",
            "Epoch 9, Batch 1, Loss: 0.6830844879150391\n",
            "Epoch 9, Batch 2, Loss: 0.8050785064697266\n",
            "Epoch 9, Batch 3, Loss: 0.6920047402381897\n",
            "Epoch 9, Batch 4, Loss: 0.6812333464622498\n",
            "Epoch 9, Batch 5, Loss: 0.6199390888214111\n",
            "Epoch 9, Batch 6, Loss: 0.8681000471115112\n",
            "Epoch 9, Batch 7, Loss: 0.4883057773113251\n",
            "Epoch 9, Batch 8, Loss: 0.6754027605056763\n",
            "Epoch 9, Batch 9, Loss: 0.4439459443092346\n",
            "Epoch 9, Batch 10, Loss: 0.9796120524406433\n",
            "Epoch 9, Batch 11, Loss: 0.49854543805122375\n",
            "Epoch 9, Batch 12, Loss: 0.7676222920417786\n",
            "Epoch 9, Batch 13, Loss: 0.8462182283401489\n",
            "Epoch 9, Batch 14, Loss: 0.523539662361145\n",
            "Epoch 9, Batch 15, Loss: 0.7456539273262024\n",
            "Epoch 9, Batch 16, Loss: 0.7520387768745422\n",
            "Epoch 9, Batch 17, Loss: 0.7773499488830566\n",
            "Epoch 9, Batch 18, Loss: 0.47244730591773987\n",
            "Epoch 9, Batch 19, Loss: 0.7516754865646362\n",
            "Epoch 9, Batch 20, Loss: 0.7727005481719971\n",
            "Epoch 9, Batch 21, Loss: 0.753436803817749\n",
            "Epoch 9, Batch 22, Loss: 0.7595829963684082\n",
            "Epoch 9, Batch 23, Loss: 0.5459690690040588\n",
            "Epoch 9, Batch 24, Loss: 0.9785836338996887\n",
            "Epoch 9, Batch 25, Loss: 0.5630838871002197\n",
            "Epoch 9, Batch 26, Loss: 0.7426533102989197\n",
            "Epoch 9, Batch 27, Loss: 0.7581641674041748\n",
            "Epoch 9, Batch 28, Loss: 0.6963778138160706\n",
            "Epoch 9, Batch 29, Loss: 0.7856701612472534\n",
            "Epoch 9, Batch 30, Loss: 0.460353821516037\n",
            "Epoch 9, Batch 31, Loss: 1.0170800685882568\n",
            "Epoch 9, Batch 32, Loss: 0.744605541229248\n",
            "Epoch 9, Batch 33, Loss: 0.9109566807746887\n",
            "Epoch 10, Batch 1, Loss: 0.6709960699081421\n",
            "Epoch 10, Batch 2, Loss: 0.6995955109596252\n",
            "Epoch 10, Batch 3, Loss: 1.0707699060440063\n",
            "Epoch 10, Batch 4, Loss: 0.6777613759040833\n",
            "Epoch 10, Batch 5, Loss: 0.8876485824584961\n",
            "Epoch 10, Batch 6, Loss: 0.7510393857955933\n",
            "Epoch 10, Batch 7, Loss: 0.5431013107299805\n",
            "Epoch 10, Batch 8, Loss: 0.7349556684494019\n",
            "Epoch 10, Batch 9, Loss: 0.6650993824005127\n",
            "Epoch 10, Batch 10, Loss: 0.7159174680709839\n",
            "Epoch 10, Batch 11, Loss: 0.648767352104187\n",
            "Epoch 10, Batch 12, Loss: 1.0917010307312012\n",
            "Epoch 10, Batch 13, Loss: 0.7412410378456116\n",
            "Epoch 10, Batch 14, Loss: 0.698521614074707\n",
            "Epoch 10, Batch 15, Loss: 0.6913484334945679\n",
            "Epoch 10, Batch 16, Loss: 0.8268816471099854\n",
            "Epoch 10, Batch 17, Loss: 0.7559239864349365\n",
            "Epoch 10, Batch 18, Loss: 0.769737958908081\n",
            "Epoch 10, Batch 19, Loss: 0.8464221954345703\n",
            "Epoch 10, Batch 20, Loss: 0.853793740272522\n",
            "Epoch 10, Batch 21, Loss: 0.7539350986480713\n",
            "Epoch 10, Batch 22, Loss: 0.4545295536518097\n",
            "Epoch 10, Batch 23, Loss: 0.6127582788467407\n",
            "Epoch 10, Batch 24, Loss: 0.7096457481384277\n",
            "Epoch 10, Batch 25, Loss: 0.7765376567840576\n",
            "Epoch 10, Batch 26, Loss: 0.8030881285667419\n",
            "Epoch 10, Batch 27, Loss: 0.7989130020141602\n",
            "Epoch 10, Batch 28, Loss: 0.5517964959144592\n",
            "Epoch 10, Batch 29, Loss: 0.5385430455207825\n",
            "Epoch 10, Batch 30, Loss: 0.44807153940200806\n",
            "Epoch 10, Batch 31, Loss: 0.4884736239910126\n",
            "Epoch 10, Batch 32, Loss: 0.6198620796203613\n",
            "Epoch 10, Batch 33, Loss: 0.8742610216140747\n",
            "Epoch 11, Batch 1, Loss: 0.7763527631759644\n",
            "Epoch 11, Batch 2, Loss: 0.7963382005691528\n",
            "Epoch 11, Batch 3, Loss: 0.47876983880996704\n",
            "Epoch 11, Batch 4, Loss: 0.7794158458709717\n",
            "Epoch 11, Batch 5, Loss: 0.854591965675354\n",
            "Epoch 11, Batch 6, Loss: 0.45371198654174805\n",
            "Epoch 11, Batch 7, Loss: 0.4568282663822174\n",
            "Epoch 11, Batch 8, Loss: 0.7837128639221191\n",
            "Epoch 11, Batch 9, Loss: 0.662351131439209\n",
            "Epoch 11, Batch 10, Loss: 0.6373262405395508\n",
            "Epoch 11, Batch 11, Loss: 0.7132485508918762\n",
            "Epoch 11, Batch 12, Loss: 0.5439850091934204\n",
            "Epoch 11, Batch 13, Loss: 0.7437074780464172\n",
            "Epoch 11, Batch 14, Loss: 0.9648740291595459\n",
            "Epoch 11, Batch 15, Loss: 0.869700014591217\n",
            "Epoch 11, Batch 16, Loss: 0.47322848439216614\n",
            "Epoch 11, Batch 17, Loss: 0.501228928565979\n",
            "Epoch 11, Batch 18, Loss: 0.6763478517532349\n",
            "Epoch 11, Batch 19, Loss: 0.7366414666175842\n",
            "Epoch 11, Batch 20, Loss: 0.8612297773361206\n",
            "Epoch 11, Batch 21, Loss: 0.9883425831794739\n",
            "Epoch 11, Batch 22, Loss: 0.6816965937614441\n",
            "Epoch 11, Batch 23, Loss: 0.6283455491065979\n",
            "Epoch 11, Batch 24, Loss: 0.6337652206420898\n",
            "Epoch 11, Batch 25, Loss: 0.7209103107452393\n",
            "Epoch 11, Batch 26, Loss: 1.075716257095337\n",
            "Epoch 11, Batch 27, Loss: 0.6366382837295532\n",
            "Epoch 11, Batch 28, Loss: 0.8433787822723389\n",
            "Epoch 11, Batch 29, Loss: 0.8644518852233887\n",
            "Epoch 11, Batch 30, Loss: 0.5033926963806152\n",
            "Epoch 11, Batch 31, Loss: 0.6829831600189209\n",
            "Epoch 11, Batch 32, Loss: 0.8438330888748169\n",
            "Epoch 11, Batch 33, Loss: 1.6132543087005615\n",
            "Epoch 12, Batch 1, Loss: 0.8025294542312622\n",
            "Epoch 12, Batch 2, Loss: 0.715471625328064\n",
            "Epoch 12, Batch 3, Loss: 0.6855342984199524\n",
            "Epoch 12, Batch 4, Loss: 0.7370830178260803\n",
            "Epoch 12, Batch 5, Loss: 0.7567132711410522\n",
            "Epoch 12, Batch 6, Loss: 0.7025344371795654\n",
            "Epoch 12, Batch 7, Loss: 0.8418607711791992\n",
            "Epoch 12, Batch 8, Loss: 0.5894109606742859\n",
            "Epoch 12, Batch 9, Loss: 0.6996377110481262\n",
            "Epoch 12, Batch 10, Loss: 0.6468616127967834\n",
            "Epoch 12, Batch 11, Loss: 0.9959917068481445\n",
            "Epoch 12, Batch 12, Loss: 0.6743760108947754\n",
            "Epoch 12, Batch 13, Loss: 0.6784080266952515\n",
            "Epoch 12, Batch 14, Loss: 0.7131313681602478\n",
            "Epoch 12, Batch 15, Loss: 0.9619707465171814\n",
            "Epoch 12, Batch 16, Loss: 0.7759894728660583\n",
            "Epoch 12, Batch 17, Loss: 0.5347605347633362\n",
            "Epoch 12, Batch 18, Loss: 0.8102937936782837\n",
            "Epoch 12, Batch 19, Loss: 0.7104802131652832\n",
            "Epoch 12, Batch 20, Loss: 0.7403681874275208\n",
            "Epoch 12, Batch 21, Loss: 0.6374638080596924\n",
            "Epoch 12, Batch 22, Loss: 0.6758958697319031\n",
            "Epoch 12, Batch 23, Loss: 0.6906266212463379\n",
            "Epoch 12, Batch 24, Loss: 0.692868709564209\n",
            "Epoch 12, Batch 25, Loss: 0.47523537278175354\n",
            "Epoch 12, Batch 26, Loss: 0.8159763813018799\n",
            "Epoch 12, Batch 27, Loss: 0.6711944341659546\n",
            "Epoch 12, Batch 28, Loss: 0.5979611873626709\n",
            "Epoch 12, Batch 29, Loss: 0.6201279163360596\n",
            "Epoch 12, Batch 30, Loss: 0.8318424224853516\n",
            "Epoch 12, Batch 31, Loss: 0.5282953977584839\n",
            "Epoch 12, Batch 32, Loss: 0.6663122177124023\n",
            "Epoch 12, Batch 33, Loss: 1.1129791736602783\n",
            "Epoch 13, Batch 1, Loss: 0.883537232875824\n",
            "Epoch 13, Batch 2, Loss: 0.8036810755729675\n",
            "Epoch 13, Batch 3, Loss: 0.7042362093925476\n",
            "Epoch 13, Batch 4, Loss: 0.592385470867157\n",
            "Epoch 13, Batch 5, Loss: 0.7980494499206543\n",
            "Epoch 13, Batch 6, Loss: 0.6188575029373169\n",
            "Epoch 13, Batch 7, Loss: 0.564598798751831\n",
            "Epoch 13, Batch 8, Loss: 0.8081607818603516\n",
            "Epoch 13, Batch 9, Loss: 0.6902124285697937\n",
            "Epoch 13, Batch 10, Loss: 0.5769809484481812\n",
            "Epoch 13, Batch 11, Loss: 0.7544372081756592\n",
            "Epoch 13, Batch 12, Loss: 0.9760503768920898\n",
            "Epoch 13, Batch 13, Loss: 0.6380888819694519\n",
            "Epoch 13, Batch 14, Loss: 0.8937063217163086\n",
            "Epoch 13, Batch 15, Loss: 0.5339063405990601\n",
            "Epoch 13, Batch 16, Loss: 0.6067854166030884\n",
            "Epoch 13, Batch 17, Loss: 0.7521464228630066\n",
            "Epoch 13, Batch 18, Loss: 0.4370391368865967\n",
            "Epoch 13, Batch 19, Loss: 0.8242872953414917\n",
            "Epoch 13, Batch 20, Loss: 0.8276371955871582\n",
            "Epoch 13, Batch 21, Loss: 0.855835497379303\n",
            "Epoch 13, Batch 22, Loss: 0.7193096876144409\n",
            "Epoch 13, Batch 23, Loss: 0.6129936575889587\n",
            "Epoch 13, Batch 24, Loss: 0.505513072013855\n",
            "Epoch 13, Batch 25, Loss: 0.47733429074287415\n",
            "Epoch 13, Batch 26, Loss: 0.9721099138259888\n",
            "Epoch 13, Batch 27, Loss: 0.9611618518829346\n",
            "Epoch 13, Batch 28, Loss: 0.7927846908569336\n",
            "Epoch 13, Batch 29, Loss: 0.7452259063720703\n",
            "Epoch 13, Batch 30, Loss: 1.0382518768310547\n",
            "Epoch 13, Batch 31, Loss: 0.554858386516571\n",
            "Epoch 13, Batch 32, Loss: 0.7588596343994141\n",
            "Epoch 13, Batch 33, Loss: 0.22624173760414124\n",
            "Epoch 14, Batch 1, Loss: 0.5096339583396912\n",
            "Epoch 14, Batch 2, Loss: 0.5966131091117859\n",
            "Epoch 14, Batch 3, Loss: 0.30724769830703735\n",
            "Epoch 14, Batch 4, Loss: 0.7851361632347107\n",
            "Epoch 14, Batch 5, Loss: 0.654627799987793\n",
            "Epoch 14, Batch 6, Loss: 0.7660301327705383\n",
            "Epoch 14, Batch 7, Loss: 0.7271603345870972\n",
            "Epoch 14, Batch 8, Loss: 0.7276672124862671\n",
            "Epoch 14, Batch 9, Loss: 1.0059609413146973\n",
            "Epoch 14, Batch 10, Loss: 0.6677227020263672\n",
            "Epoch 14, Batch 11, Loss: 0.39815500378608704\n",
            "Epoch 14, Batch 12, Loss: 0.618434488773346\n",
            "Epoch 14, Batch 13, Loss: 0.9507131576538086\n",
            "Epoch 14, Batch 14, Loss: 0.6726198196411133\n",
            "Epoch 14, Batch 15, Loss: 0.5882507562637329\n",
            "Epoch 14, Batch 16, Loss: 0.5605702996253967\n",
            "Epoch 14, Batch 17, Loss: 0.7279926538467407\n",
            "Epoch 14, Batch 18, Loss: 0.7463268041610718\n",
            "Epoch 14, Batch 19, Loss: 1.0151835680007935\n",
            "Epoch 14, Batch 20, Loss: 0.649915337562561\n",
            "Epoch 14, Batch 21, Loss: 0.7067962884902954\n",
            "Epoch 14, Batch 22, Loss: 0.7875778675079346\n",
            "Epoch 14, Batch 23, Loss: 1.0281882286071777\n",
            "Epoch 14, Batch 24, Loss: 0.7077093124389648\n",
            "Epoch 14, Batch 25, Loss: 0.6931107044219971\n",
            "Epoch 14, Batch 26, Loss: 0.7685037851333618\n",
            "Epoch 14, Batch 27, Loss: 0.737030029296875\n",
            "Epoch 14, Batch 28, Loss: 0.5242875218391418\n",
            "Epoch 14, Batch 29, Loss: 1.0344254970550537\n",
            "Epoch 14, Batch 30, Loss: 0.8109312057495117\n",
            "Epoch 14, Batch 31, Loss: 0.9250349998474121\n",
            "Epoch 14, Batch 32, Loss: 0.7099636197090149\n",
            "Epoch 14, Batch 33, Loss: 0.6558772921562195\n",
            "Epoch 15, Batch 1, Loss: 0.9264787435531616\n",
            "Epoch 15, Batch 2, Loss: 0.5860739350318909\n",
            "Epoch 15, Batch 3, Loss: 0.815181314945221\n",
            "Epoch 15, Batch 4, Loss: 0.5877730846405029\n",
            "Epoch 15, Batch 5, Loss: 1.0617483854293823\n",
            "Epoch 15, Batch 6, Loss: 0.7889176607131958\n",
            "Epoch 15, Batch 7, Loss: 0.698578417301178\n",
            "Epoch 15, Batch 8, Loss: 0.5718237161636353\n",
            "Epoch 15, Batch 9, Loss: 0.5319986939430237\n",
            "Epoch 15, Batch 10, Loss: 0.6665960550308228\n",
            "Epoch 15, Batch 11, Loss: 0.5156077146530151\n",
            "Epoch 15, Batch 12, Loss: 0.6672651171684265\n",
            "Epoch 15, Batch 13, Loss: 0.6525337100028992\n",
            "Epoch 15, Batch 14, Loss: 0.8459603190422058\n",
            "Epoch 15, Batch 15, Loss: 0.6974173188209534\n",
            "Epoch 15, Batch 16, Loss: 0.630021870136261\n",
            "Epoch 15, Batch 17, Loss: 0.679799497127533\n",
            "Epoch 15, Batch 18, Loss: 0.977138876914978\n",
            "Epoch 15, Batch 19, Loss: 0.6220330595970154\n",
            "Epoch 15, Batch 20, Loss: 1.1801458597183228\n",
            "Epoch 15, Batch 21, Loss: 0.6508724689483643\n",
            "Epoch 15, Batch 22, Loss: 0.6925499439239502\n",
            "Epoch 15, Batch 23, Loss: 0.6205353140830994\n",
            "Epoch 15, Batch 24, Loss: 0.6684442758560181\n",
            "Epoch 15, Batch 25, Loss: 0.739732563495636\n",
            "Epoch 15, Batch 26, Loss: 0.41672399640083313\n",
            "Epoch 15, Batch 27, Loss: 0.6883835792541504\n",
            "Epoch 15, Batch 28, Loss: 0.7887033224105835\n",
            "Epoch 15, Batch 29, Loss: 0.7900532484054565\n",
            "Epoch 15, Batch 30, Loss: 0.840644359588623\n",
            "Epoch 15, Batch 31, Loss: 0.4674728810787201\n",
            "Epoch 15, Batch 32, Loss: 0.5268521904945374\n",
            "Epoch 15, Batch 33, Loss: 0.8484195470809937\n",
            "RMSE for this set: 0.8213566731072197\n",
            "Training with dropout=0.75, lr=0.005, lstm_H=300\n",
            "Epoch 1, Batch 1, Loss: 8.597091674804688\n",
            "Epoch 1, Batch 2, Loss: 4.51315450668335\n",
            "Epoch 1, Batch 3, Loss: 31.023414611816406\n",
            "Epoch 1, Batch 4, Loss: 0.5527156591415405\n",
            "Epoch 1, Batch 5, Loss: 4.912868499755859\n",
            "Epoch 1, Batch 6, Loss: 2.9666595458984375\n",
            "Epoch 1, Batch 7, Loss: 2.405618667602539\n",
            "Epoch 1, Batch 8, Loss: 0.8972889184951782\n",
            "Epoch 1, Batch 9, Loss: 1.123422622680664\n",
            "Epoch 1, Batch 10, Loss: 1.057596206665039\n",
            "Epoch 1, Batch 11, Loss: 0.771264910697937\n",
            "Epoch 1, Batch 12, Loss: 0.774520993232727\n",
            "Epoch 1, Batch 13, Loss: 0.9546051621437073\n",
            "Epoch 1, Batch 14, Loss: 0.741782546043396\n",
            "Epoch 1, Batch 15, Loss: 0.8888270854949951\n",
            "Epoch 1, Batch 16, Loss: 0.6638867855072021\n",
            "Epoch 1, Batch 17, Loss: 0.8047376871109009\n",
            "Epoch 1, Batch 18, Loss: 0.7381609678268433\n",
            "Epoch 1, Batch 19, Loss: 0.7896651029586792\n",
            "Epoch 1, Batch 20, Loss: 0.58793044090271\n",
            "Epoch 1, Batch 21, Loss: 0.6142277717590332\n",
            "Epoch 1, Batch 22, Loss: 0.7125709056854248\n",
            "Epoch 1, Batch 23, Loss: 0.6111570596694946\n",
            "Epoch 1, Batch 24, Loss: 0.8140749931335449\n",
            "Epoch 1, Batch 25, Loss: 0.6487144827842712\n",
            "Epoch 1, Batch 26, Loss: 0.6862305998802185\n",
            "Epoch 1, Batch 27, Loss: 0.7188467979431152\n",
            "Epoch 1, Batch 28, Loss: 0.6730446219444275\n",
            "Epoch 1, Batch 29, Loss: 0.790876030921936\n",
            "Epoch 1, Batch 30, Loss: 0.9778085350990295\n",
            "Epoch 1, Batch 31, Loss: 0.712763786315918\n",
            "Epoch 1, Batch 32, Loss: 0.7943346500396729\n",
            "Epoch 1, Batch 33, Loss: 0.5941882729530334\n",
            "Epoch 2, Batch 1, Loss: 0.6654331684112549\n",
            "Epoch 2, Batch 2, Loss: 0.5000678896903992\n",
            "Epoch 2, Batch 3, Loss: 0.6518595218658447\n",
            "Epoch 2, Batch 4, Loss: 0.6106455326080322\n",
            "Epoch 2, Batch 5, Loss: 0.677830696105957\n",
            "Epoch 2, Batch 6, Loss: 0.6813687682151794\n",
            "Epoch 2, Batch 7, Loss: 0.9292269349098206\n",
            "Epoch 2, Batch 8, Loss: 0.4365845024585724\n",
            "Epoch 2, Batch 9, Loss: 0.8968534469604492\n",
            "Epoch 2, Batch 10, Loss: 0.753520131111145\n",
            "Epoch 2, Batch 11, Loss: 0.6017730236053467\n",
            "Epoch 2, Batch 12, Loss: 0.9888006448745728\n",
            "Epoch 2, Batch 13, Loss: 0.64805668592453\n",
            "Epoch 2, Batch 14, Loss: 0.9608039855957031\n",
            "Epoch 2, Batch 15, Loss: 0.49314606189727783\n",
            "Epoch 2, Batch 16, Loss: 0.6796770095825195\n",
            "Epoch 2, Batch 17, Loss: 0.7177649140357971\n",
            "Epoch 2, Batch 18, Loss: 0.8496578335762024\n",
            "Epoch 2, Batch 19, Loss: 0.7585455179214478\n",
            "Epoch 2, Batch 20, Loss: 0.6235200762748718\n",
            "Epoch 2, Batch 21, Loss: 0.5965787172317505\n",
            "Epoch 2, Batch 22, Loss: 0.6563459634780884\n",
            "Epoch 2, Batch 23, Loss: 0.6041980981826782\n",
            "Epoch 2, Batch 24, Loss: 0.7497586011886597\n",
            "Epoch 2, Batch 25, Loss: 0.7342799305915833\n",
            "Epoch 2, Batch 26, Loss: 0.5209656357765198\n",
            "Epoch 2, Batch 27, Loss: 0.8667706251144409\n",
            "Epoch 2, Batch 28, Loss: 0.9502160549163818\n",
            "Epoch 2, Batch 29, Loss: 0.7458968162536621\n",
            "Epoch 2, Batch 30, Loss: 0.6966902017593384\n",
            "Epoch 2, Batch 31, Loss: 0.7350479364395142\n",
            "Epoch 2, Batch 32, Loss: 0.6949787735939026\n",
            "Epoch 2, Batch 33, Loss: 0.7081236839294434\n",
            "Epoch 3, Batch 1, Loss: 0.6277023553848267\n",
            "Epoch 3, Batch 2, Loss: 0.8485490679740906\n",
            "Epoch 3, Batch 3, Loss: 0.8913643956184387\n",
            "Epoch 3, Batch 4, Loss: 1.066239595413208\n",
            "Epoch 3, Batch 5, Loss: 0.6860652565956116\n",
            "Epoch 3, Batch 6, Loss: 0.7238892316818237\n",
            "Epoch 3, Batch 7, Loss: 0.7744559049606323\n",
            "Epoch 3, Batch 8, Loss: 0.5681624412536621\n",
            "Epoch 3, Batch 9, Loss: 0.49913710355758667\n",
            "Epoch 3, Batch 10, Loss: 0.7741053104400635\n",
            "Epoch 3, Batch 11, Loss: 0.7885812520980835\n",
            "Epoch 3, Batch 12, Loss: 0.857082188129425\n",
            "Epoch 3, Batch 13, Loss: 0.7096586227416992\n",
            "Epoch 3, Batch 14, Loss: 0.6923707127571106\n",
            "Epoch 3, Batch 15, Loss: 1.045708417892456\n",
            "Epoch 3, Batch 16, Loss: 0.6412614583969116\n",
            "Epoch 3, Batch 17, Loss: 0.782194972038269\n",
            "Epoch 3, Batch 18, Loss: 0.4602470099925995\n",
            "Epoch 3, Batch 19, Loss: 0.6705347299575806\n",
            "Epoch 3, Batch 20, Loss: 0.4952375888824463\n",
            "Epoch 3, Batch 21, Loss: 0.5478264093399048\n",
            "Epoch 3, Batch 22, Loss: 0.7160603404045105\n",
            "Epoch 3, Batch 23, Loss: 0.5888054370880127\n",
            "Epoch 3, Batch 24, Loss: 1.2855534553527832\n",
            "Epoch 3, Batch 25, Loss: 0.7341300249099731\n",
            "Epoch 3, Batch 26, Loss: 0.7272306680679321\n",
            "Epoch 3, Batch 27, Loss: 0.6469765901565552\n",
            "Epoch 3, Batch 28, Loss: 0.9550883173942566\n",
            "Epoch 3, Batch 29, Loss: 0.9396185874938965\n",
            "Epoch 3, Batch 30, Loss: 0.843788743019104\n",
            "Epoch 3, Batch 31, Loss: 1.134326696395874\n",
            "Epoch 3, Batch 32, Loss: 0.7935636043548584\n",
            "Epoch 3, Batch 33, Loss: 0.6165789365768433\n",
            "Epoch 4, Batch 1, Loss: 0.5735372304916382\n",
            "Epoch 4, Batch 2, Loss: 0.5954638719558716\n",
            "Epoch 4, Batch 3, Loss: 0.7965655326843262\n",
            "Epoch 4, Batch 4, Loss: 0.470727801322937\n",
            "Epoch 4, Batch 5, Loss: 0.5466980934143066\n",
            "Epoch 4, Batch 6, Loss: 0.769900918006897\n",
            "Epoch 4, Batch 7, Loss: 0.8679512739181519\n",
            "Epoch 4, Batch 8, Loss: 0.8867698907852173\n",
            "Epoch 4, Batch 9, Loss: 0.9438636302947998\n",
            "Epoch 4, Batch 10, Loss: 0.5736398696899414\n",
            "Epoch 4, Batch 11, Loss: 0.7609440088272095\n",
            "Epoch 4, Batch 12, Loss: 1.084562063217163\n",
            "Epoch 4, Batch 13, Loss: 0.733005166053772\n",
            "Epoch 4, Batch 14, Loss: 0.4817880690097809\n",
            "Epoch 4, Batch 15, Loss: 0.7888391017913818\n",
            "Epoch 4, Batch 16, Loss: 0.8098782300949097\n",
            "Epoch 4, Batch 17, Loss: 0.6836142539978027\n",
            "Epoch 4, Batch 18, Loss: 0.6817941665649414\n",
            "Epoch 4, Batch 19, Loss: 0.8854037523269653\n",
            "Epoch 4, Batch 20, Loss: 0.9322432279586792\n",
            "Epoch 4, Batch 21, Loss: 0.6703735589981079\n",
            "Epoch 4, Batch 22, Loss: 0.72789466381073\n",
            "Epoch 4, Batch 23, Loss: 0.7388990521430969\n",
            "Epoch 4, Batch 24, Loss: 0.847467303276062\n",
            "Epoch 4, Batch 25, Loss: 0.5534425973892212\n",
            "Epoch 4, Batch 26, Loss: 0.8358888030052185\n",
            "Epoch 4, Batch 27, Loss: 0.5884718894958496\n",
            "Epoch 4, Batch 28, Loss: 0.7082334756851196\n",
            "Epoch 4, Batch 29, Loss: 0.5027090311050415\n",
            "Epoch 4, Batch 30, Loss: 0.6990950107574463\n",
            "Epoch 4, Batch 31, Loss: 0.5657714009284973\n",
            "Epoch 4, Batch 32, Loss: 0.6679623126983643\n",
            "Epoch 4, Batch 33, Loss: 0.5870643854141235\n",
            "Epoch 5, Batch 1, Loss: 0.603057324886322\n",
            "Epoch 5, Batch 2, Loss: 0.9572559595108032\n",
            "Epoch 5, Batch 3, Loss: 0.4692094326019287\n",
            "Epoch 5, Batch 4, Loss: 0.885084331035614\n",
            "Epoch 5, Batch 5, Loss: 0.5845377445220947\n",
            "Epoch 5, Batch 6, Loss: 0.8620061874389648\n",
            "Epoch 5, Batch 7, Loss: 0.7189953923225403\n",
            "Epoch 5, Batch 8, Loss: 1.0017062425613403\n",
            "Epoch 5, Batch 9, Loss: 0.681824803352356\n",
            "Epoch 5, Batch 10, Loss: 0.7069815397262573\n",
            "Epoch 5, Batch 11, Loss: 0.9964364767074585\n",
            "Epoch 5, Batch 12, Loss: 0.840927243232727\n",
            "Epoch 5, Batch 13, Loss: 0.5271355509757996\n",
            "Epoch 5, Batch 14, Loss: 0.6788575649261475\n",
            "Epoch 5, Batch 15, Loss: 0.6683323383331299\n",
            "Epoch 5, Batch 16, Loss: 0.37960439920425415\n",
            "Epoch 5, Batch 17, Loss: 0.6619359254837036\n",
            "Epoch 5, Batch 18, Loss: 0.6432301998138428\n",
            "Epoch 5, Batch 19, Loss: 0.72541344165802\n",
            "Epoch 5, Batch 20, Loss: 0.6112030744552612\n",
            "Epoch 5, Batch 21, Loss: 0.6254370808601379\n",
            "Epoch 5, Batch 22, Loss: 0.7532123327255249\n",
            "Epoch 5, Batch 23, Loss: 0.42762401700019836\n",
            "Epoch 5, Batch 24, Loss: 0.7996385097503662\n",
            "Epoch 5, Batch 25, Loss: 0.736427366733551\n",
            "Epoch 5, Batch 26, Loss: 0.8928431272506714\n",
            "Epoch 5, Batch 27, Loss: 0.8816931247711182\n",
            "Epoch 5, Batch 28, Loss: 0.6214649677276611\n",
            "Epoch 5, Batch 29, Loss: 0.6066662073135376\n",
            "Epoch 5, Batch 30, Loss: 0.6868540048599243\n",
            "Epoch 5, Batch 31, Loss: 0.6680269241333008\n",
            "Epoch 5, Batch 32, Loss: 0.8583216667175293\n",
            "Epoch 5, Batch 33, Loss: 0.3916088938713074\n",
            "Epoch 6, Batch 1, Loss: 0.6342451572418213\n",
            "Epoch 6, Batch 2, Loss: 0.44996678829193115\n",
            "Epoch 6, Batch 3, Loss: 0.7196205854415894\n",
            "Epoch 6, Batch 4, Loss: 0.5604592561721802\n",
            "Epoch 6, Batch 5, Loss: 0.5341393947601318\n",
            "Epoch 6, Batch 6, Loss: 0.8044911623001099\n",
            "Epoch 6, Batch 7, Loss: 0.7337846755981445\n",
            "Epoch 6, Batch 8, Loss: 0.7537025809288025\n",
            "Epoch 6, Batch 9, Loss: 0.7802572846412659\n",
            "Epoch 6, Batch 10, Loss: 0.7028910517692566\n",
            "Epoch 6, Batch 11, Loss: 0.5385465621948242\n",
            "Epoch 6, Batch 12, Loss: 0.571954071521759\n",
            "Epoch 6, Batch 13, Loss: 0.6771273612976074\n",
            "Epoch 6, Batch 14, Loss: 0.7259331941604614\n",
            "Epoch 6, Batch 15, Loss: 0.8099555373191833\n",
            "Epoch 6, Batch 16, Loss: 0.672706127166748\n",
            "Epoch 6, Batch 17, Loss: 0.6820249557495117\n",
            "Epoch 6, Batch 18, Loss: 0.8395208120346069\n",
            "Epoch 6, Batch 19, Loss: 0.8787645101547241\n",
            "Epoch 6, Batch 20, Loss: 0.8205715417861938\n",
            "Epoch 6, Batch 21, Loss: 0.549588143825531\n",
            "Epoch 6, Batch 22, Loss: 0.5460596084594727\n",
            "Epoch 6, Batch 23, Loss: 0.8011536002159119\n",
            "Epoch 6, Batch 24, Loss: 0.8773077726364136\n",
            "Epoch 6, Batch 25, Loss: 1.0603916645050049\n",
            "Epoch 6, Batch 26, Loss: 0.7408742904663086\n",
            "Epoch 6, Batch 27, Loss: 0.7176219820976257\n",
            "Epoch 6, Batch 28, Loss: 0.6373665928840637\n",
            "Epoch 6, Batch 29, Loss: 0.509017825126648\n",
            "Epoch 6, Batch 30, Loss: 0.6907151341438293\n",
            "Epoch 6, Batch 31, Loss: 0.7463523149490356\n",
            "Epoch 6, Batch 32, Loss: 0.45215341448783875\n",
            "Epoch 6, Batch 33, Loss: 0.4828020930290222\n",
            "Epoch 7, Batch 1, Loss: 0.6747827529907227\n",
            "Epoch 7, Batch 2, Loss: 0.6921589970588684\n",
            "Epoch 7, Batch 3, Loss: 0.6205279231071472\n",
            "Epoch 7, Batch 4, Loss: 1.0909857749938965\n",
            "Epoch 7, Batch 5, Loss: 0.5780766010284424\n",
            "Epoch 7, Batch 6, Loss: 0.663489580154419\n",
            "Epoch 7, Batch 7, Loss: 0.7844841480255127\n",
            "Epoch 7, Batch 8, Loss: 0.5940725207328796\n",
            "Epoch 7, Batch 9, Loss: 0.49060603976249695\n",
            "Epoch 7, Batch 10, Loss: 0.9139339923858643\n",
            "Epoch 7, Batch 11, Loss: 0.8220325708389282\n",
            "Epoch 7, Batch 12, Loss: 0.8108044862747192\n",
            "Epoch 7, Batch 13, Loss: 0.8260786533355713\n",
            "Epoch 7, Batch 14, Loss: 0.6773068308830261\n",
            "Epoch 7, Batch 15, Loss: 0.7650169134140015\n",
            "Epoch 7, Batch 16, Loss: 0.8244360685348511\n",
            "Epoch 7, Batch 17, Loss: 0.9311401844024658\n",
            "Epoch 7, Batch 18, Loss: 0.7485371232032776\n",
            "Epoch 7, Batch 19, Loss: 0.5562288761138916\n",
            "Epoch 7, Batch 20, Loss: 0.9023157358169556\n",
            "Epoch 7, Batch 21, Loss: 0.6883716583251953\n",
            "Epoch 7, Batch 22, Loss: 0.505707859992981\n",
            "Epoch 7, Batch 23, Loss: 0.6338876485824585\n",
            "Epoch 7, Batch 24, Loss: 0.7699937224388123\n",
            "Epoch 7, Batch 25, Loss: 0.6457757949829102\n",
            "Epoch 7, Batch 26, Loss: 0.6225276589393616\n",
            "Epoch 7, Batch 27, Loss: 0.6112021207809448\n",
            "Epoch 7, Batch 28, Loss: 0.6087762117385864\n",
            "Epoch 7, Batch 29, Loss: 0.637115478515625\n",
            "Epoch 7, Batch 30, Loss: 0.7532609701156616\n",
            "Epoch 7, Batch 31, Loss: 0.49303698539733887\n",
            "Epoch 7, Batch 32, Loss: 0.7214820981025696\n",
            "Epoch 7, Batch 33, Loss: 0.3083922266960144\n",
            "Epoch 8, Batch 1, Loss: 0.6946777701377869\n",
            "Epoch 8, Batch 2, Loss: 0.7582395672798157\n",
            "Epoch 8, Batch 3, Loss: 0.7684850692749023\n",
            "Epoch 8, Batch 4, Loss: 0.780392587184906\n",
            "Epoch 8, Batch 5, Loss: 0.6281592845916748\n",
            "Epoch 8, Batch 6, Loss: 0.7084855437278748\n",
            "Epoch 8, Batch 7, Loss: 0.7582706212997437\n",
            "Epoch 8, Batch 8, Loss: 0.8071564435958862\n",
            "Epoch 8, Batch 9, Loss: 0.6667799949645996\n",
            "Epoch 8, Batch 10, Loss: 1.0198132991790771\n",
            "Epoch 8, Batch 11, Loss: 0.6967724561691284\n",
            "Epoch 8, Batch 12, Loss: 0.5097991228103638\n",
            "Epoch 8, Batch 13, Loss: 0.6774754524230957\n",
            "Epoch 8, Batch 14, Loss: 0.882522702217102\n",
            "Epoch 8, Batch 15, Loss: 0.8889313340187073\n",
            "Epoch 8, Batch 16, Loss: 0.35041719675064087\n",
            "Epoch 8, Batch 17, Loss: 0.6711211204528809\n",
            "Epoch 8, Batch 18, Loss: 0.9422073364257812\n",
            "Epoch 8, Batch 19, Loss: 0.8314664363861084\n",
            "Epoch 8, Batch 20, Loss: 0.9050189256668091\n",
            "Epoch 8, Batch 21, Loss: 0.6512733101844788\n",
            "Epoch 8, Batch 22, Loss: 0.43283969163894653\n",
            "Epoch 8, Batch 23, Loss: 0.614566445350647\n",
            "Epoch 8, Batch 24, Loss: 0.5371150970458984\n",
            "Epoch 8, Batch 25, Loss: 0.618664026260376\n",
            "Epoch 8, Batch 26, Loss: 0.8216907978057861\n",
            "Epoch 8, Batch 27, Loss: 0.4755440950393677\n",
            "Epoch 8, Batch 28, Loss: 0.5109217762947083\n",
            "Epoch 8, Batch 29, Loss: 0.8321866989135742\n",
            "Epoch 8, Batch 30, Loss: 0.6929149627685547\n",
            "Epoch 8, Batch 31, Loss: 0.8598039150238037\n",
            "Epoch 8, Batch 32, Loss: 0.802085280418396\n",
            "Epoch 8, Batch 33, Loss: 0.5288980603218079\n",
            "Epoch 9, Batch 1, Loss: 0.6296952962875366\n",
            "Epoch 9, Batch 2, Loss: 0.6801303625106812\n",
            "Epoch 9, Batch 3, Loss: 0.7377023100852966\n",
            "Epoch 9, Batch 4, Loss: 0.8677836060523987\n",
            "Epoch 9, Batch 5, Loss: 1.0454000234603882\n",
            "Epoch 9, Batch 6, Loss: 0.8955463171005249\n",
            "Epoch 9, Batch 7, Loss: 0.8721724152565002\n",
            "Epoch 9, Batch 8, Loss: 0.7050579786300659\n",
            "Epoch 9, Batch 9, Loss: 0.7021813988685608\n",
            "Epoch 9, Batch 10, Loss: 0.6064094305038452\n",
            "Epoch 9, Batch 11, Loss: 0.6287645697593689\n",
            "Epoch 9, Batch 12, Loss: 0.9910223484039307\n",
            "Epoch 9, Batch 13, Loss: 0.6811891794204712\n",
            "Epoch 9, Batch 14, Loss: 0.7744371891021729\n",
            "Epoch 9, Batch 15, Loss: 0.42720621824264526\n",
            "Epoch 9, Batch 16, Loss: 0.5442566871643066\n",
            "Epoch 9, Batch 17, Loss: 0.7672184705734253\n",
            "Epoch 9, Batch 18, Loss: 0.6056667566299438\n",
            "Epoch 9, Batch 19, Loss: 0.8425302505493164\n",
            "Epoch 9, Batch 20, Loss: 0.815686047077179\n",
            "Epoch 9, Batch 21, Loss: 0.5580376386642456\n",
            "Epoch 9, Batch 22, Loss: 0.6427976489067078\n",
            "Epoch 9, Batch 23, Loss: 0.6180711984634399\n",
            "Epoch 9, Batch 24, Loss: 0.9876819849014282\n",
            "Epoch 9, Batch 25, Loss: 0.7199394702911377\n",
            "Epoch 9, Batch 26, Loss: 0.7780612111091614\n",
            "Epoch 9, Batch 27, Loss: 0.5918282270431519\n",
            "Epoch 9, Batch 28, Loss: 0.6090294122695923\n",
            "Epoch 9, Batch 29, Loss: 0.6272757649421692\n",
            "Epoch 9, Batch 30, Loss: 0.4200715720653534\n",
            "Epoch 9, Batch 31, Loss: 0.6568474769592285\n",
            "Epoch 9, Batch 32, Loss: 0.628485918045044\n",
            "Epoch 9, Batch 33, Loss: 0.18996113538742065\n",
            "Epoch 10, Batch 1, Loss: 0.5013514161109924\n",
            "Epoch 10, Batch 2, Loss: 0.931786060333252\n",
            "Epoch 10, Batch 3, Loss: 0.8527532815933228\n",
            "Epoch 10, Batch 4, Loss: 0.38460856676101685\n",
            "Epoch 10, Batch 5, Loss: 0.5636216998100281\n",
            "Epoch 10, Batch 6, Loss: 0.6366291642189026\n",
            "Epoch 10, Batch 7, Loss: 0.7263926863670349\n",
            "Epoch 10, Batch 8, Loss: 0.6616435050964355\n",
            "Epoch 10, Batch 9, Loss: 0.5098876953125\n",
            "Epoch 10, Batch 10, Loss: 0.8988507986068726\n",
            "Epoch 10, Batch 11, Loss: 1.0117979049682617\n",
            "Epoch 10, Batch 12, Loss: 0.6893792748451233\n",
            "Epoch 10, Batch 13, Loss: 0.4776916801929474\n",
            "Epoch 10, Batch 14, Loss: 1.0051014423370361\n",
            "Epoch 10, Batch 15, Loss: 0.34333130717277527\n",
            "Epoch 10, Batch 16, Loss: 0.8104381561279297\n",
            "Epoch 10, Batch 17, Loss: 1.084808349609375\n",
            "Epoch 10, Batch 18, Loss: 0.5007873773574829\n",
            "Epoch 10, Batch 19, Loss: 0.8942786455154419\n",
            "Epoch 10, Batch 20, Loss: 0.6425241827964783\n",
            "Epoch 10, Batch 21, Loss: 0.5764050483703613\n",
            "Epoch 10, Batch 22, Loss: 0.8408324718475342\n",
            "Epoch 10, Batch 23, Loss: 0.41958358883857727\n",
            "Epoch 10, Batch 24, Loss: 0.6290823817253113\n",
            "Epoch 10, Batch 25, Loss: 0.6807770729064941\n",
            "Epoch 10, Batch 26, Loss: 0.66275554895401\n",
            "Epoch 10, Batch 27, Loss: 0.8736962080001831\n",
            "Epoch 10, Batch 28, Loss: 0.5744600296020508\n",
            "Epoch 10, Batch 29, Loss: 0.6999223828315735\n",
            "Epoch 10, Batch 30, Loss: 0.9617084264755249\n",
            "Epoch 10, Batch 31, Loss: 0.8987386226654053\n",
            "Epoch 10, Batch 32, Loss: 0.7017848491668701\n",
            "Epoch 10, Batch 33, Loss: 0.8792430758476257\n",
            "Epoch 11, Batch 1, Loss: 0.6512715816497803\n",
            "Epoch 11, Batch 2, Loss: 0.8446390628814697\n",
            "Epoch 11, Batch 3, Loss: 1.0016369819641113\n",
            "Epoch 11, Batch 4, Loss: 0.7913784980773926\n",
            "Epoch 11, Batch 5, Loss: 0.5191115140914917\n",
            "Epoch 11, Batch 6, Loss: 0.6699984669685364\n",
            "Epoch 11, Batch 7, Loss: 0.8199734091758728\n",
            "Epoch 11, Batch 8, Loss: 0.4852190315723419\n",
            "Epoch 11, Batch 9, Loss: 0.5470380187034607\n",
            "Epoch 11, Batch 10, Loss: 0.638249397277832\n",
            "Epoch 11, Batch 11, Loss: 0.5675187706947327\n",
            "Epoch 11, Batch 12, Loss: 0.5296350121498108\n",
            "Epoch 11, Batch 13, Loss: 0.7175054550170898\n",
            "Epoch 11, Batch 14, Loss: 0.5260811448097229\n",
            "Epoch 11, Batch 15, Loss: 0.8267279267311096\n",
            "Epoch 11, Batch 16, Loss: 0.8401045203208923\n",
            "Epoch 11, Batch 17, Loss: 0.6444377899169922\n",
            "Epoch 11, Batch 18, Loss: 1.0988672971725464\n",
            "Epoch 11, Batch 19, Loss: 0.7479164600372314\n",
            "Epoch 11, Batch 20, Loss: 0.5453271269798279\n",
            "Epoch 11, Batch 21, Loss: 0.529430627822876\n",
            "Epoch 11, Batch 22, Loss: 0.552165150642395\n",
            "Epoch 11, Batch 23, Loss: 0.5941400527954102\n",
            "Epoch 11, Batch 24, Loss: 0.9131914377212524\n",
            "Epoch 11, Batch 25, Loss: 0.9481102228164673\n",
            "Epoch 11, Batch 26, Loss: 0.7989669442176819\n",
            "Epoch 11, Batch 27, Loss: 0.6809223890304565\n",
            "Epoch 11, Batch 28, Loss: 0.4746144115924835\n",
            "Epoch 11, Batch 29, Loss: 0.685230016708374\n",
            "Epoch 11, Batch 30, Loss: 0.7837643623352051\n",
            "Epoch 11, Batch 31, Loss: 0.7980339527130127\n",
            "Epoch 11, Batch 32, Loss: 0.6283174753189087\n",
            "Epoch 11, Batch 33, Loss: 1.210227608680725\n",
            "Epoch 12, Batch 1, Loss: 0.5668293237686157\n",
            "Epoch 12, Batch 2, Loss: 0.6804043650627136\n",
            "Epoch 12, Batch 3, Loss: 0.5446971654891968\n",
            "Epoch 12, Batch 4, Loss: 0.7679351568222046\n",
            "Epoch 12, Batch 5, Loss: 0.616378664970398\n",
            "Epoch 12, Batch 6, Loss: 0.7167346477508545\n",
            "Epoch 12, Batch 7, Loss: 0.7036055326461792\n",
            "Epoch 12, Batch 8, Loss: 1.0102338790893555\n",
            "Epoch 12, Batch 9, Loss: 0.8612566590309143\n",
            "Epoch 12, Batch 10, Loss: 0.941957950592041\n",
            "Epoch 12, Batch 11, Loss: 0.4684251546859741\n",
            "Epoch 12, Batch 12, Loss: 0.6966270208358765\n",
            "Epoch 12, Batch 13, Loss: 0.9926329851150513\n",
            "Epoch 12, Batch 14, Loss: 0.7042464017868042\n",
            "Epoch 12, Batch 15, Loss: 0.6635320782661438\n",
            "Epoch 12, Batch 16, Loss: 0.6105033159255981\n",
            "Epoch 12, Batch 17, Loss: 0.6423937678337097\n",
            "Epoch 12, Batch 18, Loss: 0.6307055354118347\n",
            "Epoch 12, Batch 19, Loss: 0.36524856090545654\n",
            "Epoch 12, Batch 20, Loss: 0.5691806077957153\n",
            "Epoch 12, Batch 21, Loss: 0.7100502252578735\n",
            "Epoch 12, Batch 22, Loss: 1.0399701595306396\n",
            "Epoch 12, Batch 23, Loss: 0.962874174118042\n",
            "Epoch 12, Batch 24, Loss: 1.0091582536697388\n",
            "Epoch 12, Batch 25, Loss: 0.6332277059555054\n",
            "Epoch 12, Batch 26, Loss: 0.6865785121917725\n",
            "Epoch 12, Batch 27, Loss: 0.7814379930496216\n",
            "Epoch 12, Batch 28, Loss: 0.6642755270004272\n",
            "Epoch 12, Batch 29, Loss: 0.7396458387374878\n",
            "Epoch 12, Batch 30, Loss: 0.7128851413726807\n",
            "Epoch 12, Batch 31, Loss: 0.6383814811706543\n",
            "Epoch 12, Batch 32, Loss: 0.5209403038024902\n",
            "Epoch 12, Batch 33, Loss: 0.2547770142555237\n",
            "Epoch 13, Batch 1, Loss: 0.7773574590682983\n",
            "Epoch 13, Batch 2, Loss: 0.8233637809753418\n",
            "Epoch 13, Batch 3, Loss: 0.7086091041564941\n",
            "Epoch 13, Batch 4, Loss: 0.6890736818313599\n",
            "Epoch 13, Batch 5, Loss: 0.7795370817184448\n",
            "Epoch 13, Batch 6, Loss: 0.8273000717163086\n",
            "Epoch 13, Batch 7, Loss: 0.7873765230178833\n",
            "Epoch 13, Batch 8, Loss: 0.8991213440895081\n",
            "Epoch 13, Batch 9, Loss: 0.8740054965019226\n",
            "Epoch 13, Batch 10, Loss: 0.7296949625015259\n",
            "Epoch 13, Batch 11, Loss: 0.5490077137947083\n",
            "Epoch 13, Batch 12, Loss: 0.8862180709838867\n",
            "Epoch 13, Batch 13, Loss: 0.7870504856109619\n",
            "Epoch 13, Batch 14, Loss: 0.8840521574020386\n",
            "Epoch 13, Batch 15, Loss: 0.6743893623352051\n",
            "Epoch 13, Batch 16, Loss: 0.6046045422554016\n",
            "Epoch 13, Batch 17, Loss: 0.888590395450592\n",
            "Epoch 13, Batch 18, Loss: 0.482588529586792\n",
            "Epoch 13, Batch 19, Loss: 0.9210320711135864\n",
            "Epoch 13, Batch 20, Loss: 0.7976565361022949\n",
            "Epoch 13, Batch 21, Loss: 0.4873100221157074\n",
            "Epoch 13, Batch 22, Loss: 0.7365992069244385\n",
            "Epoch 13, Batch 23, Loss: 0.6117261052131653\n",
            "Epoch 13, Batch 24, Loss: 0.7804399728775024\n",
            "Epoch 13, Batch 25, Loss: 0.5902132391929626\n",
            "Epoch 13, Batch 26, Loss: 0.5921852588653564\n",
            "Epoch 13, Batch 27, Loss: 0.712218701839447\n",
            "Epoch 13, Batch 28, Loss: 0.634415864944458\n",
            "Epoch 13, Batch 29, Loss: 0.5763733386993408\n",
            "Epoch 13, Batch 30, Loss: 0.6967517137527466\n",
            "Epoch 13, Batch 31, Loss: 0.4612959027290344\n",
            "Epoch 13, Batch 32, Loss: 0.7057127952575684\n",
            "Epoch 13, Batch 33, Loss: 0.443291574716568\n",
            "Epoch 14, Batch 1, Loss: 0.5397449731826782\n",
            "Epoch 14, Batch 2, Loss: 0.5069456696510315\n",
            "Epoch 14, Batch 3, Loss: 0.9570624828338623\n",
            "Epoch 14, Batch 4, Loss: 0.7302937507629395\n",
            "Epoch 14, Batch 5, Loss: 0.5720245242118835\n",
            "Epoch 14, Batch 6, Loss: 0.6708855628967285\n",
            "Epoch 14, Batch 7, Loss: 0.5919808745384216\n",
            "Epoch 14, Batch 8, Loss: 1.0300043821334839\n",
            "Epoch 14, Batch 9, Loss: 0.82936030626297\n",
            "Epoch 14, Batch 10, Loss: 0.551185667514801\n",
            "Epoch 14, Batch 11, Loss: 0.6256976127624512\n",
            "Epoch 14, Batch 12, Loss: 0.8213674426078796\n",
            "Epoch 14, Batch 13, Loss: 0.8504365086555481\n",
            "Epoch 14, Batch 14, Loss: 0.9947167634963989\n",
            "Epoch 14, Batch 15, Loss: 0.6255333423614502\n",
            "Epoch 14, Batch 16, Loss: 0.7405760288238525\n",
            "Epoch 14, Batch 17, Loss: 0.7689546942710876\n",
            "Epoch 14, Batch 18, Loss: 0.6808269023895264\n",
            "Epoch 14, Batch 19, Loss: 0.5314264297485352\n",
            "Epoch 14, Batch 20, Loss: 0.7932521104812622\n",
            "Epoch 14, Batch 21, Loss: 0.6071237325668335\n",
            "Epoch 14, Batch 22, Loss: 0.7157769203186035\n",
            "Epoch 14, Batch 23, Loss: 0.8198063373565674\n",
            "Epoch 14, Batch 24, Loss: 0.5123798251152039\n",
            "Epoch 14, Batch 25, Loss: 0.674494743347168\n",
            "Epoch 14, Batch 26, Loss: 0.5456702709197998\n",
            "Epoch 14, Batch 27, Loss: 0.8686032891273499\n",
            "Epoch 14, Batch 28, Loss: 1.0507664680480957\n",
            "Epoch 14, Batch 29, Loss: 0.4723326563835144\n",
            "Epoch 14, Batch 30, Loss: 0.5910158157348633\n",
            "Epoch 14, Batch 31, Loss: 0.9019299745559692\n",
            "Epoch 14, Batch 32, Loss: 0.8447371125221252\n",
            "Epoch 14, Batch 33, Loss: 0.30793625116348267\n",
            "Epoch 15, Batch 1, Loss: 0.5772451758384705\n",
            "Epoch 15, Batch 2, Loss: 0.4766010046005249\n",
            "Epoch 15, Batch 3, Loss: 0.6436362266540527\n",
            "Epoch 15, Batch 4, Loss: 0.6234688758850098\n",
            "Epoch 15, Batch 5, Loss: 0.8037732839584351\n",
            "Epoch 15, Batch 6, Loss: 0.9453983306884766\n",
            "Epoch 15, Batch 7, Loss: 0.49987518787384033\n",
            "Epoch 15, Batch 8, Loss: 0.674196720123291\n",
            "Epoch 15, Batch 9, Loss: 0.8188190460205078\n",
            "Epoch 15, Batch 10, Loss: 0.5449215173721313\n",
            "Epoch 15, Batch 11, Loss: 0.7781872749328613\n",
            "Epoch 15, Batch 12, Loss: 0.9904265403747559\n",
            "Epoch 15, Batch 13, Loss: 0.5965552926063538\n",
            "Epoch 15, Batch 14, Loss: 0.7289022207260132\n",
            "Epoch 15, Batch 15, Loss: 0.6501153707504272\n",
            "Epoch 15, Batch 16, Loss: 0.593917965888977\n",
            "Epoch 15, Batch 17, Loss: 0.8030942678451538\n",
            "Epoch 15, Batch 18, Loss: 0.7140880227088928\n",
            "Epoch 15, Batch 19, Loss: 0.623530387878418\n",
            "Epoch 15, Batch 20, Loss: 0.6754775643348694\n",
            "Epoch 15, Batch 21, Loss: 0.7435364723205566\n",
            "Epoch 15, Batch 22, Loss: 0.6225156784057617\n",
            "Epoch 15, Batch 23, Loss: 0.7970381379127502\n",
            "Epoch 15, Batch 24, Loss: 0.6475066542625427\n",
            "Epoch 15, Batch 25, Loss: 0.626863956451416\n",
            "Epoch 15, Batch 26, Loss: 1.1873468160629272\n",
            "Epoch 15, Batch 27, Loss: 0.6133419871330261\n",
            "Epoch 15, Batch 28, Loss: 0.5694653987884521\n",
            "Epoch 15, Batch 29, Loss: 0.6205555200576782\n",
            "Epoch 15, Batch 30, Loss: 0.6506668329238892\n",
            "Epoch 15, Batch 31, Loss: 0.692512035369873\n",
            "Epoch 15, Batch 32, Loss: 0.8466185331344604\n",
            "Epoch 15, Batch 33, Loss: 0.6976784467697144\n",
            "RMSE for this set: 0.7882855477306413\n",
            "\n",
            "Best parameters:\n",
            "Batch size: 0.75\n",
            "Learning rate: 0.01\n",
            "LSTM units: 200\n",
            "Best RMSE: 0.7785940308011187\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "drop_outs = [0.5, 0.75]\n",
        "learning_rates = [0.1, 0.01, 0.005]\n",
        "lstm_units = [200, 300]\n",
        "\n",
        "param_grid = list(itertools.product(drop_outs, learning_rates, lstm_units))\n",
        "\n",
        "best_rmse = 100\n",
        "best_params = None\n",
        "\n",
        "for drop_out, lr, lstm_H in param_grid:\n",
        "    print(f'Training with dropout={drop_out}, lr={lr}, lstm_H={lstm_H}')\n",
        "    config = LSTM_Config(lstm_H=lstm_H, drop_out=drop_out, lr=lr)\n",
        "    model = LSTM_NeuralModel(config)\n",
        "\n",
        "    trained_model,_,_ = train_model(model,dataset, config, num_epochs=15) #epochs to 15 according to the val pattern\n",
        "    r2_value, rmse_value = test_model(trained_model, dataset, config)\n",
        "\n",
        "    # Check if this combination gives a better RMSE\n",
        "    if rmse_value < best_rmse:\n",
        "        best_rmse = rmse_value\n",
        "        best_params = (drop_out, lr, lstm_H)\n",
        "\n",
        "    print(f'RMSE for this set: {rmse_value}')\n",
        "\n",
        "print('\\nBest parameters:')\n",
        "print(f'Drop out rate: {best_params[0]}')\n",
        "print(f'Learning rate: {best_params[1]}')\n",
        "print(f'LSTM units: {best_params[2]}')\n",
        "print(f'Best RMSE: {best_rmse}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nk9ZOXdBRndS"
      },
      "source": [
        "Best parameters:\n",
        "\n",
        "Dropout: 0.75\n",
        "\n",
        "Learning rate: 0.01\n",
        "\n",
        "LSTM units: 200\n",
        "\n",
        "Best RMSE: 0.7785940308011187"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2R4QC5LSez1"
      },
      "source": [
        "# 2. Transfer learning on US Soybean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TI7e-fFBTyeE",
        "outputId": "5ef104d5-4f35-41a2-dc9b-251c9e37367c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Batch 1, Loss: 8.444135665893555\n",
            "Epoch 1, Batch 2, Loss: 4.23391056060791\n",
            "Epoch 1, Batch 3, Loss: 156.62606811523438\n",
            "Epoch 1, Batch 4, Loss: 13.155106544494629\n",
            "Epoch 1, Batch 5, Loss: 2.5904438495635986\n",
            "Epoch 1, Batch 6, Loss: 5.687861442565918\n",
            "Epoch 1, Batch 7, Loss: 4.462691307067871\n",
            "Epoch 1, Batch 8, Loss: 5.401070594787598\n",
            "Epoch 1, Batch 9, Loss: 3.13135027885437\n",
            "Epoch 1, Batch 10, Loss: 3.1927878856658936\n",
            "Epoch 1, Batch 11, Loss: 1.5728464126586914\n",
            "Epoch 1, Batch 12, Loss: 0.41552990674972534\n",
            "Epoch 1, Batch 13, Loss: 2.360414981842041\n",
            "Epoch 1, Batch 14, Loss: 1.2094769477844238\n",
            "Epoch 1, Batch 15, Loss: 0.8995803594589233\n",
            "Epoch 1, Batch 16, Loss: 0.6203769445419312\n",
            "Epoch 1, Batch 17, Loss: 1.0515596866607666\n",
            "Epoch 1, Batch 18, Loss: 0.9306895136833191\n",
            "Epoch 1, Batch 19, Loss: 0.6813889741897583\n",
            "Epoch 1, Batch 20, Loss: 0.5063807964324951\n",
            "Epoch 1, Batch 21, Loss: 0.7880254983901978\n",
            "Epoch 1, Batch 22, Loss: 0.5281727910041809\n",
            "Epoch 1, Batch 23, Loss: 0.5911122560501099\n",
            "Epoch 1, Batch 24, Loss: 0.6062567830085754\n",
            "Epoch 1, Batch 25, Loss: 0.702894389629364\n",
            "Epoch 1, Batch 26, Loss: 0.7269663214683533\n",
            "Epoch 1, Batch 27, Loss: 0.7563398480415344\n",
            "Epoch 1, Batch 28, Loss: 0.7368215322494507\n",
            "Epoch 1, Batch 29, Loss: 0.5519751310348511\n",
            "Epoch 1, Batch 30, Loss: 0.6764147281646729\n",
            "Epoch 1, Batch 31, Loss: 0.6315048336982727\n",
            "Epoch 1, Batch 32, Loss: 0.7417770624160767\n",
            "Epoch 1, Batch 33, Loss: 0.6485277414321899\n",
            "Epoch 2, Batch 1, Loss: 0.548344612121582\n",
            "Epoch 2, Batch 2, Loss: 0.5321152210235596\n",
            "Epoch 2, Batch 3, Loss: 0.6604858636856079\n",
            "Epoch 2, Batch 4, Loss: 0.6043015718460083\n",
            "Epoch 2, Batch 5, Loss: 0.5942996740341187\n",
            "Epoch 2, Batch 6, Loss: 0.6757295727729797\n",
            "Epoch 2, Batch 7, Loss: 0.5073965191841125\n",
            "Epoch 2, Batch 8, Loss: 0.4703831076622009\n",
            "Epoch 2, Batch 9, Loss: 0.6500345468521118\n",
            "Epoch 2, Batch 10, Loss: 0.8945397138595581\n",
            "Epoch 2, Batch 11, Loss: 0.6921709775924683\n",
            "Epoch 2, Batch 12, Loss: 1.0304416418075562\n",
            "Epoch 2, Batch 13, Loss: 0.5368596315383911\n",
            "Epoch 2, Batch 14, Loss: 0.686939001083374\n",
            "Epoch 2, Batch 15, Loss: 0.6930583715438843\n",
            "Epoch 2, Batch 16, Loss: 0.9811701774597168\n",
            "Epoch 2, Batch 17, Loss: 0.6969097852706909\n",
            "Epoch 2, Batch 18, Loss: 0.63019198179245\n",
            "Epoch 2, Batch 19, Loss: 0.6127054691314697\n",
            "Epoch 2, Batch 20, Loss: 0.8453022241592407\n",
            "Epoch 2, Batch 21, Loss: 0.7174765467643738\n",
            "Epoch 2, Batch 22, Loss: 0.8230421543121338\n",
            "Epoch 2, Batch 23, Loss: 0.7717679142951965\n",
            "Epoch 2, Batch 24, Loss: 0.7811592221260071\n",
            "Epoch 2, Batch 25, Loss: 0.8612300753593445\n",
            "Epoch 2, Batch 26, Loss: 1.1023099422454834\n",
            "Epoch 2, Batch 27, Loss: 0.5932235717773438\n",
            "Epoch 2, Batch 28, Loss: 0.7290107607841492\n",
            "Epoch 2, Batch 29, Loss: 0.5850350260734558\n",
            "Epoch 2, Batch 30, Loss: 0.42079323530197144\n",
            "Epoch 2, Batch 31, Loss: 0.6577001810073853\n",
            "Epoch 2, Batch 32, Loss: 0.9934514760971069\n",
            "Epoch 2, Batch 33, Loss: 0.7097262144088745\n",
            "Epoch 3, Batch 1, Loss: 0.8172230124473572\n",
            "Epoch 3, Batch 2, Loss: 0.7984347343444824\n",
            "Epoch 3, Batch 3, Loss: 0.797270655632019\n",
            "Epoch 3, Batch 4, Loss: 0.6538420915603638\n",
            "Epoch 3, Batch 5, Loss: 0.6872360706329346\n",
            "Epoch 3, Batch 6, Loss: 0.8967339992523193\n",
            "Epoch 3, Batch 7, Loss: 0.8744499683380127\n",
            "Epoch 3, Batch 8, Loss: 0.5570591688156128\n",
            "Epoch 3, Batch 9, Loss: 0.9474286437034607\n",
            "Epoch 3, Batch 10, Loss: 0.6468440294265747\n",
            "Epoch 3, Batch 11, Loss: 0.5998070240020752\n",
            "Epoch 3, Batch 12, Loss: 0.8151309490203857\n",
            "Epoch 3, Batch 13, Loss: 0.5577972531318665\n",
            "Epoch 3, Batch 14, Loss: 0.3983883857727051\n",
            "Epoch 3, Batch 15, Loss: 0.7444068789482117\n",
            "Epoch 3, Batch 16, Loss: 0.6787167191505432\n",
            "Epoch 3, Batch 17, Loss: 0.6946921348571777\n",
            "Epoch 3, Batch 18, Loss: 0.7258718013763428\n",
            "Epoch 3, Batch 19, Loss: 0.6782176494598389\n",
            "Epoch 3, Batch 20, Loss: 0.4582628011703491\n",
            "Epoch 3, Batch 21, Loss: 0.6103559136390686\n",
            "Epoch 3, Batch 22, Loss: 0.43842238187789917\n",
            "Epoch 3, Batch 23, Loss: 0.9088397026062012\n",
            "Epoch 3, Batch 24, Loss: 0.7421514987945557\n",
            "Epoch 3, Batch 25, Loss: 0.8052375316619873\n",
            "Epoch 3, Batch 26, Loss: 0.674289345741272\n",
            "Epoch 3, Batch 27, Loss: 0.7967173457145691\n",
            "Epoch 3, Batch 28, Loss: 0.6249028444290161\n",
            "Epoch 3, Batch 29, Loss: 0.6695500612258911\n",
            "Epoch 3, Batch 30, Loss: 0.7577965259552002\n",
            "Epoch 3, Batch 31, Loss: 0.6890006065368652\n",
            "Epoch 3, Batch 32, Loss: 0.5124557018280029\n",
            "Epoch 3, Batch 33, Loss: 0.5826460719108582\n",
            "Epoch 4, Batch 1, Loss: 0.6194143891334534\n",
            "Epoch 4, Batch 2, Loss: 0.7400282621383667\n",
            "Epoch 4, Batch 3, Loss: 0.6797196865081787\n",
            "Epoch 4, Batch 4, Loss: 0.8881704211235046\n",
            "Epoch 4, Batch 5, Loss: 0.667076051235199\n",
            "Epoch 4, Batch 6, Loss: 0.7750308513641357\n",
            "Epoch 4, Batch 7, Loss: 0.3452230393886566\n",
            "Epoch 4, Batch 8, Loss: 1.0720832347869873\n",
            "Epoch 4, Batch 9, Loss: 0.4804310202598572\n",
            "Epoch 4, Batch 10, Loss: 0.6769351959228516\n",
            "Epoch 4, Batch 11, Loss: 0.6288511157035828\n",
            "Epoch 4, Batch 12, Loss: 0.633037805557251\n",
            "Epoch 4, Batch 13, Loss: 0.8584330081939697\n",
            "Epoch 4, Batch 14, Loss: 0.6151906847953796\n",
            "Epoch 4, Batch 15, Loss: 0.6440618634223938\n",
            "Epoch 4, Batch 16, Loss: 0.554092526435852\n",
            "Epoch 4, Batch 17, Loss: 0.441220760345459\n",
            "Epoch 4, Batch 18, Loss: 0.7814595699310303\n",
            "Epoch 4, Batch 19, Loss: 0.6804558038711548\n",
            "Epoch 4, Batch 20, Loss: 0.8274043202400208\n",
            "Epoch 4, Batch 21, Loss: 0.6778181791305542\n",
            "Epoch 4, Batch 22, Loss: 0.7452428340911865\n",
            "Epoch 4, Batch 23, Loss: 0.4447591006755829\n",
            "Epoch 4, Batch 24, Loss: 0.8853806257247925\n",
            "Epoch 4, Batch 25, Loss: 0.6321592926979065\n",
            "Epoch 4, Batch 26, Loss: 0.684516191482544\n",
            "Epoch 4, Batch 27, Loss: 0.9190670847892761\n",
            "Epoch 4, Batch 28, Loss: 0.7119711637496948\n",
            "Epoch 4, Batch 29, Loss: 0.7972303032875061\n",
            "Epoch 4, Batch 30, Loss: 0.9366225004196167\n",
            "Epoch 4, Batch 31, Loss: 0.4339449107646942\n",
            "Epoch 4, Batch 32, Loss: 0.6549040079116821\n",
            "Epoch 4, Batch 33, Loss: 0.5495129823684692\n",
            "Epoch 5, Batch 1, Loss: 0.5995067358016968\n",
            "Epoch 5, Batch 2, Loss: 0.5181475877761841\n",
            "Epoch 5, Batch 3, Loss: 0.7294872403144836\n",
            "Epoch 5, Batch 4, Loss: 0.6477770805358887\n",
            "Epoch 5, Batch 5, Loss: 0.6178265810012817\n",
            "Epoch 5, Batch 6, Loss: 0.6077911853790283\n",
            "Epoch 5, Batch 7, Loss: 0.7419370412826538\n",
            "Epoch 5, Batch 8, Loss: 0.7474453449249268\n",
            "Epoch 5, Batch 9, Loss: 0.49687421321868896\n",
            "Epoch 5, Batch 10, Loss: 0.7864373922348022\n",
            "Epoch 5, Batch 11, Loss: 0.9003602266311646\n",
            "Epoch 5, Batch 12, Loss: 0.24170050024986267\n",
            "Epoch 5, Batch 13, Loss: 0.7110499143600464\n",
            "Epoch 5, Batch 14, Loss: 0.8091796636581421\n",
            "Epoch 5, Batch 15, Loss: 0.8904367685317993\n",
            "Epoch 5, Batch 16, Loss: 0.6833611726760864\n",
            "Epoch 5, Batch 17, Loss: 0.6045849919319153\n",
            "Epoch 5, Batch 18, Loss: 0.6870384812355042\n",
            "Epoch 5, Batch 19, Loss: 0.7475074529647827\n",
            "Epoch 5, Batch 20, Loss: 0.6625441908836365\n",
            "Epoch 5, Batch 21, Loss: 0.8912978172302246\n",
            "Epoch 5, Batch 22, Loss: 0.8805972337722778\n",
            "Epoch 5, Batch 23, Loss: 0.7387576103210449\n",
            "Epoch 5, Batch 24, Loss: 0.7118912935256958\n",
            "Epoch 5, Batch 25, Loss: 0.680341362953186\n",
            "Epoch 5, Batch 26, Loss: 0.7381733655929565\n",
            "Epoch 5, Batch 27, Loss: 0.5904641151428223\n",
            "Epoch 5, Batch 28, Loss: 0.7852368950843811\n",
            "Epoch 5, Batch 29, Loss: 0.7022210359573364\n",
            "Epoch 5, Batch 30, Loss: 0.9271493554115295\n",
            "Epoch 5, Batch 31, Loss: 0.5637894868850708\n",
            "Epoch 5, Batch 32, Loss: 0.7907379865646362\n",
            "Epoch 5, Batch 33, Loss: 0.8180066347122192\n",
            "Epoch 6, Batch 1, Loss: 0.9078344106674194\n",
            "Epoch 6, Batch 2, Loss: 0.6667981147766113\n",
            "Epoch 6, Batch 3, Loss: 0.5570001602172852\n",
            "Epoch 6, Batch 4, Loss: 0.6947028636932373\n",
            "Epoch 6, Batch 5, Loss: 0.7041012048721313\n",
            "Epoch 6, Batch 6, Loss: 0.6668667793273926\n",
            "Epoch 6, Batch 7, Loss: 1.0254900455474854\n",
            "Epoch 6, Batch 8, Loss: 0.7579290866851807\n",
            "Epoch 6, Batch 9, Loss: 0.7937635183334351\n",
            "Epoch 6, Batch 10, Loss: 0.6445560455322266\n",
            "Epoch 6, Batch 11, Loss: 0.8080897331237793\n",
            "Epoch 6, Batch 12, Loss: 0.7439898252487183\n",
            "Epoch 6, Batch 13, Loss: 0.5351786613464355\n",
            "Epoch 6, Batch 14, Loss: 0.8694305419921875\n",
            "Epoch 6, Batch 15, Loss: 0.8391815423965454\n",
            "Epoch 6, Batch 16, Loss: 0.27365800738334656\n",
            "Epoch 6, Batch 17, Loss: 1.2413346767425537\n",
            "Epoch 6, Batch 18, Loss: 1.0399820804595947\n",
            "Epoch 6, Batch 19, Loss: 0.6645008325576782\n",
            "Epoch 6, Batch 20, Loss: 0.5687494874000549\n",
            "Epoch 6, Batch 21, Loss: 0.8895973563194275\n",
            "Epoch 6, Batch 22, Loss: 0.8253934383392334\n",
            "Epoch 6, Batch 23, Loss: 0.590060830116272\n",
            "Epoch 6, Batch 24, Loss: 0.8027587532997131\n",
            "Epoch 6, Batch 25, Loss: 0.5567765831947327\n",
            "Epoch 6, Batch 26, Loss: 0.6562851667404175\n",
            "Epoch 6, Batch 27, Loss: 0.6127368211746216\n",
            "Epoch 6, Batch 28, Loss: 0.6692236661911011\n",
            "Epoch 6, Batch 29, Loss: 0.5841666460037231\n",
            "Epoch 6, Batch 30, Loss: 0.6810370683670044\n",
            "Epoch 6, Batch 31, Loss: 0.7172998189926147\n",
            "Epoch 6, Batch 32, Loss: 0.6880839467048645\n",
            "Epoch 6, Batch 33, Loss: 0.7001665830612183\n",
            "Epoch 7, Batch 1, Loss: 0.6413372755050659\n",
            "Epoch 7, Batch 2, Loss: 0.8271462917327881\n",
            "Epoch 7, Batch 3, Loss: 0.7756142616271973\n",
            "Epoch 7, Batch 4, Loss: 0.6131731271743774\n",
            "Epoch 7, Batch 5, Loss: 0.7601315379142761\n",
            "Epoch 7, Batch 6, Loss: 1.028023362159729\n",
            "Epoch 7, Batch 7, Loss: 0.8021154403686523\n",
            "Epoch 7, Batch 8, Loss: 0.8759537935256958\n",
            "Epoch 7, Batch 9, Loss: 1.0303336381912231\n",
            "Epoch 7, Batch 10, Loss: 0.7648488879203796\n",
            "Epoch 7, Batch 11, Loss: 0.6554900407791138\n",
            "Epoch 7, Batch 12, Loss: 0.8260658383369446\n",
            "Epoch 7, Batch 13, Loss: 0.9319576621055603\n",
            "Epoch 7, Batch 14, Loss: 0.7760393023490906\n",
            "Epoch 7, Batch 15, Loss: 0.6518582105636597\n",
            "Epoch 7, Batch 16, Loss: 0.8709543943405151\n",
            "Epoch 7, Batch 17, Loss: 0.7282378077507019\n",
            "Epoch 7, Batch 18, Loss: 0.6058714389801025\n",
            "Epoch 7, Batch 19, Loss: 0.4931873679161072\n",
            "Epoch 7, Batch 20, Loss: 0.4994998872280121\n",
            "Epoch 7, Batch 21, Loss: 0.6684256792068481\n",
            "Epoch 7, Batch 22, Loss: 0.8384702205657959\n",
            "Epoch 7, Batch 23, Loss: 0.4835943281650543\n",
            "Epoch 7, Batch 24, Loss: 0.6042720079421997\n",
            "Epoch 7, Batch 25, Loss: 0.8326103091239929\n",
            "Epoch 7, Batch 26, Loss: 0.841736912727356\n",
            "Epoch 7, Batch 27, Loss: 0.8249440789222717\n",
            "Epoch 7, Batch 28, Loss: 0.5339383482933044\n",
            "Epoch 7, Batch 29, Loss: 0.7019680738449097\n",
            "Epoch 7, Batch 30, Loss: 0.5258660316467285\n",
            "Epoch 7, Batch 31, Loss: 0.6615358591079712\n",
            "Epoch 7, Batch 32, Loss: 0.8916729688644409\n",
            "Epoch 7, Batch 33, Loss: 0.5055219531059265\n",
            "Epoch 8, Batch 1, Loss: 0.5599689483642578\n",
            "Epoch 8, Batch 2, Loss: 0.7029834985733032\n",
            "Epoch 8, Batch 3, Loss: 0.9293091893196106\n",
            "Epoch 8, Batch 4, Loss: 0.2891320586204529\n",
            "Epoch 8, Batch 5, Loss: 0.662708044052124\n",
            "Epoch 8, Batch 6, Loss: 0.5523899793624878\n",
            "Epoch 8, Batch 7, Loss: 0.5852881669998169\n",
            "Epoch 8, Batch 8, Loss: 0.6831890344619751\n",
            "Epoch 8, Batch 9, Loss: 0.7552070617675781\n",
            "Epoch 8, Batch 10, Loss: 0.5507234334945679\n",
            "Epoch 8, Batch 11, Loss: 0.5265259742736816\n",
            "Epoch 8, Batch 12, Loss: 0.8081737160682678\n",
            "Epoch 8, Batch 13, Loss: 0.7928662300109863\n",
            "Epoch 8, Batch 14, Loss: 0.9084530472755432\n",
            "Epoch 8, Batch 15, Loss: 0.6415442228317261\n",
            "Epoch 8, Batch 16, Loss: 0.7364102602005005\n",
            "Epoch 8, Batch 17, Loss: 0.654881477355957\n",
            "Epoch 8, Batch 18, Loss: 0.6574606895446777\n",
            "Epoch 8, Batch 19, Loss: 0.6387935876846313\n",
            "Epoch 8, Batch 20, Loss: 1.043881893157959\n",
            "Epoch 8, Batch 21, Loss: 0.5870713591575623\n",
            "Epoch 8, Batch 22, Loss: 0.7007452249526978\n",
            "Epoch 8, Batch 23, Loss: 0.6646647453308105\n",
            "Epoch 8, Batch 24, Loss: 0.8232257962226868\n",
            "Epoch 8, Batch 25, Loss: 0.5508196353912354\n",
            "Epoch 8, Batch 26, Loss: 1.0006060600280762\n",
            "Epoch 8, Batch 27, Loss: 0.6241761445999146\n",
            "Epoch 8, Batch 28, Loss: 0.9485371112823486\n",
            "Epoch 8, Batch 29, Loss: 0.6671093702316284\n",
            "Epoch 8, Batch 30, Loss: 0.7447187900543213\n",
            "Epoch 8, Batch 31, Loss: 0.6561838984489441\n",
            "Epoch 8, Batch 32, Loss: 0.5756739974021912\n",
            "Epoch 8, Batch 33, Loss: 0.9956263303756714\n",
            "Epoch 9, Batch 1, Loss: 0.8535200357437134\n",
            "Epoch 9, Batch 2, Loss: 1.1043667793273926\n",
            "Epoch 9, Batch 3, Loss: 0.9787886738777161\n",
            "Epoch 9, Batch 4, Loss: 0.7822922468185425\n",
            "Epoch 9, Batch 5, Loss: 0.7637792825698853\n",
            "Epoch 9, Batch 6, Loss: 0.43677085638046265\n",
            "Epoch 9, Batch 7, Loss: 0.5878012180328369\n",
            "Epoch 9, Batch 8, Loss: 0.6258470416069031\n",
            "Epoch 9, Batch 9, Loss: 0.6439619064331055\n",
            "Epoch 9, Batch 10, Loss: 0.6461049318313599\n",
            "Epoch 9, Batch 11, Loss: 0.44781294465065\n",
            "Epoch 9, Batch 12, Loss: 0.6832952499389648\n",
            "Epoch 9, Batch 13, Loss: 0.7720949649810791\n",
            "Epoch 9, Batch 14, Loss: 0.5505574345588684\n",
            "Epoch 9, Batch 15, Loss: 0.7278667688369751\n",
            "Epoch 9, Batch 16, Loss: 0.4704393744468689\n",
            "Epoch 9, Batch 17, Loss: 0.7176706790924072\n",
            "Epoch 9, Batch 18, Loss: 0.5278798341751099\n",
            "Epoch 9, Batch 19, Loss: 1.0534074306488037\n",
            "Epoch 9, Batch 20, Loss: 0.4281039834022522\n",
            "Epoch 9, Batch 21, Loss: 0.7759596109390259\n",
            "Epoch 9, Batch 22, Loss: 1.0433166027069092\n",
            "Epoch 9, Batch 23, Loss: 0.7175989151000977\n",
            "Epoch 9, Batch 24, Loss: 0.5306421518325806\n",
            "Epoch 9, Batch 25, Loss: 0.6128219366073608\n",
            "Epoch 9, Batch 26, Loss: 0.5782350301742554\n",
            "Epoch 9, Batch 27, Loss: 0.7463996410369873\n",
            "Epoch 9, Batch 28, Loss: 0.8706839680671692\n",
            "Epoch 9, Batch 29, Loss: 0.7115944623947144\n",
            "Epoch 9, Batch 30, Loss: 0.943871796131134\n",
            "Epoch 9, Batch 31, Loss: 0.6808592081069946\n",
            "Epoch 9, Batch 32, Loss: 0.9238238334655762\n",
            "Epoch 9, Batch 33, Loss: 0.7063549160957336\n",
            "Epoch 10, Batch 1, Loss: 1.0335772037506104\n",
            "Epoch 10, Batch 2, Loss: 0.6336489915847778\n",
            "Epoch 10, Batch 3, Loss: 0.5771830081939697\n",
            "Epoch 10, Batch 4, Loss: 0.8688825964927673\n",
            "Epoch 10, Batch 5, Loss: 0.5298726558685303\n",
            "Epoch 10, Batch 6, Loss: 0.9847501516342163\n",
            "Epoch 10, Batch 7, Loss: 0.7122229337692261\n",
            "Epoch 10, Batch 8, Loss: 0.7335306406021118\n",
            "Epoch 10, Batch 9, Loss: 1.0336625576019287\n",
            "Epoch 10, Batch 10, Loss: 0.6907656788825989\n",
            "Epoch 10, Batch 11, Loss: 0.7272216081619263\n",
            "Epoch 10, Batch 12, Loss: 0.6490408182144165\n",
            "Epoch 10, Batch 13, Loss: 0.668512225151062\n",
            "Epoch 10, Batch 14, Loss: 0.8305298089981079\n",
            "Epoch 10, Batch 15, Loss: 0.5825080275535583\n",
            "Epoch 10, Batch 16, Loss: 0.5260121822357178\n",
            "Epoch 10, Batch 17, Loss: 0.6227805614471436\n",
            "Epoch 10, Batch 18, Loss: 0.5057539939880371\n",
            "Epoch 10, Batch 19, Loss: 0.744744062423706\n",
            "Epoch 10, Batch 20, Loss: 1.0101516246795654\n",
            "Epoch 10, Batch 21, Loss: 0.6961886882781982\n",
            "Epoch 10, Batch 22, Loss: 0.5723038911819458\n",
            "Epoch 10, Batch 23, Loss: 0.7553747892379761\n",
            "Epoch 10, Batch 24, Loss: 0.5692800283432007\n",
            "Epoch 10, Batch 25, Loss: 0.6684688329696655\n",
            "Epoch 10, Batch 26, Loss: 0.6787142157554626\n",
            "Epoch 10, Batch 27, Loss: 0.4863758683204651\n",
            "Epoch 10, Batch 28, Loss: 1.0025177001953125\n",
            "Epoch 10, Batch 29, Loss: 0.46592700481414795\n",
            "Epoch 10, Batch 30, Loss: 0.6991937160491943\n",
            "Epoch 10, Batch 31, Loss: 0.6207035779953003\n",
            "Epoch 10, Batch 32, Loss: 0.9747310876846313\n",
            "Epoch 10, Batch 33, Loss: 0.429345041513443\n",
            "Epoch 11, Batch 1, Loss: 0.40384602546691895\n",
            "Epoch 11, Batch 2, Loss: 1.0083917379379272\n",
            "Epoch 11, Batch 3, Loss: 0.8246934413909912\n",
            "Epoch 11, Batch 4, Loss: 0.6927978992462158\n",
            "Epoch 11, Batch 5, Loss: 0.6678645610809326\n",
            "Epoch 11, Batch 6, Loss: 0.5536187887191772\n",
            "Epoch 11, Batch 7, Loss: 0.4618603587150574\n",
            "Epoch 11, Batch 8, Loss: 0.6545790433883667\n",
            "Epoch 11, Batch 9, Loss: 0.7190266847610474\n",
            "Epoch 11, Batch 10, Loss: 1.0961809158325195\n",
            "Epoch 11, Batch 11, Loss: 0.6397062540054321\n",
            "Epoch 11, Batch 12, Loss: 0.839146077632904\n",
            "Epoch 11, Batch 13, Loss: 0.6034913063049316\n",
            "Epoch 11, Batch 14, Loss: 0.6172351837158203\n",
            "Epoch 11, Batch 15, Loss: 0.6074810028076172\n",
            "Epoch 11, Batch 16, Loss: 0.8413282036781311\n",
            "Epoch 11, Batch 17, Loss: 0.7464203834533691\n",
            "Epoch 11, Batch 18, Loss: 0.6510558724403381\n",
            "Epoch 11, Batch 19, Loss: 0.8430546522140503\n",
            "Epoch 11, Batch 20, Loss: 0.8933925628662109\n",
            "Epoch 11, Batch 21, Loss: 0.829826295375824\n",
            "Epoch 11, Batch 22, Loss: 0.6986368894577026\n",
            "Epoch 11, Batch 23, Loss: 0.648662269115448\n",
            "Epoch 11, Batch 24, Loss: 0.8382308483123779\n",
            "Epoch 11, Batch 25, Loss: 0.6854287385940552\n",
            "Epoch 11, Batch 26, Loss: 0.8513002395629883\n",
            "Epoch 11, Batch 27, Loss: 0.5727752447128296\n",
            "Epoch 11, Batch 28, Loss: 0.44154322147369385\n",
            "Epoch 11, Batch 29, Loss: 0.9428167343139648\n",
            "Epoch 11, Batch 30, Loss: 0.3043747842311859\n",
            "Epoch 11, Batch 31, Loss: 0.8203498125076294\n",
            "Epoch 11, Batch 32, Loss: 0.4955465793609619\n",
            "Epoch 11, Batch 33, Loss: 0.893545925617218\n",
            "Epoch 12, Batch 1, Loss: 0.5612226724624634\n",
            "Epoch 12, Batch 2, Loss: 0.5329630970954895\n",
            "Epoch 12, Batch 3, Loss: 0.7299817800521851\n",
            "Epoch 12, Batch 4, Loss: 0.8944328427314758\n",
            "Epoch 12, Batch 5, Loss: 0.7426941990852356\n",
            "Epoch 12, Batch 6, Loss: 0.6953284740447998\n",
            "Epoch 12, Batch 7, Loss: 0.4669823944568634\n",
            "Epoch 12, Batch 8, Loss: 0.6563886404037476\n",
            "Epoch 12, Batch 9, Loss: 0.6040195226669312\n",
            "Epoch 12, Batch 10, Loss: 0.8724160194396973\n",
            "Epoch 12, Batch 11, Loss: 0.5399743318557739\n",
            "Epoch 12, Batch 12, Loss: 0.7423595786094666\n",
            "Epoch 12, Batch 13, Loss: 0.7292883992195129\n",
            "Epoch 12, Batch 14, Loss: 0.9342711567878723\n",
            "Epoch 12, Batch 15, Loss: 0.6438401937484741\n",
            "Epoch 12, Batch 16, Loss: 0.6529914140701294\n",
            "Epoch 12, Batch 17, Loss: 0.7084126472473145\n",
            "Epoch 12, Batch 18, Loss: 0.8271714448928833\n",
            "Epoch 12, Batch 19, Loss: 1.1177408695220947\n",
            "Epoch 12, Batch 20, Loss: 0.7742898464202881\n",
            "Epoch 12, Batch 21, Loss: 0.4792513847351074\n",
            "Epoch 12, Batch 22, Loss: 0.6611616611480713\n",
            "Epoch 12, Batch 23, Loss: 0.8256199955940247\n",
            "Epoch 12, Batch 24, Loss: 0.77030348777771\n",
            "Epoch 12, Batch 25, Loss: 0.5378254652023315\n",
            "Epoch 12, Batch 26, Loss: 0.6154042482376099\n",
            "Epoch 12, Batch 27, Loss: 0.7621578574180603\n",
            "Epoch 12, Batch 28, Loss: 0.5923073291778564\n",
            "Epoch 12, Batch 29, Loss: 0.4081599712371826\n",
            "Epoch 12, Batch 30, Loss: 0.9111028909683228\n",
            "Epoch 12, Batch 31, Loss: 0.620475172996521\n",
            "Epoch 12, Batch 32, Loss: 0.5807932615280151\n",
            "Epoch 12, Batch 33, Loss: 0.727375864982605\n",
            "Epoch 13, Batch 1, Loss: 0.8131364583969116\n",
            "Epoch 13, Batch 2, Loss: 0.8308693170547485\n",
            "Epoch 13, Batch 3, Loss: 0.7324906587600708\n",
            "Epoch 13, Batch 4, Loss: 0.5968957543373108\n",
            "Epoch 13, Batch 5, Loss: 0.7688993215560913\n",
            "Epoch 13, Batch 6, Loss: 0.7402948141098022\n",
            "Epoch 13, Batch 7, Loss: 0.7765180468559265\n",
            "Epoch 13, Batch 8, Loss: 0.9800978899002075\n",
            "Epoch 13, Batch 9, Loss: 0.786167323589325\n",
            "Epoch 13, Batch 10, Loss: 0.712338924407959\n",
            "Epoch 13, Batch 11, Loss: 0.8249529004096985\n",
            "Epoch 13, Batch 12, Loss: 0.703343391418457\n",
            "Epoch 13, Batch 13, Loss: 0.94623863697052\n",
            "Epoch 13, Batch 14, Loss: 0.6140152215957642\n",
            "Epoch 13, Batch 15, Loss: 0.5635249614715576\n",
            "Epoch 13, Batch 16, Loss: 0.6272811889648438\n",
            "Epoch 13, Batch 17, Loss: 0.5179919600486755\n",
            "Epoch 13, Batch 18, Loss: 0.7945507764816284\n",
            "Epoch 13, Batch 19, Loss: 0.7144105434417725\n",
            "Epoch 13, Batch 20, Loss: 0.7599501609802246\n",
            "Epoch 13, Batch 21, Loss: 0.8106266260147095\n",
            "Epoch 13, Batch 22, Loss: 0.6236306428909302\n",
            "Epoch 13, Batch 23, Loss: 0.5831589102745056\n",
            "Epoch 13, Batch 24, Loss: 0.7810795903205872\n",
            "Epoch 13, Batch 25, Loss: 0.8083885908126831\n",
            "Epoch 13, Batch 26, Loss: 0.9056786298751831\n",
            "Epoch 13, Batch 27, Loss: 0.5244765877723694\n",
            "Epoch 13, Batch 28, Loss: 0.7254679203033447\n",
            "Epoch 13, Batch 29, Loss: 0.4211617112159729\n",
            "Epoch 13, Batch 30, Loss: 0.6761559247970581\n",
            "Epoch 13, Batch 31, Loss: 0.8292165398597717\n",
            "Epoch 13, Batch 32, Loss: 0.5735422372817993\n",
            "Epoch 13, Batch 33, Loss: 0.7909135818481445\n",
            "Epoch 14, Batch 1, Loss: 0.9255444407463074\n",
            "Epoch 14, Batch 2, Loss: 0.6857854127883911\n",
            "Epoch 14, Batch 3, Loss: 0.599993884563446\n",
            "Epoch 14, Batch 4, Loss: 0.4468517303466797\n",
            "Epoch 14, Batch 5, Loss: 0.802414059638977\n",
            "Epoch 14, Batch 6, Loss: 0.8404373526573181\n",
            "Epoch 14, Batch 7, Loss: 0.6128392815589905\n",
            "Epoch 14, Batch 8, Loss: 0.668809175491333\n",
            "Epoch 14, Batch 9, Loss: 0.7494242191314697\n",
            "Epoch 14, Batch 10, Loss: 0.7223871350288391\n",
            "Epoch 14, Batch 11, Loss: 0.6834113597869873\n",
            "Epoch 14, Batch 12, Loss: 0.6690486669540405\n",
            "Epoch 14, Batch 13, Loss: 0.549025297164917\n",
            "Epoch 14, Batch 14, Loss: 0.8633291125297546\n",
            "Epoch 14, Batch 15, Loss: 0.9191809296607971\n",
            "Epoch 14, Batch 16, Loss: 0.695260763168335\n",
            "Epoch 14, Batch 17, Loss: 0.9302359223365784\n",
            "Epoch 14, Batch 18, Loss: 0.566463828086853\n",
            "Epoch 14, Batch 19, Loss: 0.8031569719314575\n",
            "Epoch 14, Batch 20, Loss: 0.3367307782173157\n",
            "Epoch 14, Batch 21, Loss: 0.7282707691192627\n",
            "Epoch 14, Batch 22, Loss: 0.5384408831596375\n",
            "Epoch 14, Batch 23, Loss: 0.7844685316085815\n",
            "Epoch 14, Batch 24, Loss: 0.48882317543029785\n",
            "Epoch 14, Batch 25, Loss: 0.8519632816314697\n",
            "Epoch 14, Batch 26, Loss: 0.40630167722702026\n",
            "Epoch 14, Batch 27, Loss: 0.5442113876342773\n",
            "Epoch 14, Batch 28, Loss: 0.7454537153244019\n",
            "Epoch 14, Batch 29, Loss: 0.7927567958831787\n",
            "Epoch 14, Batch 30, Loss: 0.611703634262085\n",
            "Epoch 14, Batch 31, Loss: 0.865323543548584\n",
            "Epoch 14, Batch 32, Loss: 0.7235156297683716\n",
            "Epoch 14, Batch 33, Loss: 0.618959903717041\n",
            "Epoch 15, Batch 1, Loss: 0.8848587274551392\n",
            "Epoch 15, Batch 2, Loss: 0.978339433670044\n",
            "Epoch 15, Batch 3, Loss: 0.4806104898452759\n",
            "Epoch 15, Batch 4, Loss: 0.6210571527481079\n",
            "Epoch 15, Batch 5, Loss: 0.6712414026260376\n",
            "Epoch 15, Batch 6, Loss: 0.6857838034629822\n",
            "Epoch 15, Batch 7, Loss: 0.8346234560012817\n",
            "Epoch 15, Batch 8, Loss: 0.6521875858306885\n",
            "Epoch 15, Batch 9, Loss: 0.49459123611450195\n",
            "Epoch 15, Batch 10, Loss: 0.5644814372062683\n",
            "Epoch 15, Batch 11, Loss: 0.5333045721054077\n",
            "Epoch 15, Batch 12, Loss: 0.808568000793457\n",
            "Epoch 15, Batch 13, Loss: 0.7198799848556519\n",
            "Epoch 15, Batch 14, Loss: 0.7093467712402344\n",
            "Epoch 15, Batch 15, Loss: 0.6554234027862549\n",
            "Epoch 15, Batch 16, Loss: 0.9100643396377563\n",
            "Epoch 15, Batch 17, Loss: 0.6395562291145325\n",
            "Epoch 15, Batch 18, Loss: 0.855466365814209\n",
            "Epoch 15, Batch 19, Loss: 0.6275923848152161\n",
            "Epoch 15, Batch 20, Loss: 0.9334995746612549\n",
            "Epoch 15, Batch 21, Loss: 0.6219208240509033\n",
            "Epoch 15, Batch 22, Loss: 0.8444051742553711\n",
            "Epoch 15, Batch 23, Loss: 0.9380322098731995\n",
            "Epoch 15, Batch 24, Loss: 0.6459565758705139\n",
            "Epoch 15, Batch 25, Loss: 0.9109061360359192\n",
            "Epoch 15, Batch 26, Loss: 0.6604834794998169\n",
            "Epoch 15, Batch 27, Loss: 0.6420826315879822\n",
            "Epoch 15, Batch 28, Loss: 0.6032358407974243\n",
            "Epoch 15, Batch 29, Loss: 0.84514319896698\n",
            "Epoch 15, Batch 30, Loss: 0.5949663519859314\n",
            "Epoch 15, Batch 31, Loss: 0.6617677807807922\n",
            "Epoch 15, Batch 32, Loss: 0.5458030700683594\n",
            "Epoch 15, Batch 33, Loss: 0.6378473043441772\n"
          ]
        }
      ],
      "source": [
        "config = LSTM_Config(lstm_H=200, drop_out=0.75, lr=0.01)\n",
        "model = LSTM_NeuralModel(config)\n",
        "pretrained_model,_,_ = train_model(model, dataset, config, num_epochs=15)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "grTRrgPuU8CW"
      },
      "outputs": [],
      "source": [
        "pretrained_model.save_weights('pretrained_model.weights.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fsuzU7MWVt3",
        "outputId": "33479ada-b1e9-4768-c7d3-3e9b8e23c2a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading dataset to data/crop_yield...\n",
            "Downloading from Google Drive...\n"
          ]
        }
      ],
      "source": [
        "# change the dataset from Argentina to USA\n",
        "\n",
        "dataset_usa = get_dataset(dataset='crop_yield', download=True, split_scheme='usa')  # change the code in get_dataset to USA data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BJpZsWjWTD6V",
        "outputId": "0ed09e2d-df5a-4a02-9362-0ac59954f086"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Batch 1, Loss: 10.255167007446289\n",
            "Epoch 1, Batch 2, Loss: 0.8609815835952759\n",
            "Epoch 1, Batch 3, Loss: 355.6170654296875\n",
            "Epoch 1, Batch 4, Loss: 24.399566650390625\n",
            "Epoch 1, Batch 5, Loss: 3.685458183288574\n",
            "Epoch 1, Batch 6, Loss: 7.410475254058838\n",
            "Epoch 1, Batch 7, Loss: 10.002830505371094\n",
            "Epoch 1, Batch 8, Loss: 6.939477443695068\n",
            "Epoch 1, Batch 9, Loss: 5.5649213790893555\n",
            "Epoch 1, Batch 10, Loss: 3.798226833343506\n",
            "Epoch 1, Batch 11, Loss: 1.5659236907958984\n",
            "Epoch 1, Batch 12, Loss: 0.37722766399383545\n",
            "Epoch 1, Batch 13, Loss: 1.9394237995147705\n",
            "Epoch 1, Batch 14, Loss: 0.7858812212944031\n",
            "Epoch 1, Batch 15, Loss: 0.5056763887405396\n",
            "Epoch 1, Batch 16, Loss: 0.7146655321121216\n",
            "Epoch 1, Batch 17, Loss: 1.0617069005966187\n",
            "Epoch 1, Batch 18, Loss: 0.5120425224304199\n",
            "Epoch 1, Batch 19, Loss: 0.4185057282447815\n",
            "Epoch 1, Batch 20, Loss: 0.5436594486236572\n",
            "Epoch 1, Batch 21, Loss: 0.5753266215324402\n",
            "Epoch 1, Batch 22, Loss: 0.7148011326789856\n",
            "Epoch 1, Batch 23, Loss: 0.45950809121131897\n",
            "Epoch 1, Batch 24, Loss: 0.48567795753479004\n",
            "Epoch 1, Batch 25, Loss: 0.4269910454750061\n",
            "Epoch 1, Batch 26, Loss: 0.5094621777534485\n",
            "Epoch 1, Batch 27, Loss: 0.6615321040153503\n",
            "Epoch 1, Batch 28, Loss: 0.48909398913383484\n",
            "Epoch 1, Batch 29, Loss: 0.4985896050930023\n",
            "Epoch 1, Batch 30, Loss: 0.6154458522796631\n",
            "Epoch 1, Batch 31, Loss: 0.5172231793403625\n",
            "Epoch 1, Batch 32, Loss: 0.6576809883117676\n",
            "Epoch 1, Batch 33, Loss: 0.3487517237663269\n",
            "Epoch 1, Batch 34, Loss: 0.6303238868713379\n",
            "Epoch 1, Batch 35, Loss: 0.45318683981895447\n",
            "Epoch 1, Batch 36, Loss: 0.6222715377807617\n",
            "Epoch 1, Batch 37, Loss: 0.44720932841300964\n",
            "Epoch 1, Batch 38, Loss: 0.39365023374557495\n",
            "Epoch 1, Batch 39, Loss: 0.352944940328598\n",
            "Epoch 1, Batch 40, Loss: 0.5847522616386414\n",
            "Epoch 1, Batch 41, Loss: 0.4884505569934845\n",
            "Epoch 1, Batch 42, Loss: 0.42636507749557495\n",
            "Epoch 1, Batch 43, Loss: 0.5161638259887695\n",
            "Epoch 1, Batch 44, Loss: 0.4955439269542694\n",
            "Epoch 1, Batch 45, Loss: 0.4653918147087097\n",
            "Epoch 1, Batch 46, Loss: 0.7788166999816895\n",
            "Epoch 1, Batch 47, Loss: 0.4918084740638733\n",
            "Epoch 1, Batch 48, Loss: 0.5427753329277039\n",
            "Epoch 1, Batch 49, Loss: 0.7677097320556641\n",
            "Epoch 1, Batch 50, Loss: 0.33032354712486267\n",
            "Epoch 1, Batch 51, Loss: 0.5054118633270264\n",
            "Epoch 1, Batch 52, Loss: 0.2736111283302307\n",
            "Epoch 1, Batch 53, Loss: 0.5379229784011841\n",
            "Epoch 1, Batch 54, Loss: 0.5123597383499146\n",
            "Epoch 1, Batch 55, Loss: 0.5131366848945618\n",
            "Epoch 1, Batch 56, Loss: 0.6905521154403687\n",
            "Epoch 1, Batch 57, Loss: 0.5439539551734924\n",
            "Epoch 1, Batch 58, Loss: 0.5068175792694092\n",
            "Epoch 1, Batch 59, Loss: 0.34366703033447266\n",
            "Epoch 1, Batch 60, Loss: 0.38722550868988037\n",
            "Epoch 1, Batch 61, Loss: 0.7859964966773987\n",
            "Epoch 1, Batch 62, Loss: 0.3639759421348572\n",
            "Epoch 1, Batch 63, Loss: 0.6255108118057251\n",
            "Epoch 1, Batch 64, Loss: 0.3591373562812805\n",
            "Epoch 1, Batch 65, Loss: 0.37548330426216125\n",
            "Epoch 1, Batch 66, Loss: 0.5021050572395325\n",
            "Epoch 1, Batch 67, Loss: 0.4165053963661194\n",
            "Epoch 1, Batch 68, Loss: 0.569674015045166\n",
            "Epoch 1, Batch 69, Loss: 0.3994920253753662\n",
            "Epoch 1, Batch 70, Loss: 0.2734622359275818\n",
            "Epoch 1, Batch 71, Loss: 0.6699706315994263\n",
            "Epoch 1, Batch 72, Loss: 0.48398417234420776\n",
            "Epoch 1, Batch 73, Loss: 0.48267310857772827\n",
            "Epoch 1, Batch 74, Loss: 0.4764428734779358\n",
            "Epoch 1, Batch 75, Loss: 0.4333544969558716\n",
            "Epoch 1, Batch 76, Loss: 0.6516368389129639\n",
            "Epoch 1, Batch 77, Loss: 0.33178186416625977\n",
            "Epoch 1, Batch 78, Loss: 0.5473899245262146\n",
            "Epoch 1, Batch 79, Loss: 0.4922032952308655\n",
            "Epoch 1, Batch 80, Loss: 0.5259253978729248\n",
            "Epoch 1, Batch 81, Loss: 0.8355774283409119\n",
            "Epoch 1, Batch 82, Loss: 0.5231373310089111\n",
            "Epoch 1, Batch 83, Loss: 0.44076913595199585\n",
            "Epoch 1, Batch 84, Loss: 0.4512946605682373\n",
            "Epoch 1, Batch 85, Loss: 0.45494702458381653\n",
            "Epoch 1, Batch 86, Loss: 0.4739924669265747\n",
            "Epoch 1, Batch 87, Loss: 0.5451548099517822\n",
            "Epoch 1, Batch 88, Loss: 0.39833933115005493\n",
            "Epoch 1, Batch 89, Loss: 0.40731462836265564\n",
            "Epoch 1, Batch 90, Loss: 0.39531856775283813\n",
            "Epoch 1, Batch 91, Loss: 0.42321133613586426\n",
            "Epoch 1, Batch 92, Loss: 0.5852683186531067\n",
            "Epoch 1, Batch 93, Loss: 0.5248678922653198\n",
            "Epoch 1, Batch 94, Loss: 0.3868393003940582\n",
            "Epoch 1, Batch 95, Loss: 0.3134032189846039\n",
            "Epoch 1, Batch 96, Loss: 0.5378339886665344\n",
            "Epoch 1, Batch 97, Loss: 0.3671400845050812\n",
            "Epoch 1, Batch 98, Loss: 0.3773822784423828\n",
            "Epoch 1, Batch 99, Loss: 0.3038978576660156\n",
            "Epoch 1, Batch 100, Loss: 0.437712162733078\n",
            "Epoch 1, Batch 101, Loss: 0.49661439657211304\n",
            "Epoch 1, Batch 102, Loss: 0.4780729413032532\n",
            "Epoch 1, Batch 103, Loss: 0.3818269968032837\n",
            "Epoch 1, Batch 104, Loss: 0.48965734243392944\n",
            "Epoch 1, Batch 105, Loss: 0.6872736215591431\n",
            "Epoch 1, Batch 106, Loss: 0.5894691944122314\n",
            "Epoch 1, Batch 107, Loss: 0.5185421705245972\n",
            "Epoch 1, Batch 108, Loss: 0.5318184494972229\n",
            "Epoch 1, Batch 109, Loss: 0.7990902066230774\n",
            "Epoch 1, Batch 110, Loss: 0.6140569448471069\n",
            "Epoch 1, Batch 111, Loss: 0.5532125234603882\n",
            "Epoch 1, Batch 112, Loss: 0.7268248796463013\n",
            "Epoch 1, Batch 113, Loss: 0.47293445467948914\n",
            "Epoch 1, Batch 114, Loss: 0.4573359489440918\n",
            "Epoch 1, Batch 115, Loss: 0.40072447061538696\n",
            "Epoch 1, Batch 116, Loss: 0.7116245031356812\n",
            "Epoch 1, Batch 117, Loss: 0.6232609748840332\n",
            "Epoch 1, Batch 118, Loss: 0.5664068460464478\n",
            "Epoch 1, Batch 119, Loss: 0.5155417919158936\n",
            "Epoch 1, Batch 120, Loss: 0.6494725346565247\n",
            "Epoch 1, Batch 121, Loss: 0.6759946346282959\n",
            "Epoch 1, Batch 122, Loss: 0.4636285901069641\n",
            "Epoch 1, Batch 123, Loss: 0.6317343711853027\n",
            "Epoch 1, Batch 124, Loss: 0.43753716349601746\n",
            "Epoch 1, Batch 125, Loss: 0.5363959074020386\n",
            "Epoch 1, Batch 126, Loss: 0.4255768656730652\n",
            "Epoch 1, Batch 127, Loss: 0.2358735054731369\n",
            "Epoch 1, Batch 128, Loss: 0.3892611563205719\n",
            "Epoch 1, Batch 129, Loss: 0.36345547437667847\n",
            "Epoch 1, Batch 130, Loss: 0.43488284945487976\n",
            "Epoch 1, Batch 131, Loss: 0.45380252599716187\n",
            "Epoch 1, Batch 132, Loss: 0.4438074231147766\n",
            "Epoch 1, Batch 133, Loss: 0.41301509737968445\n",
            "Epoch 1, Batch 134, Loss: 0.5986292958259583\n",
            "Epoch 1, Batch 135, Loss: 0.5228838324546814\n",
            "Epoch 1, Batch 136, Loss: 0.5081407427787781\n",
            "Epoch 1, Batch 137, Loss: 0.5148834586143494\n",
            "Epoch 1, Batch 138, Loss: 0.3813384175300598\n",
            "Epoch 1, Batch 139, Loss: 0.8774763941764832\n",
            "Epoch 1, Batch 140, Loss: 0.46920621395111084\n",
            "Epoch 1, Batch 141, Loss: 0.4890928268432617\n",
            "Epoch 1, Batch 142, Loss: 0.7958827018737793\n",
            "Epoch 1, Batch 143, Loss: 0.5105248689651489\n",
            "Epoch 1, Batch 144, Loss: 0.6439078450202942\n",
            "Epoch 1, Batch 145, Loss: 0.44344228506088257\n",
            "Epoch 1, Batch 146, Loss: 0.5723296403884888\n",
            "Epoch 1, Batch 147, Loss: 0.48612284660339355\n",
            "Epoch 1, Batch 148, Loss: 0.6261357069015503\n",
            "Epoch 1, Batch 149, Loss: 0.46419987082481384\n",
            "Epoch 1, Batch 150, Loss: 0.3969040513038635\n",
            "Epoch 1, Batch 151, Loss: 0.3347328305244446\n",
            "Epoch 1, Batch 152, Loss: 0.787474513053894\n",
            "Epoch 1, Batch 153, Loss: 0.54244065284729\n",
            "Epoch 1, Batch 154, Loss: 0.5395686626434326\n",
            "Epoch 1, Batch 155, Loss: 0.35927921533584595\n",
            "Epoch 1, Batch 156, Loss: 0.505305290222168\n",
            "Epoch 1, Batch 157, Loss: 0.43124550580978394\n",
            "Epoch 1, Batch 158, Loss: 0.6391010880470276\n",
            "Epoch 1, Batch 159, Loss: 0.4265114665031433\n",
            "Epoch 1, Batch 160, Loss: 0.6018684506416321\n",
            "Epoch 1, Batch 161, Loss: 0.3324553966522217\n",
            "Epoch 1, Batch 162, Loss: 0.42242828011512756\n",
            "Epoch 1, Batch 163, Loss: 0.5111368894577026\n",
            "Epoch 1, Batch 164, Loss: 0.6035984754562378\n",
            "Epoch 1, Batch 165, Loss: 0.5757705569267273\n",
            "Epoch 1, Batch 166, Loss: 0.5711822509765625\n",
            "Epoch 1, Batch 167, Loss: 0.4421399235725403\n",
            "Epoch 1, Batch 168, Loss: 0.46590349078178406\n",
            "Epoch 1, Batch 169, Loss: 0.8374553918838501\n",
            "Epoch 1, Batch 170, Loss: 0.4684027433395386\n",
            "Epoch 1, Batch 171, Loss: 0.5668300986289978\n",
            "Epoch 1, Batch 172, Loss: 0.29222917556762695\n",
            "Epoch 1, Batch 173, Loss: 0.3713493347167969\n",
            "Epoch 1, Batch 174, Loss: 0.3788280785083771\n",
            "Epoch 1, Batch 175, Loss: 0.6152472496032715\n",
            "Epoch 1, Batch 176, Loss: 0.8632428050041199\n",
            "Epoch 1, Batch 177, Loss: 0.5174529552459717\n",
            "Epoch 1, Batch 178, Loss: 0.361078679561615\n",
            "Epoch 1, Batch 179, Loss: 0.4660804271697998\n",
            "Epoch 1, Batch 180, Loss: 0.6214533448219299\n",
            "Epoch 1, Batch 181, Loss: 0.5035383701324463\n",
            "Epoch 2, Batch 1, Loss: 0.5319291949272156\n",
            "Epoch 2, Batch 2, Loss: 0.8151194453239441\n",
            "Epoch 2, Batch 3, Loss: 0.6019410490989685\n",
            "Epoch 2, Batch 4, Loss: 0.5732523202896118\n",
            "Epoch 2, Batch 5, Loss: 0.5691547393798828\n",
            "Epoch 2, Batch 6, Loss: 0.3099520206451416\n",
            "Epoch 2, Batch 7, Loss: 0.46425703167915344\n",
            "Epoch 2, Batch 8, Loss: 0.6907278895378113\n",
            "Epoch 2, Batch 9, Loss: 0.42939096689224243\n",
            "Epoch 2, Batch 10, Loss: 0.6546695828437805\n",
            "Epoch 2, Batch 11, Loss: 0.6547555923461914\n",
            "Epoch 2, Batch 12, Loss: 0.6190750002861023\n",
            "Epoch 2, Batch 13, Loss: 0.5552043318748474\n",
            "Epoch 2, Batch 14, Loss: 0.6376839876174927\n",
            "Epoch 2, Batch 15, Loss: 0.506394624710083\n",
            "Epoch 2, Batch 16, Loss: 0.5330451726913452\n",
            "Epoch 2, Batch 17, Loss: 0.5771615505218506\n",
            "Epoch 2, Batch 18, Loss: 0.7173675894737244\n",
            "Epoch 2, Batch 19, Loss: 0.6389465928077698\n",
            "Epoch 2, Batch 20, Loss: 0.549701452255249\n",
            "Epoch 2, Batch 21, Loss: 0.43697595596313477\n",
            "Epoch 2, Batch 22, Loss: 0.5087213516235352\n",
            "Epoch 2, Batch 23, Loss: 0.711444616317749\n",
            "Epoch 2, Batch 24, Loss: 0.5647798180580139\n",
            "Epoch 2, Batch 25, Loss: 0.3659600615501404\n",
            "Epoch 2, Batch 26, Loss: 0.3477485179901123\n",
            "Epoch 2, Batch 27, Loss: 0.39766883850097656\n",
            "Epoch 2, Batch 28, Loss: 0.438895583152771\n",
            "Epoch 2, Batch 29, Loss: 0.6580225825309753\n",
            "Epoch 2, Batch 30, Loss: 0.44143420457839966\n",
            "Epoch 2, Batch 31, Loss: 0.6377419233322144\n",
            "Epoch 2, Batch 32, Loss: 0.4857798218727112\n",
            "Epoch 2, Batch 33, Loss: 0.5022597312927246\n",
            "Epoch 2, Batch 34, Loss: 0.4700752794742584\n",
            "Epoch 2, Batch 35, Loss: 0.36352598667144775\n",
            "Epoch 2, Batch 36, Loss: 0.3376674950122833\n",
            "Epoch 2, Batch 37, Loss: 0.4901697337627411\n",
            "Epoch 2, Batch 38, Loss: 0.3324868679046631\n",
            "Epoch 2, Batch 39, Loss: 0.5238020420074463\n",
            "Epoch 2, Batch 40, Loss: 0.45847374200820923\n",
            "Epoch 2, Batch 41, Loss: 0.6144454479217529\n",
            "Epoch 2, Batch 42, Loss: 0.6862594485282898\n",
            "Epoch 2, Batch 43, Loss: 0.2981870770454407\n",
            "Epoch 2, Batch 44, Loss: 0.45785126090049744\n",
            "Epoch 2, Batch 45, Loss: 0.6347271203994751\n",
            "Epoch 2, Batch 46, Loss: 0.38448357582092285\n",
            "Epoch 2, Batch 47, Loss: 0.2175108790397644\n",
            "Epoch 2, Batch 48, Loss: 0.5210166573524475\n",
            "Epoch 2, Batch 49, Loss: 0.4503432512283325\n",
            "Epoch 2, Batch 50, Loss: 0.5572026968002319\n",
            "Epoch 2, Batch 51, Loss: 0.6513981819152832\n",
            "Epoch 2, Batch 52, Loss: 0.4566812217235565\n",
            "Epoch 2, Batch 53, Loss: 0.4189504384994507\n",
            "Epoch 2, Batch 54, Loss: 0.6634800434112549\n",
            "Epoch 2, Batch 55, Loss: 0.5561572313308716\n",
            "Epoch 2, Batch 56, Loss: 0.1628974974155426\n",
            "Epoch 2, Batch 57, Loss: 0.5390669703483582\n",
            "Epoch 2, Batch 58, Loss: 0.504118800163269\n",
            "Epoch 2, Batch 59, Loss: 0.6557650566101074\n",
            "Epoch 2, Batch 60, Loss: 0.49388623237609863\n",
            "Epoch 2, Batch 61, Loss: 0.5023783445358276\n",
            "Epoch 2, Batch 62, Loss: 0.5099290013313293\n",
            "Epoch 2, Batch 63, Loss: 0.3353762626647949\n",
            "Epoch 2, Batch 64, Loss: 0.45540714263916016\n",
            "Epoch 2, Batch 65, Loss: 0.5742729902267456\n",
            "Epoch 2, Batch 66, Loss: 0.3889181613922119\n",
            "Epoch 2, Batch 67, Loss: 0.4571569561958313\n",
            "Epoch 2, Batch 68, Loss: 0.6435201168060303\n",
            "Epoch 2, Batch 69, Loss: 0.6773258447647095\n",
            "Epoch 2, Batch 70, Loss: 0.47040194272994995\n",
            "Epoch 2, Batch 71, Loss: 0.45984187722206116\n",
            "Epoch 2, Batch 72, Loss: 0.5459008812904358\n",
            "Epoch 2, Batch 73, Loss: 0.5276757478713989\n",
            "Epoch 2, Batch 74, Loss: 0.4314250946044922\n",
            "Epoch 2, Batch 75, Loss: 0.6063331365585327\n",
            "Epoch 2, Batch 76, Loss: 0.4065866470336914\n",
            "Epoch 2, Batch 77, Loss: 0.698572039604187\n",
            "Epoch 2, Batch 78, Loss: 0.522296667098999\n",
            "Epoch 2, Batch 79, Loss: 0.6584177613258362\n",
            "Epoch 2, Batch 80, Loss: 0.5315219759941101\n",
            "Epoch 2, Batch 81, Loss: 0.5516599416732788\n",
            "Epoch 2, Batch 82, Loss: 0.4968571364879608\n",
            "Epoch 2, Batch 83, Loss: 0.486847460269928\n",
            "Epoch 2, Batch 84, Loss: 0.291875958442688\n",
            "Epoch 2, Batch 85, Loss: 0.472321093082428\n",
            "Epoch 2, Batch 86, Loss: 0.5400488376617432\n",
            "Epoch 2, Batch 87, Loss: 0.4823399484157562\n",
            "Epoch 2, Batch 88, Loss: 0.457804411649704\n",
            "Epoch 2, Batch 89, Loss: 0.6666991710662842\n",
            "Epoch 2, Batch 90, Loss: 0.8147218227386475\n",
            "Epoch 2, Batch 91, Loss: 0.47215571999549866\n",
            "Epoch 2, Batch 92, Loss: 0.3195289969444275\n",
            "Epoch 2, Batch 93, Loss: 0.736933708190918\n",
            "Epoch 2, Batch 94, Loss: 0.46134448051452637\n",
            "Epoch 2, Batch 95, Loss: 0.3442564308643341\n",
            "Epoch 2, Batch 96, Loss: 0.5762443542480469\n",
            "Epoch 2, Batch 97, Loss: 0.36097094416618347\n",
            "Epoch 2, Batch 98, Loss: 0.5452297925949097\n",
            "Epoch 2, Batch 99, Loss: 0.4224300980567932\n",
            "Epoch 2, Batch 100, Loss: 0.33582210540771484\n",
            "Epoch 2, Batch 101, Loss: 0.5390040278434753\n",
            "Epoch 2, Batch 102, Loss: 0.3075528144836426\n",
            "Epoch 2, Batch 103, Loss: 0.48322707414627075\n",
            "Epoch 2, Batch 104, Loss: 0.6977400779724121\n",
            "Epoch 2, Batch 105, Loss: 0.60757976770401\n",
            "Epoch 2, Batch 106, Loss: 0.41504034399986267\n",
            "Epoch 2, Batch 107, Loss: 0.35448509454727173\n",
            "Epoch 2, Batch 108, Loss: 0.4148811101913452\n",
            "Epoch 2, Batch 109, Loss: 0.4983537197113037\n",
            "Epoch 2, Batch 110, Loss: 0.6066147089004517\n",
            "Epoch 2, Batch 111, Loss: 0.784903883934021\n",
            "Epoch 2, Batch 112, Loss: 0.5803287029266357\n",
            "Epoch 2, Batch 113, Loss: 0.28679358959198\n",
            "Epoch 2, Batch 114, Loss: 0.7294609546661377\n",
            "Epoch 2, Batch 115, Loss: 0.48556065559387207\n",
            "Epoch 2, Batch 116, Loss: 0.9033499956130981\n",
            "Epoch 2, Batch 117, Loss: 0.46272215247154236\n",
            "Epoch 2, Batch 118, Loss: 0.5720430016517639\n",
            "Epoch 2, Batch 119, Loss: 0.726020336151123\n",
            "Epoch 2, Batch 120, Loss: 0.7136225700378418\n",
            "Epoch 2, Batch 121, Loss: 0.7263696193695068\n",
            "Epoch 2, Batch 122, Loss: 0.46928417682647705\n",
            "Epoch 2, Batch 123, Loss: 0.5503857135772705\n",
            "Epoch 2, Batch 124, Loss: 0.4437483251094818\n",
            "Epoch 2, Batch 125, Loss: 0.49959516525268555\n",
            "Epoch 2, Batch 126, Loss: 0.7186233401298523\n",
            "Epoch 2, Batch 127, Loss: 0.4962479770183563\n",
            "Epoch 2, Batch 128, Loss: 0.5103244185447693\n",
            "Epoch 2, Batch 129, Loss: 0.4317295253276825\n",
            "Epoch 2, Batch 130, Loss: 0.4309004545211792\n",
            "Epoch 2, Batch 131, Loss: 0.5417009592056274\n",
            "Epoch 2, Batch 132, Loss: 0.5553378462791443\n",
            "Epoch 2, Batch 133, Loss: 0.4602934420108795\n",
            "Epoch 2, Batch 134, Loss: 0.3261604309082031\n",
            "Epoch 2, Batch 135, Loss: 0.6185685992240906\n",
            "Epoch 2, Batch 136, Loss: 0.3215736746788025\n",
            "Epoch 2, Batch 137, Loss: 0.3263434171676636\n",
            "Epoch 2, Batch 138, Loss: 0.5079533457756042\n",
            "Epoch 2, Batch 139, Loss: 0.5839980840682983\n",
            "Epoch 2, Batch 140, Loss: 0.40105175971984863\n",
            "Epoch 2, Batch 141, Loss: 0.5940810441970825\n",
            "Epoch 2, Batch 142, Loss: 0.36085036396980286\n",
            "Epoch 2, Batch 143, Loss: 0.6055481433868408\n",
            "Epoch 2, Batch 144, Loss: 0.4597584009170532\n",
            "Epoch 2, Batch 145, Loss: 0.5169396996498108\n",
            "Epoch 2, Batch 146, Loss: 0.39861151576042175\n",
            "Epoch 2, Batch 147, Loss: 0.4362977147102356\n",
            "Epoch 2, Batch 148, Loss: 0.5858335494995117\n",
            "Epoch 2, Batch 149, Loss: 0.4284822344779968\n",
            "Epoch 2, Batch 150, Loss: 0.3885895013809204\n",
            "Epoch 2, Batch 151, Loss: 0.5568134784698486\n",
            "Epoch 2, Batch 152, Loss: 0.5958811044692993\n",
            "Epoch 2, Batch 153, Loss: 0.605121374130249\n",
            "Epoch 2, Batch 154, Loss: 0.4268364906311035\n",
            "Epoch 2, Batch 155, Loss: 0.2320510447025299\n",
            "Epoch 2, Batch 156, Loss: 0.5158004760742188\n",
            "Epoch 2, Batch 157, Loss: 0.560836672782898\n",
            "Epoch 2, Batch 158, Loss: 0.6057389974594116\n",
            "Epoch 2, Batch 159, Loss: 0.5357768535614014\n",
            "Epoch 2, Batch 160, Loss: 0.44391047954559326\n",
            "Epoch 2, Batch 161, Loss: 0.40696394443511963\n",
            "Epoch 2, Batch 162, Loss: 0.35362446308135986\n",
            "Epoch 2, Batch 163, Loss: 0.5637232661247253\n",
            "Epoch 2, Batch 164, Loss: 0.4246145486831665\n",
            "Epoch 2, Batch 165, Loss: 0.502570629119873\n",
            "Epoch 2, Batch 166, Loss: 0.5507825016975403\n",
            "Epoch 2, Batch 167, Loss: 0.31835970282554626\n",
            "Epoch 2, Batch 168, Loss: 0.4119887948036194\n",
            "Epoch 2, Batch 169, Loss: 0.45643100142478943\n",
            "Epoch 2, Batch 170, Loss: 0.5339015126228333\n",
            "Epoch 2, Batch 171, Loss: 0.5454745292663574\n",
            "Epoch 2, Batch 172, Loss: 0.594606876373291\n",
            "Epoch 2, Batch 173, Loss: 0.3983219861984253\n",
            "Epoch 2, Batch 174, Loss: 0.5921003818511963\n",
            "Epoch 2, Batch 175, Loss: 0.6654099225997925\n",
            "Epoch 2, Batch 176, Loss: 0.35190239548683167\n",
            "Epoch 2, Batch 177, Loss: 0.6043218970298767\n",
            "Epoch 2, Batch 178, Loss: 0.4116921126842499\n",
            "Epoch 2, Batch 179, Loss: 0.5307816863059998\n",
            "Epoch 2, Batch 180, Loss: 0.46088969707489014\n",
            "Epoch 2, Batch 181, Loss: 0.6077125668525696\n",
            "Epoch 3, Batch 1, Loss: 0.49797600507736206\n",
            "Epoch 3, Batch 2, Loss: 0.6005027294158936\n",
            "Epoch 3, Batch 3, Loss: 0.5550627708435059\n",
            "Epoch 3, Batch 4, Loss: 0.505973756313324\n",
            "Epoch 3, Batch 5, Loss: 0.6971123814582825\n",
            "Epoch 3, Batch 6, Loss: 0.6197117567062378\n",
            "Epoch 3, Batch 7, Loss: 0.4729838967323303\n",
            "Epoch 3, Batch 8, Loss: 0.5860841274261475\n",
            "Epoch 3, Batch 9, Loss: 0.4732934832572937\n",
            "Epoch 3, Batch 10, Loss: 0.34310096502304077\n",
            "Epoch 3, Batch 11, Loss: 0.5144672393798828\n",
            "Epoch 3, Batch 12, Loss: 0.5741513967514038\n",
            "Epoch 3, Batch 13, Loss: 0.39695191383361816\n",
            "Epoch 3, Batch 14, Loss: 0.2785986661911011\n",
            "Epoch 3, Batch 15, Loss: 0.5826157331466675\n",
            "Epoch 3, Batch 16, Loss: 0.8758686780929565\n",
            "Epoch 3, Batch 17, Loss: 0.9320520162582397\n",
            "Epoch 3, Batch 18, Loss: 0.4512609541416168\n",
            "Epoch 3, Batch 19, Loss: 0.4053844213485718\n",
            "Epoch 3, Batch 20, Loss: 0.580363392829895\n",
            "Epoch 3, Batch 21, Loss: 0.5918199419975281\n",
            "Epoch 3, Batch 22, Loss: 0.5623337030410767\n",
            "Epoch 3, Batch 23, Loss: 0.6038717031478882\n",
            "Epoch 3, Batch 24, Loss: 0.35573673248291016\n",
            "Epoch 3, Batch 25, Loss: 0.5902405977249146\n",
            "Epoch 3, Batch 26, Loss: 0.4037487208843231\n",
            "Epoch 3, Batch 27, Loss: 0.4636680483818054\n",
            "Epoch 3, Batch 28, Loss: 0.5952099561691284\n",
            "Epoch 3, Batch 29, Loss: 0.695411205291748\n",
            "Epoch 3, Batch 30, Loss: 0.4651816189289093\n",
            "Epoch 3, Batch 31, Loss: 0.3983669877052307\n",
            "Epoch 3, Batch 32, Loss: 0.42583656311035156\n",
            "Epoch 3, Batch 33, Loss: 0.39158719778060913\n",
            "Epoch 3, Batch 34, Loss: 0.31022071838378906\n",
            "Epoch 3, Batch 35, Loss: 0.6328397989273071\n",
            "Epoch 3, Batch 36, Loss: 0.48419591784477234\n",
            "Epoch 3, Batch 37, Loss: 0.5755869150161743\n",
            "Epoch 3, Batch 38, Loss: 0.4408460259437561\n",
            "Epoch 3, Batch 39, Loss: 0.6796284317970276\n",
            "Epoch 3, Batch 40, Loss: 0.3178044557571411\n",
            "Epoch 3, Batch 41, Loss: 0.45353642106056213\n",
            "Epoch 3, Batch 42, Loss: 0.47682082653045654\n",
            "Epoch 3, Batch 43, Loss: 0.523664116859436\n",
            "Epoch 3, Batch 44, Loss: 0.4145401120185852\n",
            "Epoch 3, Batch 45, Loss: 0.27643609046936035\n",
            "Epoch 3, Batch 46, Loss: 0.5774721503257751\n",
            "Epoch 3, Batch 47, Loss: 0.6706558465957642\n",
            "Epoch 3, Batch 48, Loss: 0.649695634841919\n",
            "Epoch 3, Batch 49, Loss: 0.3238980174064636\n",
            "Epoch 3, Batch 50, Loss: 0.5459638237953186\n",
            "Epoch 3, Batch 51, Loss: 0.4058122932910919\n",
            "Epoch 3, Batch 52, Loss: 0.45854219794273376\n",
            "Epoch 3, Batch 53, Loss: 0.4842960238456726\n",
            "Epoch 3, Batch 54, Loss: 0.45847153663635254\n",
            "Epoch 3, Batch 55, Loss: 0.5716404914855957\n",
            "Epoch 3, Batch 56, Loss: 0.43384748697280884\n",
            "Epoch 3, Batch 57, Loss: 0.4724828600883484\n",
            "Epoch 3, Batch 58, Loss: 0.42805546522140503\n",
            "Epoch 3, Batch 59, Loss: 0.47055956721305847\n",
            "Epoch 3, Batch 60, Loss: 0.7021384835243225\n",
            "Epoch 3, Batch 61, Loss: 0.6281343102455139\n",
            "Epoch 3, Batch 62, Loss: 0.603312075138092\n",
            "Epoch 3, Batch 63, Loss: 0.6410423517227173\n",
            "Epoch 3, Batch 64, Loss: 0.6409432888031006\n",
            "Epoch 3, Batch 65, Loss: 0.7184463739395142\n",
            "Epoch 3, Batch 66, Loss: 0.4341962933540344\n",
            "Epoch 3, Batch 67, Loss: 0.3136429488658905\n",
            "Epoch 3, Batch 68, Loss: 0.49600735306739807\n",
            "Epoch 3, Batch 69, Loss: 0.5813925862312317\n",
            "Epoch 3, Batch 70, Loss: 0.5531931519508362\n",
            "Epoch 3, Batch 71, Loss: 0.5344431400299072\n",
            "Epoch 3, Batch 72, Loss: 0.3139820098876953\n",
            "Epoch 3, Batch 73, Loss: 0.41567134857177734\n",
            "Epoch 3, Batch 74, Loss: 0.6016753315925598\n",
            "Epoch 3, Batch 75, Loss: 0.6014206409454346\n",
            "Epoch 3, Batch 76, Loss: 0.4594353437423706\n",
            "Epoch 3, Batch 77, Loss: 0.4598548710346222\n",
            "Epoch 3, Batch 78, Loss: 0.6331520080566406\n",
            "Epoch 3, Batch 79, Loss: 0.4216642379760742\n",
            "Epoch 3, Batch 80, Loss: 0.3076190948486328\n",
            "Epoch 3, Batch 81, Loss: 0.44407856464385986\n",
            "Epoch 3, Batch 82, Loss: 0.4968736171722412\n",
            "Epoch 3, Batch 83, Loss: 0.37939244508743286\n",
            "Epoch 3, Batch 84, Loss: 0.9079859852790833\n",
            "Epoch 3, Batch 85, Loss: 0.4869251549243927\n",
            "Epoch 3, Batch 86, Loss: 0.550511360168457\n",
            "Epoch 3, Batch 87, Loss: 0.5568385720252991\n",
            "Epoch 3, Batch 88, Loss: 0.4661133885383606\n",
            "Epoch 3, Batch 89, Loss: 0.5281837582588196\n",
            "Epoch 3, Batch 90, Loss: 0.8256657123565674\n",
            "Epoch 3, Batch 91, Loss: 0.6459075212478638\n",
            "Epoch 3, Batch 92, Loss: 0.3299141824245453\n",
            "Epoch 3, Batch 93, Loss: 0.6131259202957153\n",
            "Epoch 3, Batch 94, Loss: 0.5034908056259155\n",
            "Epoch 3, Batch 95, Loss: 0.3698897957801819\n",
            "Epoch 3, Batch 96, Loss: 0.48996084928512573\n",
            "Epoch 3, Batch 97, Loss: 0.5677093267440796\n",
            "Epoch 3, Batch 98, Loss: 0.46687591075897217\n",
            "Epoch 3, Batch 99, Loss: 0.613319993019104\n",
            "Epoch 3, Batch 100, Loss: 0.41701942682266235\n",
            "Epoch 3, Batch 101, Loss: 0.3872847259044647\n",
            "Epoch 3, Batch 102, Loss: 0.4373304545879364\n",
            "Epoch 3, Batch 103, Loss: 0.6110705137252808\n",
            "Epoch 3, Batch 104, Loss: 0.3286178708076477\n",
            "Epoch 3, Batch 105, Loss: 0.7606500387191772\n",
            "Epoch 3, Batch 106, Loss: 0.6543905735015869\n",
            "Epoch 3, Batch 107, Loss: 0.34037286043167114\n",
            "Epoch 3, Batch 108, Loss: 0.6080028414726257\n",
            "Epoch 3, Batch 109, Loss: 0.5135411024093628\n",
            "Epoch 3, Batch 110, Loss: 0.5765384435653687\n",
            "Epoch 3, Batch 111, Loss: 0.4942050278186798\n",
            "Epoch 3, Batch 112, Loss: 0.42452260851860046\n",
            "Epoch 3, Batch 113, Loss: 0.4628767967224121\n",
            "Epoch 3, Batch 114, Loss: 0.4264093041419983\n",
            "Epoch 3, Batch 115, Loss: 0.5183123350143433\n",
            "Epoch 3, Batch 116, Loss: 0.4331267476081848\n",
            "Epoch 3, Batch 117, Loss: 0.3208719789981842\n",
            "Epoch 3, Batch 118, Loss: 0.5393557548522949\n",
            "Epoch 3, Batch 119, Loss: 0.4504532516002655\n",
            "Epoch 3, Batch 120, Loss: 0.35941147804260254\n",
            "Epoch 3, Batch 121, Loss: 0.5265675783157349\n",
            "Epoch 3, Batch 122, Loss: 0.5413998961448669\n",
            "Epoch 3, Batch 123, Loss: 0.3681446313858032\n",
            "Epoch 3, Batch 124, Loss: 0.6088006496429443\n",
            "Epoch 3, Batch 125, Loss: 0.46904316544532776\n",
            "Epoch 3, Batch 126, Loss: 0.5125402212142944\n",
            "Epoch 3, Batch 127, Loss: 0.4060748815536499\n",
            "Epoch 3, Batch 128, Loss: 0.45380762219429016\n",
            "Epoch 3, Batch 129, Loss: 0.5062337517738342\n",
            "Epoch 3, Batch 130, Loss: 0.48339882493019104\n",
            "Epoch 3, Batch 131, Loss: 0.4204685389995575\n",
            "Epoch 3, Batch 132, Loss: 0.31109267473220825\n",
            "Epoch 3, Batch 133, Loss: 0.33504408597946167\n",
            "Epoch 3, Batch 134, Loss: 0.4206727147102356\n",
            "Epoch 3, Batch 135, Loss: 0.3522029519081116\n",
            "Epoch 3, Batch 136, Loss: 0.4106031358242035\n",
            "Epoch 3, Batch 137, Loss: 0.5093441009521484\n",
            "Epoch 3, Batch 138, Loss: 0.5094516277313232\n",
            "Epoch 3, Batch 139, Loss: 0.4299640655517578\n",
            "Epoch 3, Batch 140, Loss: 0.7719852924346924\n",
            "Epoch 3, Batch 141, Loss: 0.6311142444610596\n",
            "Epoch 3, Batch 142, Loss: 0.4782082438468933\n",
            "Epoch 3, Batch 143, Loss: 0.47844141721725464\n",
            "Epoch 3, Batch 144, Loss: 0.5056828260421753\n",
            "Epoch 3, Batch 145, Loss: 0.4207365810871124\n",
            "Epoch 3, Batch 146, Loss: 0.34912562370300293\n",
            "Epoch 3, Batch 147, Loss: 0.6175655126571655\n",
            "Epoch 3, Batch 148, Loss: 0.38413572311401367\n",
            "Epoch 3, Batch 149, Loss: 0.7284708619117737\n",
            "Epoch 3, Batch 150, Loss: 0.36838239431381226\n",
            "Epoch 3, Batch 151, Loss: 0.5043783783912659\n",
            "Epoch 3, Batch 152, Loss: 0.4383750855922699\n",
            "Epoch 3, Batch 153, Loss: 0.4963441491127014\n",
            "Epoch 3, Batch 154, Loss: 0.6293099522590637\n",
            "Epoch 3, Batch 155, Loss: 0.505567729473114\n",
            "Epoch 3, Batch 156, Loss: 0.45023852586746216\n",
            "Epoch 3, Batch 157, Loss: 0.621785044670105\n",
            "Epoch 3, Batch 158, Loss: 0.4265260398387909\n",
            "Epoch 3, Batch 159, Loss: 0.5031179785728455\n",
            "Epoch 3, Batch 160, Loss: 0.513163685798645\n",
            "Epoch 3, Batch 161, Loss: 0.5312614440917969\n",
            "Epoch 3, Batch 162, Loss: 0.40886056423187256\n",
            "Epoch 3, Batch 163, Loss: 0.5631147623062134\n",
            "Epoch 3, Batch 164, Loss: 0.5563597679138184\n",
            "Epoch 3, Batch 165, Loss: 0.5375125408172607\n",
            "Epoch 3, Batch 166, Loss: 0.5216760635375977\n",
            "Epoch 3, Batch 167, Loss: 0.8369144201278687\n",
            "Epoch 3, Batch 168, Loss: 0.6137869358062744\n",
            "Epoch 3, Batch 169, Loss: 0.4681163430213928\n",
            "Epoch 3, Batch 170, Loss: 0.5008794665336609\n",
            "Epoch 3, Batch 171, Loss: 0.5016281604766846\n",
            "Epoch 3, Batch 172, Loss: 0.3192965090274811\n",
            "Epoch 3, Batch 173, Loss: 0.3634887635707855\n",
            "Epoch 3, Batch 174, Loss: 0.40219926834106445\n",
            "Epoch 3, Batch 175, Loss: 0.5259166955947876\n",
            "Epoch 3, Batch 176, Loss: 0.6019229888916016\n",
            "Epoch 3, Batch 177, Loss: 0.4564978778362274\n",
            "Epoch 3, Batch 178, Loss: 0.5417936444282532\n",
            "Epoch 3, Batch 179, Loss: 0.7417397499084473\n",
            "Epoch 3, Batch 180, Loss: 0.3778890073299408\n",
            "Epoch 3, Batch 181, Loss: 0.5657457113265991\n",
            "Epoch 4, Batch 1, Loss: 0.5300561189651489\n",
            "Epoch 4, Batch 2, Loss: 0.40774837136268616\n",
            "Epoch 4, Batch 3, Loss: 0.3693563938140869\n",
            "Epoch 4, Batch 4, Loss: 0.7370069026947021\n",
            "Epoch 4, Batch 5, Loss: 0.41998231410980225\n",
            "Epoch 4, Batch 6, Loss: 0.607666015625\n",
            "Epoch 4, Batch 7, Loss: 0.5110353827476501\n",
            "Epoch 4, Batch 8, Loss: 0.5659903287887573\n",
            "Epoch 4, Batch 9, Loss: 0.6123189330101013\n",
            "Epoch 4, Batch 10, Loss: 0.5329188108444214\n",
            "Epoch 4, Batch 11, Loss: 0.48090073466300964\n",
            "Epoch 4, Batch 12, Loss: 0.6158444881439209\n",
            "Epoch 4, Batch 13, Loss: 0.5261697769165039\n",
            "Epoch 4, Batch 14, Loss: 0.3292168974876404\n",
            "Epoch 4, Batch 15, Loss: 0.3643884062767029\n",
            "Epoch 4, Batch 16, Loss: 0.5308387279510498\n",
            "Epoch 4, Batch 17, Loss: 0.4082092046737671\n",
            "Epoch 4, Batch 18, Loss: 0.4311688542366028\n",
            "Epoch 4, Batch 19, Loss: 0.5635759830474854\n",
            "Epoch 4, Batch 20, Loss: 0.28914472460746765\n",
            "Epoch 4, Batch 21, Loss: 0.32274651527404785\n",
            "Epoch 4, Batch 22, Loss: 0.616462230682373\n",
            "Epoch 4, Batch 23, Loss: 0.3709164261817932\n",
            "Epoch 4, Batch 24, Loss: 0.5617403984069824\n",
            "Epoch 4, Batch 25, Loss: 0.3213841915130615\n",
            "Epoch 4, Batch 26, Loss: 0.4074053168296814\n",
            "Epoch 4, Batch 27, Loss: 0.8056439161300659\n",
            "Epoch 4, Batch 28, Loss: 0.18386170268058777\n",
            "Epoch 4, Batch 29, Loss: 0.6134930849075317\n",
            "Epoch 4, Batch 30, Loss: 0.5489293336868286\n",
            "Epoch 4, Batch 31, Loss: 0.48636892437934875\n",
            "Epoch 4, Batch 32, Loss: 0.5066989660263062\n",
            "Epoch 4, Batch 33, Loss: 0.6053631901741028\n",
            "Epoch 4, Batch 34, Loss: 0.5294472575187683\n",
            "Epoch 4, Batch 35, Loss: 0.47242939472198486\n",
            "Epoch 4, Batch 36, Loss: 0.461792916059494\n",
            "Epoch 4, Batch 37, Loss: 0.47918081283569336\n",
            "Epoch 4, Batch 38, Loss: 0.6117613315582275\n",
            "Epoch 4, Batch 39, Loss: 0.5838227272033691\n",
            "Epoch 4, Batch 40, Loss: 0.41483399271965027\n",
            "Epoch 4, Batch 41, Loss: 0.40824416279792786\n",
            "Epoch 4, Batch 42, Loss: 0.2579907178878784\n",
            "Epoch 4, Batch 43, Loss: 0.45782899856567383\n",
            "Epoch 4, Batch 44, Loss: 0.6507265567779541\n",
            "Epoch 4, Batch 45, Loss: 0.6046228408813477\n",
            "Epoch 4, Batch 46, Loss: 0.5439624786376953\n",
            "Epoch 4, Batch 47, Loss: 0.5985232591629028\n",
            "Epoch 4, Batch 48, Loss: 0.3272915780544281\n",
            "Epoch 4, Batch 49, Loss: 0.47435200214385986\n",
            "Epoch 4, Batch 50, Loss: 0.3837738633155823\n",
            "Epoch 4, Batch 51, Loss: 0.6733677387237549\n",
            "Epoch 4, Batch 52, Loss: 0.45889919996261597\n",
            "Epoch 4, Batch 53, Loss: 0.652933657169342\n",
            "Epoch 4, Batch 54, Loss: 0.8563921451568604\n",
            "Epoch 4, Batch 55, Loss: 0.5549749135971069\n",
            "Epoch 4, Batch 56, Loss: 0.7094285488128662\n",
            "Epoch 4, Batch 57, Loss: 0.5844752192497253\n",
            "Epoch 4, Batch 58, Loss: 0.503364622592926\n",
            "Epoch 4, Batch 59, Loss: 0.48094627261161804\n",
            "Epoch 4, Batch 60, Loss: 0.4377688467502594\n",
            "Epoch 4, Batch 61, Loss: 1.0169106721878052\n",
            "Epoch 4, Batch 62, Loss: 0.8983871340751648\n",
            "Epoch 4, Batch 63, Loss: 0.42280998826026917\n",
            "Epoch 4, Batch 64, Loss: 0.39103972911834717\n",
            "Epoch 4, Batch 65, Loss: 0.3270341157913208\n",
            "Epoch 4, Batch 66, Loss: 0.43876636028289795\n",
            "Epoch 4, Batch 67, Loss: 0.4897010922431946\n",
            "Epoch 4, Batch 68, Loss: 0.5737859606742859\n",
            "Epoch 4, Batch 69, Loss: 0.4876083433628082\n",
            "Epoch 4, Batch 70, Loss: 0.5046432614326477\n",
            "Epoch 4, Batch 71, Loss: 0.7714958190917969\n",
            "Epoch 4, Batch 72, Loss: 0.3531002104282379\n",
            "Epoch 4, Batch 73, Loss: 0.5472315549850464\n",
            "Epoch 4, Batch 74, Loss: 0.7066993117332458\n",
            "Epoch 4, Batch 75, Loss: 0.589289128780365\n",
            "Epoch 4, Batch 76, Loss: 0.7040750980377197\n",
            "Epoch 4, Batch 77, Loss: 0.3483421206474304\n",
            "Epoch 4, Batch 78, Loss: 0.5940274596214294\n",
            "Epoch 4, Batch 79, Loss: 0.5482483506202698\n",
            "Epoch 4, Batch 80, Loss: 0.48445162177085876\n",
            "Epoch 4, Batch 81, Loss: 0.5036695599555969\n",
            "Epoch 4, Batch 82, Loss: 0.4718751907348633\n",
            "Epoch 4, Batch 83, Loss: 0.5194792747497559\n",
            "Epoch 4, Batch 84, Loss: 0.6524637937545776\n",
            "Epoch 4, Batch 85, Loss: 0.7031489014625549\n",
            "Epoch 4, Batch 86, Loss: 0.44051703810691833\n",
            "Epoch 4, Batch 87, Loss: 0.6342973709106445\n",
            "Epoch 4, Batch 88, Loss: 0.5747754573822021\n",
            "Epoch 4, Batch 89, Loss: 0.5941123962402344\n",
            "Epoch 4, Batch 90, Loss: 0.4100342094898224\n",
            "Epoch 4, Batch 91, Loss: 0.40488022565841675\n",
            "Epoch 4, Batch 92, Loss: 0.43224960565567017\n",
            "Epoch 4, Batch 93, Loss: 0.482543021440506\n",
            "Epoch 4, Batch 94, Loss: 0.47013190388679504\n",
            "Epoch 4, Batch 95, Loss: 0.4957629442214966\n",
            "Epoch 4, Batch 96, Loss: 0.42099830508232117\n",
            "Epoch 4, Batch 97, Loss: 0.38619229197502136\n",
            "Epoch 4, Batch 98, Loss: 0.5543022155761719\n",
            "Epoch 4, Batch 99, Loss: 0.46360957622528076\n",
            "Epoch 4, Batch 100, Loss: 0.4182053208351135\n",
            "Epoch 4, Batch 101, Loss: 0.7258826494216919\n",
            "Epoch 4, Batch 102, Loss: 0.37516093254089355\n",
            "Epoch 4, Batch 103, Loss: 0.6633117198944092\n",
            "Epoch 4, Batch 104, Loss: 0.354695200920105\n",
            "Epoch 4, Batch 105, Loss: 0.7217245101928711\n",
            "Epoch 4, Batch 106, Loss: 0.5116937160491943\n",
            "Epoch 4, Batch 107, Loss: 0.49887144565582275\n",
            "Epoch 4, Batch 108, Loss: 0.32494014501571655\n",
            "Epoch 4, Batch 109, Loss: 0.5575037002563477\n",
            "Epoch 4, Batch 110, Loss: 0.3677849769592285\n",
            "Epoch 4, Batch 111, Loss: 0.5613303780555725\n",
            "Epoch 4, Batch 112, Loss: 0.4142824709415436\n",
            "Epoch 4, Batch 113, Loss: 0.4948308765888214\n",
            "Epoch 4, Batch 114, Loss: 0.44715428352355957\n",
            "Epoch 4, Batch 115, Loss: 0.4450297951698303\n",
            "Epoch 4, Batch 116, Loss: 0.5382777452468872\n",
            "Epoch 4, Batch 117, Loss: 0.4375208020210266\n",
            "Epoch 4, Batch 118, Loss: 0.5393766164779663\n",
            "Epoch 4, Batch 119, Loss: 0.5110821723937988\n",
            "Epoch 4, Batch 120, Loss: 0.43645232915878296\n",
            "Epoch 4, Batch 121, Loss: 0.41794854402542114\n",
            "Epoch 4, Batch 122, Loss: 0.3837737739086151\n",
            "Epoch 4, Batch 123, Loss: 0.28969743847846985\n",
            "Epoch 4, Batch 124, Loss: 0.7876492738723755\n",
            "Epoch 4, Batch 125, Loss: 0.43149060010910034\n",
            "Epoch 4, Batch 126, Loss: 0.38508957624435425\n",
            "Epoch 4, Batch 127, Loss: 0.5562760829925537\n",
            "Epoch 4, Batch 128, Loss: 0.46225404739379883\n",
            "Epoch 4, Batch 129, Loss: 0.5252923369407654\n",
            "Epoch 4, Batch 130, Loss: 0.6400174498558044\n",
            "Epoch 4, Batch 131, Loss: 0.6148539781570435\n",
            "Epoch 4, Batch 132, Loss: 0.3309258818626404\n",
            "Epoch 4, Batch 133, Loss: 0.48767295479774475\n",
            "Epoch 4, Batch 134, Loss: 0.6192388534545898\n",
            "Epoch 4, Batch 135, Loss: 0.3655674457550049\n",
            "Epoch 4, Batch 136, Loss: 0.3412015438079834\n",
            "Epoch 4, Batch 137, Loss: 0.3768989145755768\n",
            "Epoch 4, Batch 138, Loss: 0.524707019329071\n",
            "Epoch 4, Batch 139, Loss: 0.5274854898452759\n",
            "Epoch 4, Batch 140, Loss: 0.40543854236602783\n",
            "Epoch 4, Batch 141, Loss: 0.5207088589668274\n",
            "Epoch 4, Batch 142, Loss: 0.5243545770645142\n",
            "Epoch 4, Batch 143, Loss: 0.4622790813446045\n",
            "Epoch 4, Batch 144, Loss: 0.41894474625587463\n",
            "Epoch 4, Batch 145, Loss: 0.6319730281829834\n",
            "Epoch 4, Batch 146, Loss: 0.4494670629501343\n",
            "Epoch 4, Batch 147, Loss: 0.6931506395339966\n",
            "Epoch 4, Batch 148, Loss: 0.5031926035881042\n",
            "Epoch 4, Batch 149, Loss: 0.5208912491798401\n",
            "Epoch 4, Batch 150, Loss: 0.48788517713546753\n",
            "Epoch 4, Batch 151, Loss: 0.3179672062397003\n",
            "Epoch 4, Batch 152, Loss: 0.555183470249176\n",
            "Epoch 4, Batch 153, Loss: 0.4550084173679352\n",
            "Epoch 4, Batch 154, Loss: 0.4707864224910736\n",
            "Epoch 4, Batch 155, Loss: 0.4653755724430084\n",
            "Epoch 4, Batch 156, Loss: 0.5554217100143433\n",
            "Epoch 4, Batch 157, Loss: 0.4215325713157654\n",
            "Epoch 4, Batch 158, Loss: 0.1903374046087265\n",
            "Epoch 4, Batch 159, Loss: 0.6612303853034973\n",
            "Epoch 4, Batch 160, Loss: 0.2894636392593384\n",
            "Epoch 4, Batch 161, Loss: 0.6731881499290466\n",
            "Epoch 4, Batch 162, Loss: 0.6272611618041992\n",
            "Epoch 4, Batch 163, Loss: 0.5662540197372437\n",
            "Epoch 4, Batch 164, Loss: 0.4609825015068054\n",
            "Epoch 4, Batch 165, Loss: 0.6758817434310913\n",
            "Epoch 4, Batch 166, Loss: 0.4213337302207947\n",
            "Epoch 4, Batch 167, Loss: 0.4179045855998993\n",
            "Epoch 4, Batch 168, Loss: 0.7445492744445801\n",
            "Epoch 4, Batch 169, Loss: 0.5247365832328796\n",
            "Epoch 4, Batch 170, Loss: 0.374619722366333\n",
            "Epoch 4, Batch 171, Loss: 0.4271833598613739\n",
            "Epoch 4, Batch 172, Loss: 0.3781101107597351\n",
            "Epoch 4, Batch 173, Loss: 0.626520574092865\n",
            "Epoch 4, Batch 174, Loss: 0.4785485565662384\n",
            "Epoch 4, Batch 175, Loss: 0.48627352714538574\n",
            "Epoch 4, Batch 176, Loss: 0.5822498202323914\n",
            "Epoch 4, Batch 177, Loss: 0.31478315591812134\n",
            "Epoch 4, Batch 178, Loss: 0.4498255252838135\n",
            "Epoch 4, Batch 179, Loss: 0.3461362421512604\n",
            "Epoch 4, Batch 180, Loss: 0.5420568585395813\n",
            "Epoch 4, Batch 181, Loss: 0.5380340218544006\n",
            "Epoch 5, Batch 1, Loss: 0.5841599106788635\n",
            "Epoch 5, Batch 2, Loss: 0.34884822368621826\n",
            "Epoch 5, Batch 3, Loss: 0.42488282918930054\n",
            "Epoch 5, Batch 4, Loss: 0.3646474778652191\n",
            "Epoch 5, Batch 5, Loss: 0.4411322772502899\n",
            "Epoch 5, Batch 6, Loss: 0.6212271451950073\n",
            "Epoch 5, Batch 7, Loss: 0.5699427127838135\n",
            "Epoch 5, Batch 8, Loss: 0.5322018265724182\n",
            "Epoch 5, Batch 9, Loss: 0.7181564569473267\n",
            "Epoch 5, Batch 10, Loss: 0.41733479499816895\n",
            "Epoch 5, Batch 11, Loss: 0.6915106773376465\n",
            "Epoch 5, Batch 12, Loss: 0.4327932894229889\n",
            "Epoch 5, Batch 13, Loss: 0.5617334842681885\n",
            "Epoch 5, Batch 14, Loss: 0.6911741495132446\n",
            "Epoch 5, Batch 15, Loss: 0.5716866254806519\n",
            "Epoch 5, Batch 16, Loss: 0.37102532386779785\n",
            "Epoch 5, Batch 17, Loss: 0.5416418313980103\n",
            "Epoch 5, Batch 18, Loss: 0.46509742736816406\n",
            "Epoch 5, Batch 19, Loss: 0.4779631197452545\n",
            "Epoch 5, Batch 20, Loss: 0.7060034871101379\n",
            "Epoch 5, Batch 21, Loss: 0.44627583026885986\n",
            "Epoch 5, Batch 22, Loss: 0.49022287130355835\n",
            "Epoch 5, Batch 23, Loss: 0.6883347034454346\n",
            "Epoch 5, Batch 24, Loss: 0.41655993461608887\n",
            "Epoch 5, Batch 25, Loss: 0.24835987389087677\n",
            "Epoch 5, Batch 26, Loss: 0.6657768487930298\n",
            "Epoch 5, Batch 27, Loss: 0.44672566652297974\n",
            "Epoch 5, Batch 28, Loss: 0.4592844843864441\n",
            "Epoch 5, Batch 29, Loss: 0.40329816937446594\n",
            "Epoch 5, Batch 30, Loss: 0.7781847715377808\n",
            "Epoch 5, Batch 31, Loss: 0.536277174949646\n",
            "Epoch 5, Batch 32, Loss: 0.5360538959503174\n",
            "Epoch 5, Batch 33, Loss: 0.5072181224822998\n",
            "Epoch 5, Batch 34, Loss: 0.32667475938796997\n",
            "Epoch 5, Batch 35, Loss: 0.4283428490161896\n",
            "Epoch 5, Batch 36, Loss: 0.3024951219558716\n",
            "Epoch 5, Batch 37, Loss: 0.5229339003562927\n",
            "Epoch 5, Batch 38, Loss: 0.3480837941169739\n",
            "Epoch 5, Batch 39, Loss: 0.4361066222190857\n",
            "Epoch 5, Batch 40, Loss: 0.8016132116317749\n",
            "Epoch 5, Batch 41, Loss: 0.3377875089645386\n",
            "Epoch 5, Batch 42, Loss: 0.6548469066619873\n",
            "Epoch 5, Batch 43, Loss: 0.6236094832420349\n",
            "Epoch 5, Batch 44, Loss: 0.6706336140632629\n",
            "Epoch 5, Batch 45, Loss: 0.4690761864185333\n",
            "Epoch 5, Batch 46, Loss: 0.48034781217575073\n",
            "Epoch 5, Batch 47, Loss: 0.4667896330356598\n",
            "Epoch 5, Batch 48, Loss: 0.46449899673461914\n",
            "Epoch 5, Batch 49, Loss: 0.5305262804031372\n",
            "Epoch 5, Batch 50, Loss: 0.547113299369812\n",
            "Epoch 5, Batch 51, Loss: 0.4367942810058594\n",
            "Epoch 5, Batch 52, Loss: 0.7288722395896912\n",
            "Epoch 5, Batch 53, Loss: 0.39650195837020874\n",
            "Epoch 5, Batch 54, Loss: 0.3476021885871887\n",
            "Epoch 5, Batch 55, Loss: 0.636066198348999\n",
            "Epoch 5, Batch 56, Loss: 0.5130627155303955\n",
            "Epoch 5, Batch 57, Loss: 0.626964807510376\n",
            "Epoch 5, Batch 58, Loss: 0.4407546818256378\n",
            "Epoch 5, Batch 59, Loss: 0.49902012944221497\n",
            "Epoch 5, Batch 60, Loss: 0.3913649320602417\n",
            "Epoch 5, Batch 61, Loss: 0.6223822236061096\n",
            "Epoch 5, Batch 62, Loss: 0.4168267548084259\n",
            "Epoch 5, Batch 63, Loss: 0.46831047534942627\n",
            "Epoch 5, Batch 64, Loss: 0.7315170168876648\n",
            "Epoch 5, Batch 65, Loss: 0.5916067361831665\n",
            "Epoch 5, Batch 66, Loss: 0.4071146249771118\n",
            "Epoch 5, Batch 67, Loss: 0.509651243686676\n",
            "Epoch 5, Batch 68, Loss: 0.6102700233459473\n",
            "Epoch 5, Batch 69, Loss: 0.6058741807937622\n",
            "Epoch 5, Batch 70, Loss: 0.5189225077629089\n",
            "Epoch 5, Batch 71, Loss: 0.4650803506374359\n",
            "Epoch 5, Batch 72, Loss: 0.6557756066322327\n",
            "Epoch 5, Batch 73, Loss: 0.6687278747558594\n",
            "Epoch 5, Batch 74, Loss: 0.689595103263855\n",
            "Epoch 5, Batch 75, Loss: 0.5729219913482666\n",
            "Epoch 5, Batch 76, Loss: 0.6748459339141846\n",
            "Epoch 5, Batch 77, Loss: 0.4615676701068878\n",
            "Epoch 5, Batch 78, Loss: 0.39466941356658936\n",
            "Epoch 5, Batch 79, Loss: 0.26075494289398193\n",
            "Epoch 5, Batch 80, Loss: 0.36869052052497864\n",
            "Epoch 5, Batch 81, Loss: 0.18187034130096436\n",
            "Epoch 5, Batch 82, Loss: 0.7454999089241028\n",
            "Epoch 5, Batch 83, Loss: 0.5954479575157166\n",
            "Epoch 5, Batch 84, Loss: 0.47115641832351685\n",
            "Epoch 5, Batch 85, Loss: 0.42805156111717224\n",
            "Epoch 5, Batch 86, Loss: 0.41323527693748474\n",
            "Epoch 5, Batch 87, Loss: 0.4457851052284241\n",
            "Epoch 5, Batch 88, Loss: 0.47724515199661255\n",
            "Epoch 5, Batch 89, Loss: 0.5704818367958069\n",
            "Epoch 5, Batch 90, Loss: 0.41333651542663574\n",
            "Epoch 5, Batch 91, Loss: 0.5112228393554688\n",
            "Epoch 5, Batch 92, Loss: 0.42828890681266785\n",
            "Epoch 5, Batch 93, Loss: 0.2960354685783386\n",
            "Epoch 5, Batch 94, Loss: 0.3860887885093689\n",
            "Epoch 5, Batch 95, Loss: 0.3756937086582184\n",
            "Epoch 5, Batch 96, Loss: 0.5240946412086487\n",
            "Epoch 5, Batch 97, Loss: 0.5269464254379272\n",
            "Epoch 5, Batch 98, Loss: 0.3990108370780945\n",
            "Epoch 5, Batch 99, Loss: 0.6601470112800598\n",
            "Epoch 5, Batch 100, Loss: 0.4819392263889313\n",
            "Epoch 5, Batch 101, Loss: 0.6932247877120972\n",
            "Epoch 5, Batch 102, Loss: 0.6065644025802612\n",
            "Epoch 5, Batch 103, Loss: 0.33282148838043213\n",
            "Epoch 5, Batch 104, Loss: 0.6004033088684082\n",
            "Epoch 5, Batch 105, Loss: 0.4437134861946106\n",
            "Epoch 5, Batch 106, Loss: 0.6485792398452759\n",
            "Epoch 5, Batch 107, Loss: 0.5670596361160278\n",
            "Epoch 5, Batch 108, Loss: 0.4274388253688812\n",
            "Epoch 5, Batch 109, Loss: 0.5423853397369385\n",
            "Epoch 5, Batch 110, Loss: 0.30824699997901917\n",
            "Epoch 5, Batch 111, Loss: 0.527504563331604\n",
            "Epoch 5, Batch 112, Loss: 0.6817800402641296\n",
            "Epoch 5, Batch 113, Loss: 0.4541640281677246\n",
            "Epoch 5, Batch 114, Loss: 0.6182243824005127\n",
            "Epoch 5, Batch 115, Loss: 0.5597403049468994\n",
            "Epoch 5, Batch 116, Loss: 0.36184966564178467\n",
            "Epoch 5, Batch 117, Loss: 0.46923303604125977\n",
            "Epoch 5, Batch 118, Loss: 0.5077864527702332\n",
            "Epoch 5, Batch 119, Loss: 0.3463706970214844\n",
            "Epoch 5, Batch 120, Loss: 0.4805660843849182\n",
            "Epoch 5, Batch 121, Loss: 0.33875173330307007\n",
            "Epoch 5, Batch 122, Loss: 0.3722047507762909\n",
            "Epoch 5, Batch 123, Loss: 0.48039424419403076\n",
            "Epoch 5, Batch 124, Loss: 0.5888528823852539\n",
            "Epoch 5, Batch 125, Loss: 0.5505238771438599\n",
            "Epoch 5, Batch 126, Loss: 0.4996555745601654\n",
            "Epoch 5, Batch 127, Loss: 0.5046374797821045\n",
            "Epoch 5, Batch 128, Loss: 0.5150492191314697\n",
            "Epoch 5, Batch 129, Loss: 0.6703730821609497\n",
            "Epoch 5, Batch 130, Loss: 0.6109070777893066\n",
            "Epoch 5, Batch 131, Loss: 0.49938255548477173\n",
            "Epoch 5, Batch 132, Loss: 0.4220121204853058\n",
            "Epoch 5, Batch 133, Loss: 0.5130598545074463\n",
            "Epoch 5, Batch 134, Loss: 0.5716359615325928\n",
            "Epoch 5, Batch 135, Loss: 0.33896398544311523\n",
            "Epoch 5, Batch 136, Loss: 0.6381373405456543\n",
            "Epoch 5, Batch 137, Loss: 0.6743955612182617\n",
            "Epoch 5, Batch 138, Loss: 0.5124006867408752\n",
            "Epoch 5, Batch 139, Loss: 0.497370183467865\n",
            "Epoch 5, Batch 140, Loss: 0.5016477108001709\n",
            "Epoch 5, Batch 141, Loss: 0.8279526233673096\n",
            "Epoch 5, Batch 142, Loss: 0.46244943141937256\n",
            "Epoch 5, Batch 143, Loss: 0.4314368963241577\n",
            "Epoch 5, Batch 144, Loss: 0.29256191849708557\n",
            "Epoch 5, Batch 145, Loss: 0.6815589070320129\n",
            "Epoch 5, Batch 146, Loss: 0.44808584451675415\n",
            "Epoch 5, Batch 147, Loss: 0.23549577593803406\n",
            "Epoch 5, Batch 148, Loss: 0.5978763103485107\n",
            "Epoch 5, Batch 149, Loss: 0.5488513708114624\n",
            "Epoch 5, Batch 150, Loss: 0.6364278197288513\n",
            "Epoch 5, Batch 151, Loss: 0.5559991598129272\n",
            "Epoch 5, Batch 152, Loss: 0.5002268552780151\n",
            "Epoch 5, Batch 153, Loss: 0.5151723623275757\n",
            "Epoch 5, Batch 154, Loss: 0.5522754192352295\n",
            "Epoch 5, Batch 155, Loss: 0.4407220482826233\n",
            "Epoch 5, Batch 156, Loss: 0.42310386896133423\n",
            "Epoch 5, Batch 157, Loss: 0.5689961314201355\n",
            "Epoch 5, Batch 158, Loss: 0.5481414794921875\n",
            "Epoch 5, Batch 159, Loss: 0.4832068085670471\n",
            "Epoch 5, Batch 160, Loss: 0.526091456413269\n",
            "Epoch 5, Batch 161, Loss: 0.6746649742126465\n",
            "Epoch 5, Batch 162, Loss: 0.3042565882205963\n",
            "Epoch 5, Batch 163, Loss: 0.3218134343624115\n",
            "Epoch 5, Batch 164, Loss: 0.5182732343673706\n",
            "Epoch 5, Batch 165, Loss: 0.37400156259536743\n",
            "Epoch 5, Batch 166, Loss: 0.5456340312957764\n",
            "Epoch 5, Batch 167, Loss: 0.3416990637779236\n",
            "Epoch 5, Batch 168, Loss: 0.602837085723877\n",
            "Epoch 5, Batch 169, Loss: 0.6009457111358643\n",
            "Epoch 5, Batch 170, Loss: 0.6231354475021362\n",
            "Epoch 5, Batch 171, Loss: 0.5559395551681519\n",
            "Epoch 5, Batch 172, Loss: 0.4622282385826111\n",
            "Epoch 5, Batch 173, Loss: 0.5568405985832214\n",
            "Epoch 5, Batch 174, Loss: 0.49202054738998413\n",
            "Epoch 5, Batch 175, Loss: 0.5628202557563782\n",
            "Epoch 5, Batch 176, Loss: 0.5268574357032776\n",
            "Epoch 5, Batch 177, Loss: 0.3374207317829132\n",
            "Epoch 5, Batch 178, Loss: 0.45695629715919495\n",
            "Epoch 5, Batch 179, Loss: 0.3866267204284668\n",
            "Epoch 5, Batch 180, Loss: 0.5515880584716797\n",
            "Epoch 5, Batch 181, Loss: 0.4954303205013275\n",
            "Epoch 1, Batch 1, Loss: 0.5985932350158691\n",
            "Epoch 1, Batch 2, Loss: 0.9062845706939697\n",
            "Epoch 1, Batch 3, Loss: 0.5979344844818115\n",
            "Epoch 1, Batch 4, Loss: 0.5190889835357666\n",
            "Epoch 1, Batch 5, Loss: 0.6056115031242371\n",
            "Epoch 1, Batch 6, Loss: 0.7295634746551514\n",
            "Epoch 1, Batch 7, Loss: 0.4335806369781494\n",
            "Epoch 1, Batch 8, Loss: 0.5117753744125366\n",
            "Epoch 1, Batch 9, Loss: 0.6501758694648743\n",
            "Epoch 1, Batch 10, Loss: 0.5230056047439575\n",
            "Epoch 1, Batch 11, Loss: 0.6610794067382812\n",
            "Epoch 1, Batch 12, Loss: 0.4503658711910248\n",
            "Epoch 1, Batch 13, Loss: 0.46228742599487305\n",
            "Epoch 1, Batch 14, Loss: 0.6840476989746094\n",
            "Epoch 1, Batch 15, Loss: 0.4209511876106262\n",
            "Epoch 1, Batch 16, Loss: 0.5964727401733398\n",
            "Epoch 1, Batch 17, Loss: 0.47354063391685486\n",
            "Epoch 1, Batch 18, Loss: 0.7022120952606201\n",
            "Epoch 1, Batch 19, Loss: 0.4994812607765198\n",
            "Epoch 1, Batch 20, Loss: 0.25495225191116333\n",
            "Epoch 1, Batch 21, Loss: 0.4939159154891968\n",
            "Epoch 1, Batch 22, Loss: 0.4309505820274353\n",
            "Epoch 1, Batch 23, Loss: 0.4410244822502136\n",
            "Epoch 1, Batch 24, Loss: 0.6618759036064148\n",
            "Epoch 1, Batch 25, Loss: 0.5213897228240967\n",
            "Epoch 1, Batch 26, Loss: 0.47606754302978516\n",
            "Epoch 1, Batch 27, Loss: 0.5652705430984497\n",
            "Epoch 1, Batch 28, Loss: 0.3102545738220215\n",
            "Epoch 1, Batch 29, Loss: 0.43266069889068604\n",
            "Epoch 1, Batch 30, Loss: 0.6445614695549011\n",
            "Epoch 1, Batch 31, Loss: 0.5579686760902405\n",
            "Epoch 1, Batch 32, Loss: 0.4570164978504181\n",
            "Epoch 1, Batch 33, Loss: 0.39650824666023254\n",
            "Epoch 1, Batch 34, Loss: 0.34991008043289185\n",
            "Epoch 1, Batch 35, Loss: 0.5556187629699707\n",
            "Epoch 1, Batch 36, Loss: 0.522210955619812\n",
            "Epoch 1, Batch 37, Loss: 0.6704862713813782\n",
            "Epoch 1, Batch 38, Loss: 0.4015766978263855\n",
            "Epoch 1, Batch 39, Loss: 0.39696431159973145\n",
            "Epoch 1, Batch 40, Loss: 0.7234623432159424\n",
            "Epoch 1, Batch 41, Loss: 0.5711002945899963\n",
            "Epoch 1, Batch 42, Loss: 0.44630056619644165\n",
            "Epoch 1, Batch 43, Loss: 0.3689221143722534\n",
            "Epoch 1, Batch 44, Loss: 0.5768637657165527\n",
            "Epoch 1, Batch 45, Loss: 0.566864013671875\n",
            "Epoch 1, Batch 46, Loss: 0.8950226902961731\n",
            "Epoch 2, Batch 1, Loss: 0.5292540788650513\n",
            "Epoch 2, Batch 2, Loss: 0.5919113755226135\n",
            "Epoch 2, Batch 3, Loss: 0.41168856620788574\n",
            "Epoch 2, Batch 4, Loss: 0.5137996077537537\n",
            "Epoch 2, Batch 5, Loss: 0.5683566331863403\n",
            "Epoch 2, Batch 6, Loss: 0.5935810804367065\n",
            "Epoch 2, Batch 7, Loss: 0.38914161920547485\n",
            "Epoch 2, Batch 8, Loss: 0.4892694652080536\n",
            "Epoch 2, Batch 9, Loss: 0.2871542274951935\n",
            "Epoch 2, Batch 10, Loss: 0.5291004180908203\n",
            "Epoch 2, Batch 11, Loss: 0.6417945623397827\n",
            "Epoch 2, Batch 12, Loss: 0.48453617095947266\n",
            "Epoch 2, Batch 13, Loss: 0.43110787868499756\n",
            "Epoch 2, Batch 14, Loss: 0.5561976432800293\n",
            "Epoch 2, Batch 15, Loss: 0.37741661071777344\n",
            "Epoch 2, Batch 16, Loss: 0.6496905088424683\n",
            "Epoch 2, Batch 17, Loss: 0.4960867762565613\n",
            "Epoch 2, Batch 18, Loss: 0.7101492285728455\n",
            "Epoch 2, Batch 19, Loss: 0.48759764432907104\n",
            "Epoch 2, Batch 20, Loss: 0.23479801416397095\n",
            "Epoch 2, Batch 21, Loss: 0.5267866849899292\n",
            "Epoch 2, Batch 22, Loss: 0.4391944110393524\n",
            "Epoch 2, Batch 23, Loss: 0.44823330640792847\n",
            "Epoch 2, Batch 24, Loss: 0.6697768568992615\n",
            "Epoch 2, Batch 25, Loss: 0.5242154598236084\n",
            "Epoch 2, Batch 26, Loss: 0.4733869433403015\n",
            "Epoch 2, Batch 27, Loss: 0.5620666742324829\n",
            "Epoch 2, Batch 28, Loss: 0.3102463185787201\n",
            "Epoch 2, Batch 29, Loss: 0.43930917978286743\n",
            "Epoch 2, Batch 30, Loss: 0.6520178914070129\n",
            "Epoch 2, Batch 31, Loss: 0.5667673349380493\n",
            "Epoch 2, Batch 32, Loss: 0.4628646969795227\n",
            "Epoch 2, Batch 33, Loss: 0.40250739455223083\n",
            "Epoch 2, Batch 34, Loss: 0.3423725962638855\n",
            "Epoch 2, Batch 35, Loss: 0.5516813397407532\n",
            "Epoch 2, Batch 36, Loss: 0.5406760573387146\n",
            "Epoch 2, Batch 37, Loss: 0.6904782652854919\n",
            "Epoch 2, Batch 38, Loss: 0.4238492250442505\n",
            "Epoch 2, Batch 39, Loss: 0.3867185115814209\n",
            "Epoch 2, Batch 40, Loss: 0.7034974098205566\n",
            "Epoch 2, Batch 41, Loss: 0.5506238341331482\n",
            "Epoch 2, Batch 42, Loss: 0.4608123004436493\n",
            "Epoch 2, Batch 43, Loss: 0.39467179775238037\n",
            "Epoch 2, Batch 44, Loss: 0.5820939540863037\n",
            "Epoch 2, Batch 45, Loss: 0.5323408842086792\n",
            "Epoch 2, Batch 46, Loss: 0.818300724029541\n",
            "Epoch 3, Batch 1, Loss: 0.526237964630127\n",
            "Epoch 3, Batch 2, Loss: 0.6085653901100159\n",
            "Epoch 3, Batch 3, Loss: 0.4031867980957031\n",
            "Epoch 3, Batch 4, Loss: 0.5063102841377258\n",
            "Epoch 3, Batch 5, Loss: 0.5332749485969543\n",
            "Epoch 3, Batch 6, Loss: 0.5845200419425964\n",
            "Epoch 3, Batch 7, Loss: 0.384563684463501\n",
            "Epoch 3, Batch 8, Loss: 0.4900151789188385\n",
            "Epoch 3, Batch 9, Loss: 0.32119646668434143\n",
            "Epoch 3, Batch 10, Loss: 0.5033600926399231\n",
            "Epoch 3, Batch 11, Loss: 0.6240409016609192\n",
            "Epoch 3, Batch 12, Loss: 0.4689671993255615\n",
            "Epoch 3, Batch 13, Loss: 0.42821067571640015\n",
            "Epoch 3, Batch 14, Loss: 0.5727032423019409\n",
            "Epoch 3, Batch 15, Loss: 0.38297173380851746\n",
            "Epoch 3, Batch 16, Loss: 0.6137101650238037\n",
            "Epoch 3, Batch 17, Loss: 0.4766465723514557\n",
            "Epoch 3, Batch 18, Loss: 0.6902723908424377\n",
            "Epoch 3, Batch 19, Loss: 0.4866413176059723\n",
            "Epoch 3, Batch 20, Loss: 0.2477114200592041\n",
            "Epoch 3, Batch 21, Loss: 0.5017952919006348\n",
            "Epoch 3, Batch 22, Loss: 0.42996567487716675\n",
            "Epoch 3, Batch 23, Loss: 0.43902143836021423\n",
            "Epoch 3, Batch 24, Loss: 0.6583156585693359\n",
            "Epoch 3, Batch 25, Loss: 0.5278078317642212\n",
            "Epoch 3, Batch 26, Loss: 0.4843263626098633\n",
            "Epoch 3, Batch 27, Loss: 0.564189076423645\n",
            "Epoch 3, Batch 28, Loss: 0.2886720597743988\n",
            "Epoch 3, Batch 29, Loss: 0.43009525537490845\n",
            "Epoch 3, Batch 30, Loss: 0.6485883593559265\n",
            "Epoch 3, Batch 31, Loss: 0.5732306241989136\n",
            "Epoch 3, Batch 32, Loss: 0.46619054675102234\n",
            "Epoch 3, Batch 33, Loss: 0.3999038636684418\n",
            "Epoch 3, Batch 34, Loss: 0.3443957567214966\n",
            "Epoch 3, Batch 35, Loss: 0.5379453897476196\n",
            "Epoch 3, Batch 36, Loss: 0.5359184145927429\n",
            "Epoch 3, Batch 37, Loss: 0.7171406745910645\n",
            "Epoch 3, Batch 38, Loss: 0.4545251131057739\n",
            "Epoch 3, Batch 39, Loss: 0.3872908353805542\n",
            "Epoch 3, Batch 40, Loss: 0.6903017163276672\n",
            "Epoch 3, Batch 41, Loss: 0.5379400253295898\n",
            "Epoch 3, Batch 42, Loss: 0.46295368671417236\n",
            "Epoch 3, Batch 43, Loss: 0.4141714870929718\n",
            "Epoch 3, Batch 44, Loss: 0.5882471203804016\n",
            "Epoch 3, Batch 45, Loss: 0.5232260823249817\n",
            "Epoch 3, Batch 46, Loss: 0.7877786159515381\n",
            "Epoch 4, Batch 1, Loss: 0.5253370404243469\n",
            "Epoch 4, Batch 2, Loss: 0.6073011159896851\n",
            "Epoch 4, Batch 3, Loss: 0.3978068232536316\n",
            "Epoch 4, Batch 4, Loss: 0.5062204599380493\n",
            "Epoch 4, Batch 5, Loss: 0.5165435075759888\n",
            "Epoch 4, Batch 6, Loss: 0.5796383023262024\n",
            "Epoch 4, Batch 7, Loss: 0.3830656111240387\n",
            "Epoch 4, Batch 8, Loss: 0.48358723521232605\n",
            "Epoch 4, Batch 9, Loss: 0.33337944746017456\n",
            "Epoch 4, Batch 10, Loss: 0.4896441102027893\n",
            "Epoch 4, Batch 11, Loss: 0.6128771305084229\n",
            "Epoch 4, Batch 12, Loss: 0.44763538241386414\n",
            "Epoch 4, Batch 13, Loss: 0.42473626136779785\n",
            "Epoch 4, Batch 14, Loss: 0.5776442885398865\n",
            "Epoch 4, Batch 15, Loss: 0.37943384051322937\n",
            "Epoch 4, Batch 16, Loss: 0.5988138318061829\n",
            "Epoch 4, Batch 17, Loss: 0.46184733510017395\n",
            "Epoch 4, Batch 18, Loss: 0.6640754342079163\n",
            "Epoch 4, Batch 19, Loss: 0.4788673222064972\n",
            "Epoch 4, Batch 20, Loss: 0.25564873218536377\n",
            "Epoch 4, Batch 21, Loss: 0.4909838140010834\n",
            "Epoch 4, Batch 22, Loss: 0.4327392578125\n",
            "Epoch 4, Batch 23, Loss: 0.42088159918785095\n",
            "Epoch 4, Batch 24, Loss: 0.6360675096511841\n",
            "Epoch 4, Batch 25, Loss: 0.5362886786460876\n",
            "Epoch 4, Batch 26, Loss: 0.49172675609588623\n",
            "Epoch 4, Batch 27, Loss: 0.5583747029304504\n",
            "Epoch 4, Batch 28, Loss: 0.2775161862373352\n",
            "Epoch 4, Batch 29, Loss: 0.42521125078201294\n",
            "Epoch 4, Batch 30, Loss: 0.6537097692489624\n",
            "Epoch 4, Batch 31, Loss: 0.5608649253845215\n",
            "Epoch 4, Batch 32, Loss: 0.45654797554016113\n",
            "Epoch 4, Batch 33, Loss: 0.4022306203842163\n",
            "Epoch 4, Batch 34, Loss: 0.347056120634079\n",
            "Epoch 4, Batch 35, Loss: 0.5441601872444153\n",
            "Epoch 4, Batch 36, Loss: 0.5237460136413574\n",
            "Epoch 4, Batch 37, Loss: 0.7022072076797485\n",
            "Epoch 4, Batch 38, Loss: 0.45789051055908203\n",
            "Epoch 4, Batch 39, Loss: 0.3906632959842682\n",
            "Epoch 4, Batch 40, Loss: 0.6721266508102417\n",
            "Epoch 4, Batch 41, Loss: 0.5199751853942871\n",
            "Epoch 4, Batch 42, Loss: 0.452017217874527\n",
            "Epoch 4, Batch 43, Loss: 0.40578556060791016\n",
            "Epoch 4, Batch 44, Loss: 0.5903695821762085\n",
            "Epoch 4, Batch 45, Loss: 0.518028974533081\n",
            "Epoch 4, Batch 46, Loss: 0.7636604309082031\n",
            "Epoch 5, Batch 1, Loss: 0.5210604667663574\n",
            "Epoch 5, Batch 2, Loss: 0.6009795069694519\n",
            "Epoch 5, Batch 3, Loss: 0.39769983291625977\n",
            "Epoch 5, Batch 4, Loss: 0.4964960217475891\n",
            "Epoch 5, Batch 5, Loss: 0.5236889123916626\n",
            "Epoch 5, Batch 6, Loss: 0.567719578742981\n",
            "Epoch 5, Batch 7, Loss: 0.38562241196632385\n",
            "Epoch 5, Batch 8, Loss: 0.4763900637626648\n",
            "Epoch 5, Batch 9, Loss: 0.317577600479126\n",
            "Epoch 5, Batch 10, Loss: 0.49584832787513733\n",
            "Epoch 5, Batch 11, Loss: 0.5968838334083557\n",
            "Epoch 5, Batch 12, Loss: 0.4339340329170227\n",
            "Epoch 5, Batch 13, Loss: 0.4159853458404541\n",
            "Epoch 5, Batch 14, Loss: 0.5572783946990967\n",
            "Epoch 5, Batch 15, Loss: 0.37262797355651855\n",
            "Epoch 5, Batch 16, Loss: 0.5894360542297363\n",
            "Epoch 5, Batch 17, Loss: 0.4615105986595154\n",
            "Epoch 5, Batch 18, Loss: 0.6371763944625854\n",
            "Epoch 5, Batch 19, Loss: 0.48285117745399475\n",
            "Epoch 5, Batch 20, Loss: 0.26041245460510254\n",
            "Epoch 5, Batch 21, Loss: 0.4836885333061218\n",
            "Epoch 5, Batch 22, Loss: 0.44264888763427734\n",
            "Epoch 5, Batch 23, Loss: 0.4369218647480011\n",
            "Epoch 5, Batch 24, Loss: 0.6404074430465698\n",
            "Epoch 5, Batch 25, Loss: 0.5351805090904236\n",
            "Epoch 5, Batch 26, Loss: 0.49034515023231506\n",
            "Epoch 5, Batch 27, Loss: 0.5663021802902222\n",
            "Epoch 5, Batch 28, Loss: 0.2843989431858063\n",
            "Epoch 5, Batch 29, Loss: 0.4239847660064697\n",
            "Epoch 5, Batch 30, Loss: 0.6566383838653564\n",
            "Epoch 5, Batch 31, Loss: 0.5629876255989075\n",
            "Epoch 5, Batch 32, Loss: 0.4582732319831848\n",
            "Epoch 5, Batch 33, Loss: 0.4050530195236206\n",
            "Epoch 5, Batch 34, Loss: 0.342914342880249\n",
            "Epoch 5, Batch 35, Loss: 0.5417995452880859\n",
            "Epoch 5, Batch 36, Loss: 0.5237183570861816\n",
            "Epoch 5, Batch 37, Loss: 0.6965522766113281\n",
            "Epoch 5, Batch 38, Loss: 0.4365862011909485\n",
            "Epoch 5, Batch 39, Loss: 0.3809185326099396\n",
            "Epoch 5, Batch 40, Loss: 0.6890666484832764\n",
            "Epoch 5, Batch 41, Loss: 0.5141444802284241\n",
            "Epoch 5, Batch 42, Loss: 0.4569486081600189\n",
            "Epoch 5, Batch 43, Loss: 0.39722737669944763\n",
            "Epoch 5, Batch 44, Loss: 0.583994448184967\n",
            "Epoch 5, Batch 45, Loss: 0.5092278718948364\n",
            "Epoch 5, Batch 46, Loss: 0.7656172513961792\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 700x300 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "USA transfer learning model R¬≤: -0.0183\n",
            "USA transfer learning model RMSE: 0.6897\n"
          ]
        }
      ],
      "source": [
        "\n",
        "class TransferModel(tf.keras.Model):\n",
        "    def __init__(self, pretrained_model, new_dense_units, drop_out=0.75, name='TransferModel'):\n",
        "        super(TransferModel, self).__init__(name=name)\n",
        "\n",
        "        # Use the LSTM part from the pretrained model\n",
        "        self.lstm = pretrained_model.lstm  # LSTM layer from the pretrained model\n",
        "        self.lstm.trainable = False  # Freeze the LSTM layers\n",
        "\n",
        "        # Add new dense layers\n",
        "        self.dense1 = Dense(new_dense_units, activation='relu')\n",
        "        self.dense2 = Dense(1, activation=None)  # Final output layer\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        x = self.lstm(inputs, training=training)  # Pass inputs through frozen LSTM\n",
        "        x = self.dense1(x)  # Pass through new Dense layer\n",
        "        output = self.dense2(x)  # Final output layer\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "pretrained_model.load_weights('pretrained_model.weights.h5')  # Load pretrained weights\n",
        "\n",
        "usa_trans_model = TransferModel(pretrained_model, new_dense_units=356, drop_out=0.75)\n",
        "usa_trans_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), loss='rmse')\n",
        "\n",
        "usa_trans_model, r2list, rmselist = train_model(usa_trans_model, dataset_usa, config, num_epochs=5)\n",
        "r2list_eval, rmselist_eval = eval_model(usa_trans_model, dataset_usa, config, 5)\n",
        "plot_training_results(r2list, rmselist, r2list_eval, rmselist_eval)\n",
        "r2, rmse = test_model(usa_trans_model, dataset_usa, config)\n",
        "\n",
        "print(f'USA transfer learning model R¬≤: {r2:.4f}')\n",
        "print(f'USA transfer learning model RMSE: {rmse:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "IxwiT83MgEL1",
        "outputId": "f79ae2d1-ca12-480b-f1ed-e5d6604b7a9b"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAArIAAAEiCAYAAAAF9zFeAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAB8KUlEQVR4nO3deVwU9f8H8NfsyS0eICCnFyB44y2CCgqaaZ6oKR5ZVpZKl1QeZEVamZpa9iux+mqAZ+UJeYFXpYYHghciHhze3MuyO78/cFeWXWAXWIbdfT8fj62dmc/MvN+7OPve2c98hmFZlgUhhBBCCCEGhsd1AIQQQgghhNQFFbKEEEIIIcQgUSFLCCGEEEIMEhWyhBBCCCHEIFEhSwghhBBCDBIVsoQQQgghxCBRIUsIIYQQQgwSFbKEEEIIIcQgUSFLCCGEEEIMEhWyhBiRZcuWgWEYPHjwgOtQCCGEAGAYBvPmzeM6DKNFhSyp1ubNm8EwjPIhEAjQpk0bzJgxA3fv3lVpGxsbi/79+yMgIAA+Pj748ccftdoHy7L49ddfMWjQINja2sLCwgKdO3fGJ598gqKiIn2kVS+KQrG6R05ODtchEkIaiS7HSAAIDAwEwzDo0KGDxu0lJiYqt7V9+3aVZRcvXsT48ePh5uYGMzMztGnTBsHBwfj2229V2rm7u1d7fAoJCdEqr6ysLMydOxfu7u4Qi8Wwt7fHmDFjcOLECS1fmcZV0zF57ty5XIdH9EzAdQCk6fvkk0/g4eGB0tJSnD59Gps3b8bx48dx6dIlmJmZAQD69OmDY8eOQSgUIiUlBT169EBQUBDc3d2r3a5MJsOUKVMQHx8Pf39/LFu2DBYWFkhOTkZUVBS2bduGv/76C61bt26kTLX33XffwcrKSm2+ra1t4wdDCOGUNsdIBTMzM1y/fh3//PMPevfurbJsy5YtMDMzQ2lpqcr8kydPYvDgwXB1dcWcOXPg4OCA27dv4/Tp01izZg3eeustlfbdunXDO++8oxank5NTrbmcOHECI0aMAAC88sor6NSpE3JycrB582b4+/tr3F9TEBwcjOnTp6vN79ixIwfRkEbFElKNmJgYFgD777//qsz/4IMPWABsXFycxvXOnTvH8ng8NjMzs8btf/755ywA9t1331Vb9scff7A8Ho8NCQmpewJ1VFRUVO2ypUuXsgDY+/fvN2JE2mvq8RFiTHQ9RgYEBLA+Pj6sp6cnu2DBApVlJSUlrI2NDTtu3DgWALtt2zblshEjRrB2dnbs48eP1WLIzc1VmXZzc2NHjhxZp3wePXrEOjg4sK1bt2avX7+usqy4uJj19/dneTwee+LEiTptv65KSkpYmUxW7XIA7JtvvtmIEemmqcdn6KhrAdGZv78/AODGjRtqywoKChAeHo758+fDzc2t2m2UlJTgyy+/RMeOHREdHa22fNSoUQgPD8eBAwdw+vRpAMALL7yAtm3batxev3794OfnpzLvf//7H3r27Alzc3O0aNECYWFhuH37tkqbwMBA+Pr64uzZsxg0aBAsLCzw4Ycf1vwCaOHo0aNgGAZxcXH48MMP4eDgAEtLS7z44otqMQDAtm3blLG2atUKL7/8ssafJtPT0zFx4kTY2dnB3Nwcnp6e+Oijj9TaPXnyBDNmzICtrS2aNWuGmTNnori4WKVNYmIiBg4cCFtbW1hZWcHT07NBcifE1NV0jASAyZMnIy4uDnK5XDnvzz//RHFxMSZOnKjW/saNG/Dx8dH4i4+9vX3DBA1g48aNyMnJwZdffol27dqpLDM3N8fPP/8MhmHwySefAADOnDkDhmHw888/q23r4MGDYBgGe/bsUc67e/cuZs2ahdatW0MsFsPHxwebNm1SWU9x7IyNjcXHH3+MNm3awMLCAvn5+fXOr/Lxvn///jA3N4eHhwe+//57tbZ5eXmYPXs2WrduDTMzM3Tt2lVjnnK5HGvWrEHnzp1hZmYGOzs7hISE4MyZM2ptd+/eDV9fX2XuBw4cUFleUFCABQsWqHTpCA4Oxrlz5+qduzGjrgVEZ5mZmQCA5s2bq8wvKSnBmDFj0L59e3z55Zc1buP48eN4/Pgx5s+fD4FA85/h9OnTERMTgz179qBv376YNGkSpk+fjn///Re9evVStrt16xZOnz6tss/PPvsMixcvxsSJE/HKK6/g/v37+PbbbzFo0CD8999/Kh8IDx8+RGhoKMLCwvDyyy9r1ZXh0aNHavMEAoHaB81nn30GhmHwwQcfIC8vD6tXr0ZQUBBSUlJgbm4OoKKf3cyZM9GrVy9ER0cjNzcXa9aswYkTJ1RivXDhAvz9/SEUCvHqq6/C3d0dN27cwJ9//onPPvtMZb8TJ06Eh4cHoqOjce7cOfz444+wt7fHihUrAACpqal44YUX0KVLF3zyyScQi8W4fv16k+0DR4ghqe4YqTBlyhQsW7YMR48exZAhQwAAW7duxdChQzUWpm5ubjh16hQuXboEX1/fWvcvlUo1XvBpaWmpPO5o8ueff8LMzExjMQ0AHh4eGDhwIA4fPoySkhL4+fmhbdu2iI+PR3h4uErbuLg4NG/eHMOHDwcA5Obmom/fvsoLn+zs7LB//37Mnj0b+fn5WLBggcr6y5cvh0gkwrvvvguJRAKRSFRjzqWlpRpztrGxUVn38ePHGDFiBCZOnIjJkycjPj4er7/+OkQiEWbNmgWg4rMsMDAQ169fx7x58+Dh4YFt27ZhxowZePLkCebPn6/c3uzZs7F582aEhobilVdeQXl5OZKTk3H69GmVkyvHjx/Hzp078cYbb8Da2hpr167FuHHjkJWVhZYtWwIA5s6di+3bt2PevHno1KkTHj58iOPHjyMtLQ09evSoMX+TxvUpYdJ0KX42++uvv9j79++zt2/fZrdv387a2dmxYrGYvX37trJtcXExGxQUxE6dOpWVSqW1bnv16tUsAHbXrl3Vtnn06BELgB07dizLsiz79OlTViwWs++8845Ku5UrV7IMw7C3bt1iWZZlMzMzWT6fz3722Wcq7S5evMgKBAKV+QEBASwA9vvvv681ZpZ9/tO9poenp6ey3ZEjR1gAbJs2bdj8/Hzl/Pj4eBYAu2bNGpZlWbasrIy1t7dnfX192ZKSEmW7PXv2sADYJUuWKOcNGjSItba2VuapIJfL1eKbNWuWSpuXXnqJbdmypXL6m2++oS4IhNSTLsdIln3etYBlWdbPz4+dPXs2y7Is+/jxY1YkErE///yz8thRuWtBQkICy+fzWT6fz/br1499//332YMHD7JlZWVqMbm5uVV7jIqOjq4xH1tbW7Zr1641tnn77bdZAOyFCxdYlmXZyMhIVigUso8ePVK2kUgkrK2trcpxaPbs2ayjoyP74MEDle2FhYWxzZo1Y4uLi1mWfX7sbNu2rXJebarLFwD722+/Kdspjvdff/21SqzdunVj7e3tla+n4vPpf//7n7JdWVkZ269fP9bKykp5TD98+DALgH377bfVYqp8XAbAikQile4a58+fZwGw3377rXJes2bNqAtCHVDXAlKroKAg2NnZwcXFBePHj4elpSX++OMPODs7K9t8+umnOHz4MG7fvo2goCAEBgbi1KlT1W6zoKAAAGBtbV1tG8UyxU9KNjY2CA0NRXx8PFiWVbaLi4tD37594erqCgDYuXMn5HI5Jk6ciAcPHigfDg4O6NChA44cOaKyH7FYjJkzZ+r0muzYsQOJiYkqj5iYGLV206dPV8lx/PjxcHR0xL59+wBU/DSXl5eHN954Q+WikJEjR8LLywt79+4FANy/fx9JSUmYNWuWMk8FhmHU9lv1Sl1/f388fPhQ+VoqzvL+/vvvKj9vEkJ0p80xsqopU6Zg586dKCsrw/bt28Hn8/HSSy9pbBscHIxTp07hxRdfxPnz57Fy5UoMHz4cbdq0wR9//KHWvk+fPmrHp8TEREyePLnGPAoKCmo8JgPqx+VJkyZBKpVi586dyjYJCQl48uQJJk2aBKBidJodO3Zg1KhRYFlW5bg8fPhwPH36VO3n8/Dw8BrPHlc1evRojTkPHjxYpZ1AIMBrr72mnBaJRHjttdeQl5eHs2fPAgD27dsHBwcHlddLKBTi7bffRmFhIY4dOwag4nOAYRgsXbpULZ6qx+WgoCCV7hpdunSBjY0NMjIylPNsbW3x999/4969e1rnTahrAdHC+vXr0bFjRzx9+hSbNm1CUlISxGKxSpvPPvtM7eftmigOhoqCVhNNxe6kSZOwe/dunDp1Cv3798eNGzdw9uxZrF69Wtnm2rVrYFm22iFuhEKhynSbNm1q/dmqqkGDBqFVq1a1tqsaA8MwaN++vfKnx1u3bgEAPD091db18vLC8ePHAUB5sNPmZ0UAasWu4ifOx48fw8bGBpMmTcKPP/6IV155BYsWLcLQoUMxduxYjB8/Hjwefb8lRBfaHCOrCgsLw7vvvov9+/djy5YteOGFF2osInv16qUsfM+fP49du3bhm2++wfjx45GSkoJOnTop27Zq1QpBQUE652FtbV3jMRlQPy537doVXl5eiIuLw+zZswFUnFxo1aqVstvE/fv38eTJE/zwww/44YcfNG43Ly9PZdrDw0On2J2dnbXK2cnJCZaWlirzFCMbZGZmom/fvrh16xY6dOigdiz09vYG8Py4fePGDTg5OaFFixa17rfqMRmoOC4/fvxYOb1y5UqEh4fDxcUFPXv2xIgRIzB9+vRqrw0hFaiQJbXq3bu3sq/PmDFjMHDgQEyZMgVXrlzROASVNhQHhAsXLmDMmDEa21y4cAEAVA7Qo0aNgoWFBeLj49G/f3/Ex8eDx+NhwoQJyjZyuRwMw2D//v3g8/lq260asy7f+g2FprwBKM9km5ubIykpCUeOHMHevXtx4MABxMXFYciQIUhISKh2fUKIurocIx0dHREYGIivv/4aJ06cwI4dO7Tal0gkQq9evdCrVy907NgRM2fOxLZt2zSeFdSVt7c3/vvvP0gkkmoL8QsXLkAoFKp8SZ80aRI+++wzPHjwANbW1vjjjz8wefJk5fUPil99Xn75ZbW+tApdunRRmTa243Jtx2Sg4toGf39/7Nq1CwkJCfjyyy+xYsUK7Ny5E6GhoY0VqsGhUy9EJ3w+H9HR0bh37x7WrVtX5+0orpbfunUrZDKZxja//PILgIrRChQsLS3xwgsvYNu2bZDL5YiLi4O/v7/K+Ijt2rUDy7Lw8PBAUFCQ2qNv3751jltX165dU5lmWRbXr19Xjq+rGNnhypUrauteuXJFuVzxjfzSpUsNFhuPx8PQoUOxatUqXL58GZ999hkOHz6s1vWCEKI9XY6RU6ZMQXJyMmxsbJRjt+pCUTxnZ2fXKdaqXnjhBZSWlmLbtm0al2dmZiI5ORlDhgxRKTQnTZqE8vJy7NixA/v370d+fj7CwsKUy+3s7GBtbQ2ZTKbxmBwUFNSgoy/U5N69e2o327l69SoAqByXr127ptbtKj09XbkcqPisuXfvnsaLf+vK0dERb7zxBnbv3o2bN2+iZcuWOv3aaYqokCU6CwwMRO/evbF69Wq1gbu1ZWFhgXfffRdXrlzROHzU3r17sXnzZgwfPlyt8Jw0aRLu3buHH3/8EefPn1f2w1IYO3Ys+Hw+oqKiVL7tAhWF5MOHD+sUc1388ssvKj/Vbd++HdnZ2cpv135+frC3t8f3338PiUSibLd//36kpaVh5MiRACo+CAYNGoRNmzYhKytLZR9Vc9SGpgNvt27dAEAlDkKI7rQ9Ro4fPx5Lly7Fhg0bauzedOTIEY3/zhV97TV1TaqL1157Dfb29njvvfdU+m4CFaMCzJw5EyzLYsmSJSrLvL290blzZ8TFxSEuLg6Ojo4YNGiQcjmfz8e4ceOwY8cOjV/G79+/3yDxa6O8vBwbN25UTpeVlWHjxo2ws7NDz549AQAjRoxATk4O4uLiVNb79ttvYWVlhYCAAADAuHHjwLIsoqKi1Paj63FZJpPh6dOnKvPs7e3h5OREx+RaUNcCUifvvfceJkyYgM2bN9f5FoCLFi3Cf//9hxUrVuDUqVMYN24czM3Ncfz4cfzvf/+Dt7e3xnH7RowYAWtra7z77rvKA2Rl7dq1w6efforIyEhkZmZizJgxsLa2xs2bN7Fr1y68+uqrePfdd+sUs8L27ds1/mQYHBysMnxXixYtMHDgQMycORO5ublYvXo12rdvjzlz5gCo6K+7YsUKzJw5EwEBAZg8ebJy+C13d3csXLhQua21a9di4MCB6NGjB1599VV4eHggMzMTe/fuRUpKik7xf/LJJ0hKSsLIkSPh5uaGvLw8bNiwAc7Ozhg4cGDdXhRCiJI2x8hmzZph2bJltW7rrbfeQnFxMV566SV4eXmhrKwMJ0+eRFxcHNzd3dUuVr179y7+97//qW3Hysqq2q5cANCyZUts374dI0eORI8ePdTu7HX9+nWsWbMG/fv3V1t30qRJWLJkCczMzDB79my1/qVffPEFjhw5gj59+mDOnDno1KkTHj16hHPnzuGvv/6q91nNq1evasy5devWCA4OVk47OTlhxYoVyMzMRMeOHREXF4eUlBT88MMPyusnXn31VWzcuBEzZszA2bNn4e7uju3bt+PEiRNYvXq1sn/w4MGDMW3aNKxduxbXrl1DSEgI5HI5kpOTMXjwYMybN0/r+AsKCuDs7Izx48eja9eusLKywl9//YV///0XX3/9db1eG6PHwUgJxEBUd9calmVZmUzGtmvXjm3Xrh1bXl5e533IZDI2JiaGHTBgAGtjY8OamZmxPj4+bFRUFFtYWFjtelOnTmUBsEFBQdW22bFjBztw4EDW0tKStbS0ZL28vNg333yTvXLlirJN5eFwtFHT8FsA2CNHjrAs+3wImd9++42NjIxk7e3tWXNzc3bkyJFqw2exLMvGxcWx3bt3Z8ViMduiRQt26tSp7J07d9TaXbp0iX3ppZdYW1tb1szMjPX09GQXL16sFl/VYbUU7+XNmzdZlmXZQ4cOsaNHj2adnJxYkUjEOjk5sZMnT2avXr2q9WtBiKnT9RipzfFG0/Bb+/fvZ2fNmsV6eXmxVlZWrEgkYtu3b8++9dZbGu/sVd3xyc3NTau8bt68yc6ZM4d1dXVlhUIh26pVK/bFF19kk5OTq13n2rVryv0cP35cY5vc3Fz2zTffZF1cXFihUMg6ODiwQ4cOZX/44Yca869NTcfkgIAAZTvF63/mzBm2X79+rJmZGevm5sauW7dOY6wzZ85kW7VqxYpEIrZz585sTEyMWrvy8nL2yy+/ZL28vFiRSMTa2dmxoaGh7NmzZ1Xi0zSslpubGxseHs6ybMUwYO+99x7btWtX1tramrW0tGS7du3KbtiwQevXwVQxLFuH3yUJITU6evQoBg8ejG3btmH8+PFch0MIISYvMDAQDx48aNBrDQj3qI8sIYQQQggxSFTIEkIIIYQQg0SFLCGEEEIIMUjUR5YQQgghhBgkOiNLCCGEEEIMEhWyhBBCCCHEIJnUDRHkcjnu3bsHa2trMAzDdTiEEAPHsiwKCgrg5OSkNgC8KaFjKyGkIelybDWpQvbevXtwcXHhOgxCiJG5ffs2nJ2duQ6DM3RsJYTogzbHVpMqZBW3lbt9+zZsbGxqbS+VSpGQkIBhw4Ypb11nKih3yp1yr11+fj5cXFyUxxZTpeuxFaC/NcrdtHI31bwB/R9bTaqQVfzkZWNjo3Uha2FhARsbG5P8w6PcKXdTUp/cTf3ndF2PrQD9rVHuppW7qeYN6P/YarqdugghhBBCiEEzuEJ2/fr1cHd3h5mZGfr06YN//vmH65AIIYQQQggHDKqQjYuLQ0REBJYuXYpz586ha9euGD58OPLy8rgOzajIZXLcOnYLj5Me49axW5DL5FyH1Ggod8rd1HLngkwuw7Fbx5D0OAnHbh2DTC7jOiRCiIEyqD6yq1atwpw5czBz5kwAwPfff4+9e/di06ZNWLRoEcfRGYe0nWk4MP8A8u/kAwBurboFG2cbhKwJgfdYb46j0y9TzT05uwgP0x7gZvhOtdw9fh6Llt6t4O9oyXGU+mHKuXNlZ9pOzD8wH3fy7wAAVt1aBWcbZ6wJWYOx3mM5jo5wQSaTQSqVch2GXkmlUggEApSWlkImM60vbppyFwqF4PP5DbJ9gylky8rKcPbsWURGRirn8Xg8BAUF4dSpUxrXkUgkkEgkyun8/IoPKqlUqtU/GkUbY/8HpvDHsTtIO5AB2bMPdIX8u/nYcSAD3jZmeDHAOIcYMuXc76fm4WpLa5SHdAR+PKOcXxTaEenNLdExNQ/SVpS7JqZybGgoO9N2Ynz8eLBQvTP63fy7GB8/Htsnbqdi1oSwLIucnBw8efKE61D0jmVZODg44Pbt2yZ3cWh1udva2sLBwaHer4fBFLIPHjyATCZD69atVea3bt0a6enpGteJjo5GVFSU2vyEhARYWFhove/ExETdgjVArIzFlWNiCF7vA7CArNKHOn+2HwSv98GVn//F3vzzYPgN84+QZVmAhfKhMg2AlT97In+27NnzioWq7XXZlnLfiucyFjdv2tac+6Z/Ef/vSYCBynaV+8Lz6VqXsaj4IK/UTq29tssqb6tybVC1fTXLWBmLnC05wOTuELzRB0BF/vxXKvIu3/A3Lv/2Hx5NdVB53xmGqXgtlDOgMl3TcgaVlumwntbbrbxtVLrqlXn2MjDPX5asb7KACd0qcuczkG0+B/707src03efB29jp2r/5ouLizXOJ+pkchnmH5ivVsQCFX/DDBgsOLAAoz1Hg89rmDM1pGlTFLH29vawsLAw6gJPLpejsLAQVlZWJnfzlKq5syyL4uJiZbdQR0fHem3fYArZuoiMjERERIRyWjEu2bBhw6odIuZkbikYBuhnbwapVIrExEQEBwdDKBTiVF4pWBbo39qssVJoNLeO3cL5tVvAL5arFjRznhU03/8D2aazSP/NDHyRACzLPvtAYvC8VmMrnlepqyraKp5XqqkURYni4MV7Ps0Az5fxmCrtFc+rTKNSe2VBU2kf1f0fAHgl4NmlQ/BGHzCuzSA/eB28kA7gj/SEbP9VyC8/QEZapW3zFM+ZSs+hNo9hmIqe6CrtdN8OGAYMX4ftKParaTtV4uH5Vfxffi8fgjf6gD+3NxgeA3luIXgB7kCgB/I0veZqr636vMpFpNprrnFede+ppjaVitpa5jGKv6EqBC9Uev5abwhe6w0AKN/wN2Q/noEMgK+NL9wC3DSur/iVh9QuOStZ2Z1AExYsbuffRnJWMgLdAxsvMMIJmUymLGJbtmzJdTh6J5fLUVZWBjMzM5MsZKvmbm5uDgDIy8uDvb19vboZGEwh26pVK/D5fOTm5qrMz83NhYODg8Z1xGIxxGKx2nyhUFjtWGYCgRTJ2cXg8/no3VKobP/PQylO5kng72jByRhwLMtCxgLlchblz/4vlbMoZ1mUyytPP2ujYbqcRcU8+bN12OftiqwsIfpzGiAWgJWUQ/BGH2VBCwCCub0hmFvxIV9T754qJ8MMEv8FL/Bf8Ho+HdoR/NCOHEbUuBRFH6+1FdDaiuNoGh8rlamclS+5X1Ltv3lTGw+yPrILshu0HTFs5eXlAKDTr6PEuCjee6lUahqFrEgkQs+ePXHo0CGMGTMGQEWVf+jQIcybN6/B9jPAoeKFTc4uhkxWUQSfyitVFrGK5SzLQlqpGCyXo1Jh+Wy60vPyZ8ukVYpI1fU0F6jlz/alV2IhmDb6/1BWnrirNK3xF2HlPEb5XJdlgOqJOTxrr6mNtKgMj64/Vp4uZjxbgeExYOUs2Eu5gJwFWBatO7eGua2Z2knI5ycKGdUTxsrnjMZ1GDCaTyBquY76CUjd1mHA4EHafRxbdhSQs+ANbQd+aEew5TIwAj5ke69AfvAawAJDvxgKhy6tq3ldn7++Nb/+Vd+n538JtbettA8N7bTZT9X5t09kIfbFWIAF+NO6QfCKH9gyGRgRH/xX/JTFrLWjad+1q6E4Wmv386G27YhhU3RvMubuBKRmDfXeG0whCwAREREIDw+Hn58fevfujdWrV6OoqEg5ikFDqVzMwqU/kCeBmM/g3P0S/JNXgnJ5xdlRLgl5gIDHQMgwEPAYCHiA4NlzxbKKaQ3tlNPP2/HBYsfYeBTdfgrei94QTO2q/FAv/+kMZL+mwNrJCq+fnwsev+KnAUWBVPH8+f/V5jXxA5VcJseal7Yi/25+RZ9Ybztl7rLjtyD76QxsnG0w8+Z8Ze7GQt7XCWfT76Po2VlnxU/q/Ff8IHijD9isJ7A8cBV9+7Uxutw7DHSFjY0YRaEdIXjFTy13MIDlgatw9XflOlSj4O/qD2cbZ9zNv6uxnywDBs42zvB39ecgOkKIoTKoQnbSpEm4f/8+lixZgpycHHTr1g0HDhxQuwCsIQxwsMDx7GKwz4owiYyFpJq2PAaqhSGPgYABhMrnlQrIZ8s0tlObrihOhZWWCXgM+Ix+isMRb/phx4EMCKZ2Vf9QL5MhNKQtzEUG9SejFR6fh5A1IRW5P7vIp2pBExLS1ugKOaAid4+fxyK9uSXKv/tbeRZS9uMZgAEEr/eBR5gv5U7qjc/jY03IGoyPHw8GjEoxqzhrvjpkNV3oRYgBCAwMRLdu3bB69WquQzGsQhYA5s2b16BdCapzIqe44jDLygGGh64txejRylzjGU5eEz/jqK1H/d0g8LBTFnJAxYe6uJkYeL0PHjkab18mRe78LSmQmFjuLb1bwSvtAW7uv4rKly5ZHrgKjzBftPRuxVls+mbKuXNhrPdYbJ+4XWUcWQBoJm6Gn0b/RENvkTqRyWVIzkpGdkE2HK0d4e/qr9cvRDNmzMDPP/8MABAIBHB2dsaECRPwySefwMys4mJwxcmmU6dOoW/fvsp1JRIJnJ2d8ejRIxw5cgSBgYEAgGPHjiEqKgopKSkoLS1FmzZt0L9/f/zf//0fRCIRjh49isGDB2uMJzs7u9prhSrH/OTJE+zevbue2VfYuXNnk7lGwOAK2cZwIqcYydnF6G8vxuMzh9HcbwhO5klgI+Irux0YIznLotXpW7j74xm0f6E9pB2lGBg6EG0Ht8Wp+6WQsxz3p9AjOcvC39EC/VYMQUaIO47vP24yufs7WgKOlpBnzkfGkQyV3I39bKQp586Vsd5jMdpzNI5kHMEX+7/AoUeH0LZ5WypiSZ1UvcEGgEa5wUZISAhiYmIglUpx9uxZhIeHg2EYrFixQtnGxcUFMTExKoXsrl27YGVlhUePHinnXb58GSEhIXjrrbewdu1amJub49q1a9ixY4fazROuXLmiNuqSvb19g+UllUq1KlBbtGjRYPusLzpSV6EoYv0dLdDPvuKbVT97M/g7WiA5uxgncox33Mj+Lc3w8MO/AAB95vdB80HN4RbgBh6fhwEOFkZ9hyN/R0sMcLAAj8+DW4CbSeWuoCl3U2HKuXOBz+MjwC0A052mg8/wcS7nHK4/us51WMTAKG6wUXVYN8UNNnam7dTbvsViMRwcHODi4oIxY8YgKChIbcz58PBwxMbGoqSkRDkvJiYG4eHhKu0SEhLg4OCAlStXwtfXF+3atUNISAj+7//+TzlMlYK9vT0cHBxUHrUN57Vs2TL8/PPP+P333ysuCmYYHD16FJmZmWAYBnFxcQgICICZmRm2bNmChw8fYvLkyWjTpg0sLCzQuXNn/PbbbyrbDAwMxIIFC5TT7u7u+PzzzzFr1ixYW1vD1dUVP/zwgy4vaZ3R0boKxZm5qmdeK4oZC6M+M3cj8QZKn5TCysEKLgNduA6HEGLkmgmaYajHUABA3KU4jqMhXGNZFkVlRVo98kvz8fb+t6u9wQYAzN8/H/ml+Vptj63HZ/ulS5dw8uRJiEQilfk9e/aEu7s7duzYAQC4ffs2kpOTMW3aNJV2Dg4OyM7ORlJSUp1jqMm7776LiRMnIiQkBNnZ2cjOzkb//v2VyxctWoT58+cjLS0Nw4cPR2lpKXr27Im9e/fi0qVLePXVVzFt2jT8888/Ne7n66+/hp+fH/777z+88cYbeP3113HlyhW95FQZdS2ooqYzb8bcrQAAUuNSAQCdJnSiM1KEkEYxsdNEJGQkIDY1Fh8N+ojrcAiHiqXFsIpumLGrWbC4U3AHzVY006p9YWQhLEXa//K2Z88eWFlZoby8HBKJBDweD+vWrVNrN2vWLGzatAlTpkzBb7/9htDQUNjZ2am0mTBhAg4ePIiAgAA4ODigb9++GDp0KKZPn67WjcDZWfWW2W5ubkhNTa0xVisrK5ibm0MikWjsS7tgwQKMHavaDePdd99VPn/rrbdw8OBBxMfHo3fv3tXuZ8SIEXjjjTcAAB988AG++eYbHDlyBB06dKgxvvqiaoUAAMpLy5G+u+JWvz6TfDiOhhBiKl7s+CJEfBEu5V3CpbxLXIdDiFYGDx6MlJQU/P333wgPD8fMmTMxbtw4tXYvv/wyTp06hYyMDGzdulXjcKF8Ph8xMTG4c+cOVq5ciTZt2uDzzz+Hj48PsrNVbxCSnJyMlJQU5WPfvn31zsXPz09lWiaTYfny5ejcuTNatGgBKysrHDx4EFlZWTVup0uXLsrnDMPAwcFBeRtafaIzsgQAcP3AdZQVlMHG2QYu/VxQLivnOiRCiAmwNbNFaPtQ/H7ld8RdioPvEF+uQyIcsRBaoDCyUKu2SbeSMGLriFrb7ZuyD4PcBmm1b11YWlqiffv2AIBNmzaha9eu+OmnnzB79myVdi1btsQLL7yAOXPmQCKRIDQ0FEVFRRq32aZNG0ybNg3Tpk3D8uXL0bFjR3z//feIiopStvHw8ICtra1OsWqTS2Vffvkl1qxZg9WrV6Nz586wtLTEggULUFZWVuN2ql4kxjAM5HJ9382JzsiSZ5TdCiZ2qva+9IQQog9hvmEAgNjU2Hr1VSSGjWEYWIostXoMazcMzjbOKncIVNkWGLjYuGBYu2Faba8+Y7PzeDx8+OGH+Pjjj1Uu7FKYNWsWjh49ikmTJml9K9bmzZvD0dGx2qJXVyKRSG0EhOqcOHECo0ePxssvv4yuXbuibdu2uHr1aoPEoQ9UyBJIi6W48mdFh2zfSXQ2hBDSuEZ1HAULoQWuP7qOc9nnuA6HGADFDTYAqBWzXNxgY8KECeDz+Vi/fr3aspCQEOTm5uLDDz/UuO7GjRvx+uuvIyEhATdu3EBqaio++OADpKamYtSoUSpt8/LykJOTo/KQSqW1xufu7o4LFy7gypUrePDgQY3rdOjQAYmJiTh58iTS0tLw2muvITc3t9Z9cIUKWYKre69CWiSFrYctnHo5cR0OIaQekpKSMGrUKDg5OYFhGK0GQJdIJPjoo4/g5uYGsVgMd3d3bNq0Sf/BPmMpssQLHV8AAMReim20/RLDprjBRhubNirznW2csX3i9kYdm1ggEGDevHlYuXKl2llUhmHQqlUrtVENFHr37o3CwkLMnTsXPj4+CAgIwOnTp7F7924EBASotPX09ISjo6PK4+zZs7XGN2fOHHh6esLPzw92dnY4ceJEtW0//vhj9OjRA8OHD0dgYCAcHBwwZsyY2l8EjlAfWaLsVuAz0Ucvt74lhDSeoqIidO3aFbNmzVK7Erk6EydORG5uLn766Se0b98e2dnZjdK3rbIwnzDEp8YjLjUOK4JXgMfQeRZSO8UNNhrzzl6bN2/WOH/RokVYtGgRANTYRcbW1lZleffu3fHrr7/WuM/AwMB6dbuxs7NDQkKC2nxN22zRokWtX4CPHj2qMp2ZmanWJiUlBQD0fiyhQtbESQokuLb3GgAarYAQYxAaGorQ0FCt2x84cADHjh1DRkaG8m497u7ueoqueqEdQmEtssbt/Ns4dfsUBrgOaPQYiGHi8/gIdA/kOgzCESpkTdzVP6+ivLQcLTq0gEO3mu/VTAgxPn/88Qf8/PywcuVK/Prrr7C0tMSLL76I5cuXq91VSEEikUAikSin8/PzAVTc3lKb/nqKtpX/zwcfoz1H438X/4etF7ait2P141Uauqq5mxJFzuXl5WBZFnK5vNHP/nNBceZTkXNDqzrebGV79+6Fv79/g+9TW9XlLpfLwbIspFKp2kVwuvzboELWxCm7FUyibgWEmKKMjAwcP34cZmZm2LVrFx48eIA33ngDDx8+RExMjMZ1oqOjVYYEUkhISICFhW7DGFW+radHkQcAYMv5LRhaPhR8pnEu1OFK1VuampKTJ0/CwcEBhYWFtQ7rZEwKCgr0st2a7grm6Oio/LLJpaq5l5WVoaSkBElJSSgvVx3ys7i4WOvtUiFrwkqflOL6gYr7m9NoBYSYJrlcDoZhsGXLFjRrVnEXpFWrVmH8+PHYsGGDxrOykZGRiIiIUE7n5+fDxcUFw4YNq/HMUGVSqRSJiYkIDg5Wjj8ZLAvG+rXr8ajkESw7WWKIx5AGyLDp0ZS7qVDk3r9/f2RnZ8PKygpmZmZch6V3LMuioKAA1tbWejlp1K1btwbfZkOpLvfS0lKYm5tj0KBBan8DuhTeVMiasPTf0yErk8Gukx3sfe25DocQwgFHR0e0adNGWcQCgLe3N1iWxZ07dzTeXlIsFkMsFqvNFwqFOhdmldcRCoUY7z0eP5z7ATuu7MDwjsN1zMaw1OX1MhYCgQAMw4DH44HHM/4L+xQ/qStyNiXV5c7j8cAwjMZ/B7r8uzCtV5OoqNytgBBimgYMGIB79+6hsPD5HZWuXr0KHo+ndl/3xqC4OcKOtB0ok5nOT86EkLqhQtZEFT8sRkZiBgAqZAkxJoWFhcr7sAPAzZs3kZKSorxPemRkJKZPn65sP2XKFLRs2RIzZ87E5cuXkZSUhPfeew+zZs2q9mIvfRrkNggOVg54VPIIf2X81ej7J4QYFipkTVT6rnTIy+Vo3bU1Wnm24jocQkgDOXPmDLp3747u3bsDACIiItC9e3csWbIEAJCdna0sagHAysoKiYmJePLkCfz8/DB16lSMGjUKa9eu5SR+Po+PiZ0mAqCbIxBCakd9ZE0UdSsgxDjVNnC6psHcvby8mtQV9GG+YVj7z1rsTt+NEmkJzIWNf2aYEGIY6IysCSrKK8LNwzcB0GgFhJCmp69zX7g1c0NBWQH2X9/PdTiEEC0cPXoUDMPgyZMnjbpfKmRN0OUdl8HKWTj5OaF52+Zch0MIISoYhsEkn0kAqHsBaXpmzJgBhmGUV9x7eHjg/fffR2lpqbKNYvnp06dV1pVIJGjZsiUYhlG5zeuxY8cwZMgQtGjRAhYWFujQoQPCw8OVY+wqikRNj5ycnEbJu6miQtYEUbcCQkhTN8m3opDdc3UPCiT6GUSeGLbk7CKcyNE8cP6JnGIkZxfpbd8hISHIzs5GRkYGvvnmG2zcuBFLly5VaePi4qJ2U5Fdu3bByspKZd7ly5cREhICPz8/JCUl4eLFi/j2228hEokgk8lU2l65cgXZ2dkqD3t70x4+kwpZE1NwrwC3km4BAHwmUiFLCGmaujt0R4cWHVBSXoI/r/7JdTikCeIxDJKzi9WK2Yoithg8Pd6tUiwWw8HBAS4uLhgzZgyCgoLU+pmHh4cjNjYWJSUlynkxMTEIDw9XaZeQkAAHBwesXLkSvr6+aNeuHUJCQvB///d/aiOH2Nvbw8HBQeWh7bi0P/74I7y9vWFmZgYvLy9s2LBBuax///744IMPVNrfv38fQqFQedewX3/9FX5+frC2toaDgwOmTJmCvLw8rfatT1TImpjL2y8DLODczxnNXJvVvgIhhHCAYRjlmLLUvcA0sCyLMpn2j1525ujf2hzJ2cVIuleEMhmLpHtFSM4uRv/W5uhlZ671tmq6QLI2ly5dwsmTJyESiVTm9+zZE+7u7tixYwcA4Pbt20hOTsa0adNU2jk4OCA7O7vG28zW15YtW7BkyRJ89tlnSEtLw+eff47Fixfj559/BgBMnToVsbGxKq9DXFwcnJyc4O/vD6DirmzLly/H+fPnsXv3bmRmZmLGjBl6i1lbNGqBiaFuBYQQQxHmG4blSctx4PoBPC55jObm1KffmEnlwKoLD+u07sncEpzMLal2ujYRXVpCxNd+f3v27IGVlRXKy8shkUjA4/Gwbt06tXazZs3Cpk2bMGXKFPz2228IDQ2FnZ2dSpsJEybg4MGDCAgIgIODA/r27YuhQ4di+vTpard8rnqTEjc3N6SmptYa79KlS/H1119j7NixAAAPDw9cvnwZGzduRHh4OCZOnIgFCxbg+PHjysJ169atmDx5svK2srNmzVJur23btli7di169eqFwsJCte4SjYnOyJqQp7ef4vbJ2wADdBrfietwCCGkRp3sOqGzfWdI5VLsSt/FdTiEKA0ePBgpKSn4+++/ER4ejpkzZ2LcuHFq7V5++WWcOnUKGRkZ2Lp1K2bOnKnWhs/nIyYmBnfu3MHKlSvRpk0bfP755/Dx8UF2drZK2+TkZOUNT1JSUrBv375aYy0qKsKNGzcwe/ZsWFlZKR+ffvopbty4AQCws7PDsGHDsGXLFgAVN1I5deoUpk6dqtzO2bNnMWrUKLi6usLa2hoBAQEAoDIuNRfojKwJSY2v+NbmOtAVNm1samlNCCHcC/MNw8XDFxF7KRazus+qfQVisIS8ijOjujqdW4yTuSXgM4CMBfq3Nkff1hY671sXlpaWaN++PQBg06ZN6Nq1K3766SfMnj1bpV3Lli3xwgsvYM6cOZBIJAgNDUVRkeaL0Nq0aYNp06Zh2rRpWL58OTp27Ijvv/8eUVFRyjYeHh6wtbXVKVbF7af/7//+D3369FFZxuc/Pw09depUvP322/j222+xdetWdO7cGZ07dwZQUQwPHz4cw4cPx5YtW2BnZ4esrCwMHz5cObICVwzmjOxnn32G/v37w8LCQuc3kVSgbgWEEEOjGIbr0M1DyCvi/sISoj8Mw0DE1+3x7/2KLgT+jhZ4r1sr+Dta4GRuCf69X6LTdph6XBjG4/Hw4Ycf4uOPP1a5sEth1qxZOHr0KCZNmqRSONakefPmcHR0rLbo1UXr1q3h5OSEjIwMtG/fXuXh4eGhbDd69GiUlpbiwIED2Lp1q8rZ2PT0dDx8+BBffPEF/P394eXl1SQu9AIMqJAtKyvDhAkT8Prrr3MdikF6nPEY9/69B4bHULcCQojBaNeiHXo59YKclWPH5R1ch0OaEMXoBP6OFhjgUHEGdoCDBfwdLTSOZqBPEyZMAJ/Px/r169WWhYSEIDc3Fx9++KHGdTdu3IjXX38dCQkJuHHjBlJTU/HBBx8gNTUVo0aNUmmbl5eHnJwclYdUKq01vqioKERHR2Pt2rW4evUqLl68iJiYGKxatUrZxtLSEmPGjMHixYuRlpaGyZMnK5e5urpCJBLh22+/RUZGBv744w8sX75c25dHrwymkI2KisLChQuVp7mJbhTdCtwD3WHVmrtO2YQQoivl6AWpNHoBeU7OsipFrIKimJXXYyQCXQkEAsybNw8rV65UO4vKMAxatWqlNqqBQu/evVFYWIi5c+fCx8cHAQEBOH36NHbv3q3sh6rg6ekJR0dHlcfZs2drje+VV17Bjz/+iJiYGHTu3BkBAQHYvHmzyhlZoKJ7wfnz5+Hv7w9XV1flfDs7O2zevBnbtm1Dp06d8MUXX+Crr77S9uXRK+ojayKoWwEhxFBN9JmIdxLeQfKtZNzJvwNnG+faVyJGz9/RstplVYvbhrR582aN8xctWoRFixYBQI3Dedna2qos7969O3799dca9xkYGFivIcIAYMqUKZgyZUqNbUJDQ6vdz+TJk1XO0gKqeTZEjHVh1IWsRCKBRCJRTufn5wOoGAtNm1PxijbatG3KHl59iJyUHDB8Bu1HtTep3OuCcqfcdV2H6JezjTP8Xf2RnJWMbanbsLDfQq5DIoQ0EZwWsosWLcKKFStqbJOWlgYvL686bT86Olrlaj+FhIQEWFho/22t6t06DE1OfMV9mK26WOHoP0d1WtfQc68Pyt006ZJ7cXHj9cEzdWG+YUjOSkZsaiwVsoRUUdM4rvv371eODWuMOC1k33nnnVrvCtG2bds6bz8yMhIRERHK6fz8fLi4uGDYsGFqgwxrIpVKkZiYiODgYAiFwjrHwbX/++j/AACD3hiEriO6arWOseReF5Q75a5t7opfeYj+je80Hm/tfwv/3P0HGY8z0LZ53T8bCDE2KSkp1S5r06ZN4wXCAU4LWTs7O7U7XDQksVgMsVisNl8oFOr0Ia1r+6bk/uX7uJ96HzwhDz7jfHTOw5Bzry/KnXLXpi1pHPaW9hjiMQR/ZfyFuEtxiPSP5DokQpoMxZi2pshgRi3IyspCSkoKsrKyIJPJlHe1UAz0SzS7FHcJANB+eHuYNzfnOBpCCKm7MB8avYAQospgCtklS5age/fuWLp0KQoLC9G9e3d0794dZ86c4Tq0JotlWRqtgBBiNF7yfglCnhAXci/g8v3LXIdD6kFxAwIurnInTUNDvfcGU8hu3rwZLMuqPQIDA7kOrcnKvZCLh1cegi/mw/NFT67DIYSQemlh3gLD2w8HAMRdiuM4GlIfAkFFz0a6YNJ0Kd77+nbRMurht0yd4mxshxEdILZR7ytMCCGGJswnDHuu7kFsaiyWBS6r161FCXf4fD5sbW2Vtzm1sLAw6vdSLpejrKwMpaWl4PEM5hxig6iaO8uyKC4uRl5eHmxtbbW+bW91qJA1UtStgBBijF70fBFmAjNcfXgVKTkp6O7YneuQSB05ODgAgLKYNWYsy6KkpATm5uZGXbBrUl3utra2yr+B+qBC1khln83G44zHEFoI0fGFjlyHQwghDcJabI0XOr6A7Ze3I/ZSLBWyBoxhGDg6OsLe3t7oby4ilUqRlJSEQYMGmdxoJ5pyFwqF9T4Tq0CFrJFSjFbQ8YWOEFlqvr8zIYQYojCfMGy/vB1xqXH4IugLkzvDZWz4fH6DFTVNFZ/PR3l5OczMzEyukNV37qbVUcNEsCyLy/EVV/RStwJCiLEZ0WEErERWuPX0Fv6++zfX4RBCOESFrBG6c/oOnmY9hchKhPahpjtIMiHEOJkLzTHGawwAIPYSjSlLiCmjQtYIKS7y8hztCaG5af2EQQgxDYqbI8SnxkMml3EcDSGEK1TIGhlWzuLyNupWQIipSkpKwqhRo+Dk5ASGYbB7926t1z1x4gQEAgG6deumt/gaSnC7YDQ3a47swmwkZyVzHQ4hhCNUyBqZrONZKLhXAHEzMdoNa8d1OISQRlZUVISuXbti/fr1Oq335MkTTJ8+HUOHDtVTZA1LxBdhnPc4ANS9gBBTRoWskVGMVuD9kjcEYhqUghBTExoaik8//RQvvfSSTuvNnTsXU6ZMQb9+/fQUWcOb5DsJALD98nZIZcY9fBMhRDMqZI2IvFyOtO1pAKhbASFEezExMcjIyMDSpUu5DkUnge6BsLe0x8OShzh08xDX4RBCOECn7IxI5rFMFOUVwbylOTyGenAdDiHEAFy7dg2LFi1CcnIyBALtPhIkEgkkEolyOj8/H0DFwOfaDmyvaFffgfDHeY3Dd2e/w9YLWzHUzTC6RTRU7obIVHM31byBuuWuS1sqZI2IYrQC77He4AuNe3BpQkj9yWQyTJkyBVFRUejYUfs7AEZHRyMqKkptfkJCAiwsLHSKITExUaf2VbkUugAAdqTuwChmFEQ8w7kBTH1zN2Smmrup5g3olntxcbHWbamQNRIyqQxpO6hbASFEewUFBThz5gz+++8/zJs3DwAgl8vBsiwEAgESEhIwZMgQtfUiIyMRERGhnM7Pz4eLiwuGDRsGGxsbrfYtlUqRmJiI4ODget3tJ4QNwYZ1G3Cn4A6YDgxGeI6o87YaS0PlbohMNXdTzRuoW+6KX3m0QYWskbh56CZKHpXA0t4S7gHuXIdDCDEANjY2uHjxosq8DRs24PDhw9i+fTs8PDR3URKLxRCLxWrzhUKhzh/SdVmnqkm+k/D1qa+xPX07xvuOr9e2GlND5G6oTDV3U80b0C13XV4jKmSNhLJbwThv8AR0DR8hpqqwsBDXr19XTt+8eRMpKSlo0aIFXF1dERkZibt37+KXX34Bj8eDr6+vyvr29vYwMzNTm9+UhfmG4etTX+PPq3+iqKwIliJLrkMihDQSqniMQLmkHGm7qFsBIQQ4c+YMunfvju7duwMAIiIi0L17dyxZsgQAkJ2djaysLC5DbHA9HXuiXfN2KJYWY8/VPVyHQwhpRFTIGoEbCTcgeSqBlaMVXAe6ch0OIYRDgYGBYFlW7bF582YAwObNm3H06NFq11+2bBlSUlIaJdaGwjAMwnwrblkbm0o3RyDElFAhawQU3Qo6TegEHp/eUkKI6VEUsvuu7cPT0qccR0MIaSxU9Rg4aYkUV36/AgDwnWQ4fdoIIaQh+dr7wsfOB2WyMuxO3811OISQRkKFrIG7vv86ygrLYONiA+e+zlyHQwghnKHuBYSYHipkDZyiW4HPRB8wPIbjaAghhDuTfCYBABJvJOJB8QOOoyGENAYqZA1YWVEZru65CoBGKyCEkA4tO6CnY0/IWBl2XN7BdTiEkEZAhawBu7b3GqTFUjRv2xxOfk5ch0MIIZxTnJWl7gWEmAYqZA2YcrSCiZ3AMNStgBBCJvpMBAAcyzyGewX3OI6GEKJvVMgaKEmBBNf2XQNAoxUQQoiCm60b+rv0BwsW21K3cR0OIUTPqJA1UFf+uILy0nK07NgSrbu25jocQghpMsJ8aPQCQkwFFbIGSjlawSQf6lZACCGVTPCZAB7Dw+k7p3Hz8U2uwyGE6JFBFLKZmZmYPXs2PDw8YG5ujnbt2mHp0qUoKyvjOjROlD4pxfUD1wHQaAWEEFKVg5UDAt0DAQDxqfHcBkMI0SuDKGTT09Mhl8uxceNGpKam4ptvvsH333+PDz/8kOvQOJG+Ox1yqRx2Pnaw97HnOhxCCGlyqHsBIabBIArZkJAQxMTEYNiwYWjbti1efPFFvPvuu9i5cyfXoXGicrcCQggh6sZ6j4WAJ0BKTgquPLjCdTiEED0RcB1AXT19+hQtWrSosY1EIoFEIlFO5+fnAwCkUimkUmmt+1C00aZtYyl+WIyMvzIAAJ5jPfUWW1PMvbFQ7pS7ruuQpqelRUsMazcM+67tQ1xqHJYELOE6JEKIHhhkIXv9+nV8++23+Oqrr2psFx0djaioKLX5CQkJsLCw0Hp/iYmJOseoLw8THkJeLoe5hzn+vv43cF2/+2tKuTc2yt006ZJ7cXGxHiMh9RXmE4Z91/bht0u/YfGgxXRhLCFGiNNCdtGiRVixYkWNbdLS0uDl5aWcvnv3LkJCQjBhwgTMmTOnxnUjIyMRERGhnM7Pz4eLiwuGDRsGGxubWuOTSqVITExEcHAwhEJhre0bw9a1WwEAfWb3Qf8R/fW2n6aYe2Oh3Cl3bXNX/MpTH3l5ebC3r76ve3l5Oc6dO4fevXvXe1+mZrTXaIj5YqQ/SMfFvIvo0roL1yERQhoYp4XsO++8gxkzZtTYpm3btsrn9+7dw+DBg9G/f3/88MMPtW5fLBZDLBarzRcKhTp9SOvaXl8Kcwtx6+gtAECXyV0aJaamkjsXKHfKXZu29eXo6Ijs7GxlMdu5c2fs27cPLi4uAICHDx+iX79+kMlk9d6XqbER22Bkx5HYmbYTsZdiqZAlxAhxWsja2dnBzs5Oq7Z3797F4MGD0bNnT8TExIDHM4jr1BpU2o40sHIWTr2c0Lxtc67DIYQ0AJZlVaYzMzPV+t5WbUO0F+YTpixkPxvyGXUvIMTIGEQ1ePfuXQQGBsLV1RVfffUV7t+/j5ycHOTk5HAdWqOi0QoIMU1UfNXdyI4jYSm0xM0nN/HvvX+5DocQ0sAMopBNTEzE9evXcejQITg7O8PR0VH5MBUF9wpwK7miW4HPRCpkCSFEGxZCC7zo+SIAIPYSjSlLiLExiEJ2xowZYFlW48NUpG5LBVjApb8Lmrk04zocQkgDYRgGBQUFyM/Px9OnT8EwDAoLC5Gfn698kPoJ8624OUJcahzkrJzjaAghDckgh98yRdStgBDjxLIsOnbsqDLdvXt3lWnqWlA/w9sNRzNxM9wruIfjWccxyG0Q1yERQhoIFbIG4GnWU9w5dQdggE7jO3EdDiGkAR05coTrEIyeWCDGWO+xiEmJQeylWCpkCTEiVMgagNT4irOxboPcYO1kzXE0hJCGFBAQwHUIJiHMNwwxKTHYdnkb1oauhYBHH3+EGAOD6CNr6qhbASHGq7y8XOVW2gCQm5uLqKgovP/++zh+/DhHkRmXIR5D0MqiFR4UP8Dhm4e5DocQ0kCokG3iHt14hHtn7oHhMfAe6811OISQBjZnzhy8/fbbyumCggL06tUL69evx8GDBzF48GDs27ePwwiNg4AnwIROEwAAcZfiOI6GENJQqJBt4hTdCtwHu8OqtRXH0RBCGtqJEycwbtw45fQvv/wCmUyGa9eu4fz584iIiMCXX36p9faSkpIwatQoODk5gWEY7N69u8b2O3fuRHBwMOzs7GBjY4N+/frh4MGDdU2nSVOMXrAzfSck5ZJaWhNCDIFOhaxUKsWVK1eU06dOnWrwgIgq6lZAiHG7e/cuOnTooJw+dOgQxo0bh2bNKobZCw8PR2pqqtbbKyoqQteuXbF+/Xqt2iclJSE4OBj79u3D2bNnMXjwYIwaNQr//fefbokYgIGuA+Fk7YQnpU+QcCOB63AIIQ1Ap0I2PDwco0aNwocffggAeOedd/QSFKnw4MoD5J7PBU/Ao24FhBgpMzMzlJSUKKdPnz6NPn36qCwvLCzUenuhoaH49NNP8dJLL2nVfvXq1Xj//ffRq1cvdOjQAZ9//jk6dOiAP//8U/skDASP4WGSzyQAQGwq3RyBEGOgUyF76dIlXL16FUKhUOtv+6TuFGdj2wa1hUVLC46jIYToQ7du3fDrr78CAJKTk5Gbm4shQ4Yol9+4cQNOTk6NFo9cLkdBQQFatGjRaPtsTIruBb+n/45iaTHH0RBC6kun8UcUt4SNiorClClTcPPmTb0ERSpQtwJCjN+SJUsQGhqK+Ph4ZGdnY8aMGSq33961axcGDBjQaPF89dVXKCwsxMSJE6ttI5FIVEZaUNx9TCqVQiqVarUfRTtt2zeUbnbd4GHrgZtPbuL3tN8x3nt8o+4f4C73psBUczfVvIG65a5LW50K2QEDBqC8vBwCgQDff/89pk+frtampKQE5ubmumyWaJB3KQ/3L98HX8SH1xgvrsMhhOhJQEAAzp49i4SEBDg4OGDChAkqy7t164bevXs3Sixbt25FVFQUfv/9d9jb21fbLjo6GlFRUWrzExISYGGh269HiYmJOsdZXz1EPXATN7H28FpY3OTu1y4ucm8qTDV3U80b0C334mLtfy3RqZBdsmSJ8rmNjY3K1bASiQTr1q3Dl19+iZycHF02SzS4FHcJANBueDuY2ZpxHA0hRJ+8vb3h7a25H/yrr77aKDHExsbilVdewbZt2xAUFFRj28jISERERCin8/Pz4eLigmHDhsHGxkar/UmlUiQmJiI4OBhCobBeseuqTW4b7PhpB/4r/A8Dhw6EjVi7mBsKl7lzzVRzN9W8gbrlrviVRxs6FbJlZWVYunQpEhMTIRKJ8P7772PMmDGIiYnBRx99BD6fj4ULF+qySaIBy7K4HH8ZAHUrIMTYJSUladVu0CD93Vb1t99+w6xZsxAbG4uRI0fW2l4sFkMsFqvNFwqFOn9I12Wd+urZpie8Wnkh/UE69t3Yh2ldpzXq/hW4yL2pMNXcTTVvQLfcdXmNdCpkFy9ejI0bNyIoKAgnT57EhAkTMHPmTJw+fRqrVq3ChAkTwOfzddkk0SD3fC4eXn0IgZkAni96ch0OIUSPAgMDwTAMgIovsZowDAOZTKbV9goLC3H9+nXl9M2bN5GSkoIWLVrA1dUVkZGRuHv3Ln755RcAFd0JwsPDsWbNGvTp00f5i5q5ublyCDBjwzAMwnzCsOzYMsSmxnJWyBJC6k+nUQu2bduGX375Bdu3b0dCQgJkMhnKy8tx/vx5hIWFURHbQBTdCjqM6ACxtfpZD0KI8WjevDlcXFywePFiXLt2DY8fP1Z7PHr0SOvtnTlzBt27d0f37t0BABEREejevbuya1h2djaysrKU7X/44QeUl5fjzTffhKOjo/Ixf/78hk20iZnkWzEMV8KNBDwsfshxNISQutLpjOydO3fQs2dPAICvry/EYjEWLlyoPJtA6o9lWRqtgBATkp2djV27dmHTpk1YuXIlRowYgdmzZyMkJKROx9bAwMBqz+wCwObNm1Wmjx49qvM+jIFXKy90c+iGlJwU7EzbiTk953AdEiGkDnQ6IyuTySASiZTTAoEAVlZ029SGdO/MPTy5+QRCCyE6jOxQ+wqEEIMmEokwadIkHDx4EOnp6ejSpQvmzZsHFxcXfPTRRygvL+c6RKMV5lMxpizdHIEQw6XTGVmWZTFjxgxlJ//S0lLMnTsXlpaWKu127tzZcBGaGMXZ2I6jOkJkKaqlNSHEmLi6umLJkiWYNm0aZs+ejS+++ALvvPOO0d6cgGuTfCdh0aFFOJp5FDmFOXCwcuA6JEKIjnS+Ra29vT2aNWuGZs2a4eWXX4aTk5NyWvEgdcPKWaTGU7cCQkyRRCLB1q1bERQUBF9fX7Rq1Qp79+6lIlaP3G3d0de5L+SsHNsvb+c6HEJIHeh0RjYmJkZfcRAAd07fQf7tfIisRegQSt0KCDEF//zzD2JiYhAbGwt3d3fMnDkT8fHxVMA2kjCfMJy+cxqxl2Ixr/c8rsMhhOhIp0KW6JditAKv0V4QmNFbQ4gp6Nu3L1xdXfH2228rL6Y9fvy4WrsXX3yxsUMzCRN8JmDhwYU4cfsEsp5mwbWZK9chEUJ0QNVSEyGXyXF5G90EgRBTlJWVheXLl1e7XJdxZIlunKydEOAegKOZRxGfGo93+7/LdUiEEB3o1EeW6E/W8SwUZhfCzNYM7Ya14zocQkgjkcvltT4KCgq4DtOoKUcvuESjFxBiaKiQbSIUoxV4veQFvohuLEEIqbgAbNWqVWjbti3XoRi1cZ3Ggc/wcTb7LK49vMZ1OIQQHVAh2wTIy+W4vJ26FRBiiiQSCSIjI+Hn54f+/ftj9+7dAIBNmzbBw8MD33zzDRYuXMhtkEaulUUrBLUNAgDEpcZxHA0hRBdUyDYBmUczUXy/GOYtzeExxIPrcAghjWjJkiX47rvv4O7ujszMTEyYMAGvvvoqVq9ejVWrViEzMxMffPAB12EavTBf6l5AiCGiQrYJUIxW4D3OG3whdSsgxJRs27YNv/zyC7Zv346EhATIZDKUl5fj/PnzCAsLA59Px4TGMMZrDER8EVLvp+JS3iWuwyGEaMlgCtkXX3wRrq6uMDMzg6OjI6ZNm4Z79+5xHVa9yaQypO9MBwD4TvLlOBpCSGO7c+eOctgtX19fiMViLFy4EAzDcByZabE1s0Vo+1AAdFaWEENiMIXs4MGDER8fjytXrmDHjh24ceMGxo8fz3VY9ZbxVwZKHpXAsrUl3ALcuA6HENLIZDIZRKLnt6MWCASwsrLiMCLTVbl7AcuyHEdDCNGGwYwjW/liBzc3NyxatAhjxoyBVCqFUCjkMLL6UYxW0Gl8J/D4BvO9ghDSQFiWxYwZMyAWiwEApaWlmDt3LiwtLVXa7dy5k4vwTMqojqNgIbTAjcc3cDb7LPyc/LgOiRBSC4MpZCt79OgRtmzZgv79+xt0EVsuKUf67opuBTRaASGmKTw8XGX65Zdf5igSYimyxKiOoxCXGofYS7FUyBJiAAyqkP3ggw+wbt06FBcXo2/fvtizZ0+N7SUSCSQSiXI6Pz8fACCVSiGVSmvdn6KNNm3r4ureq5A8lcDKyQqOvR31tp+60HfuTRnlTrnruk59xMTE1HsbpOGE+YYhLjUO8anxWBm8EjyGfikjpCnjtJBdtGgRVqxYUWObtLQ0eHl5AQDee+89zJ49G7du3UJUVBSmT5+OPXv2VHtRRHR0NKKiotTmJyQkwMLCQus4ExMTtW6ri1trbgEAzHqYYf+B/XrZR33pK3dDQLmbJl1yLy4u1mMkhAsh7UNgI7bB7fzbOHX7FAa4DuA6JEJIDTgtZN955x3MmDGjxjaV72jTqlUrtGrVCh07doS3tzdcXFxw+vRp9OvXT+O6kZGRiIiIUE7n5+fDxcUFw4YNg42NTa3xSaVSJCYmIjg4uMG7MEhLpFjz8hoAwIj3R8C5r3ODbr++9Jl7U0e5U+7a5q74lYcYDzOBGV7yegk/n/8ZsZdiqZAlpInjtJC1s7ODnZ1dndaVy+UAoNJ1oCqxWKy8gKIyoVCo04e0ru21ce2PaygrLEMz12ZwH+jeZIfa0UfuhoJyp9y1aUuMT5hvGH4+/zPiL8fjm5BvIOAZVC88QkyKQXT++fvvv7Fu3TqkpKTg1q1bOHz4MCZPnox27dpVeza2qVOOVjCxU5MtYgkhxBQN9RiKluYtkVeUh2OZx7gOhxBSA4MoZC0sLLBz504MHToUnp6emD17Nrp06YJjx45pPOPa1JUVluHqnqsA6CYIhBDS1Aj5QozvVDFOOd0cgZCmzSB+L+ncuTMOHz7MdRgN5uqeqygvKUfzts3h2NOR63AIIYRUMclnEjae3YgdaTuwfuR6iPii2lcihDQ6gzgja2wU3Qp8JvlQtwJCCGmCBrkNgoOVAx6XPkbiDdMdxYOQpo4K2UYmyZfg2v5rAOgmCIQQ0lTxeXxM7DQRABCbSt0LCGmqqJBtZOm/p0MmkaGlZ0u07tKa63AIIYRUI8w3DACwO303SqQlHEdDCNGECtlGdjn+MgDqVkAIIU1dX+e+cGvmhsKyQuy7to/rcAghGlAh24hKHpfg+sHrAGi0AkIIaeoYhsEkn0kAqHsBIU0VFbKNKH13OuRSOex97WHXqW43giCEkJokJSVh1KhRcHJyAsMw2L17d63rHD16FD169IBYLEb79u2xefNmvcdpKBTdC/Zc3YMCSQHH0RBCqqJCthFVHq2AEEL0oaioCF27dsX69eu1an/z5k2MHDkSgwcPRkpKChYsWIBXXnkFBw8e1HOkhqGbQzd0bNkRpeWl+PPqn1yHQwipwiDGkTUGxQ+KkfFXBgAqZAkh+hMaGorQ0FCt23///ffw8PDA119/DQDw9vbG8ePH8c0332D48OH6CtNgMAyDMJ8wfJL0CWIvxWJK5ylch0QIqYQK2UaStjMNrIyFQ3cHtOzQkutwCCEEAHDq1CkEBQWpzBs+fDgWLFhQ7ToSiQQSiUQ5nZ+fDwCQSqWQSqVa7VfRTtv2XBrrORafJH2CA9cPIC8/D83Nm9dre4aUe0Mz1dxNNW+gbrnr0pYK2UZC3QoIIU1RTk4OWrdWHQqwdevWyM/PR0lJCczNzdXWiY6ORlRUlNr8hIQEWFhY6LT/xETDuNmAu5k7MkszsXz7cgS1DKp9BS0YSu76YKq5m2regG65FxcXa92WCtlGUJhbiMyjmQAAn4lUyBJCDFtkZCQiIiKU0/n5+XBxccGwYcNgY2Oj1TakUikSExMRHBwMoVCor1AbzEXbi1h8dDHSBelYNWJVvbZlaLk3JFPN3VTzBuqWu+JXHm1QIdsILm+/DFbOok3vNmjuUb+fpAghpCE5ODggNzdXZV5ubi5sbGw0no0FALFYDLFYrDZfKBTq/CFdl3W4MKXLFCw+uhiHMw/jcdlj2Fva13ubhpK7Pphq7qaaN6Bb7rq8RjRqQSOgbgWEkKaqX79+OHTokMq8xMRE9OvXj6OImqa2zduil1MvyFk5tl/eznU4hJBnqJDVs/y7+cg6ngUA6DShE8fREEKMXWFhIVJSUpCSkgKgYnitlJQUZGVVHIciIyMxffp0Zfu5c+ciIyMD77//PtLT07FhwwbEx8dj4cKFXITfpCnGlI29RDdHIKSpoEJWzy5vuwywgMsAFzRzacZ1OIQQI3fmzBl0794d3bt3BwBERESge/fuWLJkCQAgOztbWdQCgIeHB/bu3YvExER07doVX3/9NX788UcaekuDiT4TAQDJWcm4k3+H42gIIQD1kdU76lZACGlMgYGBYFm22uWa7toVGBiI//77T49RGQdnG2f4u/ojOSsZ8anxiOgXUftKhBC9ojOyevTk1hPcOX0HYIBO46lbASGEGDrqXkBI00KFrB6lxlecjXUPcIe1ozXH0RBCCKmv8Z3Gg8fw8O+9f3Hj0Q2uwyHE5FEhq0fUrYAQQoyLvaU9hnoMBQDEpcZxHA0hhApZPXl0/RGyz2aD4TPwHufNdTiEEEIaCHUvIKTpoIu99ETRrcBjiAcs7Sw5joYYAplMxvl9uKVSKQQCAUpLSyGTyTiNpbFpyl0oFILP53McGWlqXvJ6CXP3zMXFvIu4fP8yOtnRNRCEcIUKWT2hbgVEWyzLIicnB0+ePOE6FLAsCwcHB9y+fRsMw3AdTqOqLndbW1s4ODiY3OtBqtfcvDlC2ofgz6t/Iu5SHKIGR3EdEiEmiwpZPXiQ/gC5F3LBE/Dg/RJ1KyA1UxSx9vb2sLCw4LRgksvlKCwshJWVFXg80+p5VDV3lmVRXFyMvLw8AICjoyPHEZKmJMw3DH9e/ROxqbFYFriMvugQwhEqZPXgUtwlAEDb4LYwb6H5XuWEABXdCRRFbMuWLbkOB3K5HGVlZTAzMzPJQrZq7ubmFf9+8/LyYG9vT90MiNKLni/CXGCOqw+vIiUnBd0du3MdEiEmybQ+qRoBy7LUrYBoTdEn1sLCguNISHUU7w3X/ZdJ02IlssILHV8AQBd9EcIlKmQbWN6lPDxIewC+iA+vMV5ch0MMBP0s2XTRe0OqM8lnEgAgNjW2xrupEUL0hwrZBqY4G9s+pD3MmplxHA0hhBB9GdFhBKxEVsh6moXTd05zHQ4hJokK2QZE3QoIaVoCAwOxYMECrsMgRspcaI4xXmMAUPcCQrhicIWsRCJBt27dwDAMUlJSuA5HRc5/OXh0/REEZgJ0HNWR63CIiZHL5Mg8momLv11E5tFMyGVyve5vxowZYBgGDMNAKBTCw8MD77//PkpLSwEAUVFRGDZsGHx9fTF58mRIJBKttjlmzJgGi3Hnzp1Yvnx5g22PkKrCfCpujhB/OR4yuWmNvUxIU2Bwoxa8//77cHJywvnz57kORY1itIIOIztAbC3mOBpiStJ2puHA/APIv5OvnGfjbIOQNSHwHqu/IeBCQkIQExMDqVSKs2fPIjw8HAzDYMWKFYiMjIRIJAIAdOjQARkZGfD2bphYpFIphEJhre1atGjRIPsjpDrB7YLR3Kw5cgpzkHQrCYM9BnMdEiEmxaDOyO7fvx8JCQn46quvuA5FDcuyuBx/GQB1KyCNK21nGuLHx6sUsQCQfzcf8ePjkbYzTW/7FovFcHBwgIuLC8aMGYOgoCAkJiYCgLKIXbJkCcaOHVtrEbts2TL8/PPP+P3335Vneo8ePYrMzEwwDIO4uDgEBATAzMwMW7ZswcOHDzF58mS0adMGFhYW6Ny5M3777TeVbVbtWuDu7o7PP/8cs2bNgrW1NVxdXfHDDz807ItCTIqIL8I473EAqHsBIVwwmDOyubm5mDNnDnbv3q31UEUSiUTl58z8/IoPeqlUqtVQOoo22rS9+89dPMl8AqGlEB7DPAx+qB5dcjc2jZm7VCoFy7KQy+WQyyu6ArAsC2mxdvuWy+TY/9Z+QNMF0ywABtj/9n64DXEDj1/z91ahxfMznIqYasKyrEq7S5cu4eTJk3Bzc4NcLkd+fj5ef/119OvXD/Pmzat1exEREbh8+TLy8/OxadMmABVnVO/duwcAWLRoEb788kts2rQJZmZmKC4uRo8ePfDee+/BxsYG+/btw7Rp0+Dh4YHevXtXm8vXX3+NTz75BIsWLcKOHTvw+uuvw9/fHx07dtTYXi6XV7wnUqnaOLKm+O+DqAvzDcOP//2I7WnbsW7EOgj5tf9aQAhpGAZRyLIsixkzZmDu3Lnw8/NDZmamVutFR0cjKkr91oEJCQk6jdupOMNUk7ub7gIALHtYIvFo7e0NhTa5G6vGyF0gEMDBwQGFhYUoKysDAEiLpNjgvKFhdsACBXcL8GXzL2tt+sadNyC0rPgALigoqLW9VCrF3r17YWNjg/LyckgkEvB4PKxYsQL5+fmYMmUKzpw5g+vXr+PXX3/F8uXL0bdv3xq3KRAIwOfzlf8+S0tLUVhYCAB47bXXEBQUpNJ+zpw5yufTp0/H3r17sWXLFnh5VQx9V15ejrKyMuWXWLlcjqCgIEydOhUAMHfuXHzzzTfYv3+/8s5dVXMvKytDSUkJkpKSUF5errKsuLi41teJGL9A90C0tmyN3KJc/JXxF0I7hHIdEiEmg9NCdtGiRVixYkWNbdLS0pCQkICCggJERkbqtP3IyEhEREQop/Pz8+Hi4oJhw4bBxsam1vWlUikSExMRHBxcY388Vs5i3bx1AIChbw+F5whPneJsirTN3Rg1Zu6lpaW4ffs2rKysYGZWMVxbGb9Mr/usjrWNNYQWQhQUFMDa2rrW8VOFQiECAwOxYcMGFBUVYfXq1RAIBHj55ZcBAHv27NE5BqFQCIFAoPLv08rKCgAwYMAAlfkymQzR0dHYtm0b7t69i7KyMkgkEtjY2CjbCQQCiEQi5TSPx0PPnj1VtuPo6KjMWVPupaWlMDc3x6BBg5TvkYKiQCamjc/jY0KnCVj37zrEpcZRIUtII+K0kH3nnXcwY8aMGtu0bdsWhw8fxqlTpyAWq15A5efnh6lTp+Lnn3/WuK5YLFZbB6j4sNSlQKmtfdaJLBTcKYDYRgyvF7wgEBrEiW6t6PpaGZPGyF0mk4FhGPB4POVtUcVWYkQWavel7VbSLWwdsbXWdlP2TYHbILca2wgthMpB3RUx1YRhGFhZWSl/ko+JiUHXrl0RExOD2bNnaxW/pm1W3bfiubW1tcr8lStXYu3atVi9ejU6d+4MS0tLLFiwAFKpVKVd1e2JRCK15SzLKotXTftXjMxQ9e/BVP9tEHVhvmFY9+867Erfhe/Lv4eZgMYRJ6QxcFpx2dnZwc7OrtZ2a9euxaeffqqcvnfvHoYPH464uDj06dNHnyFqRTF2rOdoTwjMjKeIJdxgGAYiS5FWbdsNawcbZxvk383X3E+WqRi9oN2wdrX2kQVQr7sT8Xg8fPjhh4iIiMCUKVNgbm6u8zZEIhFkMu2GMDpx4gRGjx6tPAMsl8tx9epVdOrUSef9ElJf/Vz6wcXGBbfzb+PA9QPK8WUJIfplEKMWuLq6wtfXV/lQnAFq164dnJ2dOY1NLpPj8jYarYBwg8fnIWRNSMVE1Z4Az6ZDVodoVcQ2hAkTJoDP52P9+vV1Wt/d3R0XLlzAlStX8ODBgxovpurQoQMSExNx8uRJpKWl4bXXXkNubm5dQyekXngM7/kta2n0AkIajUEUsk1ZVnIWCnMKYdbcDO2C23EdDjFB3mO9MXH7RNi0Ue33beNsg4nbJ+p1HNmqBAIB5s2bh5UrV6KoqEjn9efMmQNPT0/4+fnBzs4OJ06cqLbtxx9/jB49emD48OEIDAyEg4NDg95MgRBdhflW3Bzhz6t/oqhM979/QojuDPJ3cHd393r9BNqQFDdB8HrJC3wRv5bWhOiH91hveI72RFZyFgqyC2DtaA1Xf1e9nondvHmzxvmLFi3CokWL6rRNOzs7JCQkqM3X9O+9RYsW2L17d43bO3r0qMq0phFPFHcIrG14MEJq08OxB9o1b4cbj2/gz6t/KgtbQoj+0BnZepCXy5G2o2Kwed9JvhxHQ0wdj8+De6A7Ok/uDPdA90brTkAIqcAwjLJ4pe4FhDQO+qSrh5tHbqL4fjEsWlnAY4gH1+EQ0uRZWVlV+0hOTuY6PELqTVHI7r++H09Kn3AbDCEmwCC7FjQVitEKvMd5gyeg7wSE1EbxM74mbdq0abxACNETX3tf+Nj5IPV+Knan78aMbjO4DokQo0bVVx3JymTKe9jTaAWEaKd9+/bVPuoyXBfRbP369XB3d4eZmRn69OmDf/75p8b2q1evhqenJ8zNzeHi4oKFCxeitLS0kaI1PtS9gJDGQ4VsHWX8lYHSx6WwcrCqdaB5QghpLHFxcYiIiMDSpUtx7tw5dO3aFcOHD0deXp7G9lu3bsWiRYuwdOlSpKWl4aeffkJcXBw+/PDDRo7ceCiG4for4y/cL7rPcTSEGDcqZOtI2a1gvDddVEMIaTJWrVqFOXPmYObMmejUqRO+//57WFhYYNOmTRrbnzx5EgMGDMCUKVPg7u6OYcOGYfLkybWexSXV69CyA3o69oSMlWFH2g6uwyHEqFEFVgflpeVI350OgEYrIIQ0HWVlZTh79iyCgoKU83g8HoKCgnDq1CmN6/Tv3x9nz55VFq4ZGRnYt28fRowY0SgxGyvqXkBI46CLverg+sHrkORLYN3GGi79XbgOhxBCAAAPHjyATCZD69atVea3bt0a6enpGteZMmUKHjx4gIEDB4JlWZSXl2Pu3Lk1di2QSCSQSCTK6fz8fACAVCqt8W5slSnaadve0LzU8SW8l/gekm4l4dajW3CydlIuM/bca2KquZtq3kDdctelLRWydaDoVuAz0QcMr+p9QQkhxHAcPXoUn3/+OTZs2IA+ffrg+vXrmD9/PpYvX47FixdrXCc6OhpRUVFq8xMSEmBhYaHT/hMTE+sUtyHwtvRGWlEalu9cjlF2o9SWG3PutTHV3E01b0C33IuLi7VuS4WsjqTFUlz54woAGq2AEEN09OhRDB48GI8fP4atrS3X4TSoVq1agc/nIzc3V2V+bm4uHBwcNK6zePFiTJs2Da+88goAoHPnzigqKsKrr76Kjz76CDyeeg+0yMhIREREKKfz8/Ph4uKCYcOGwcbGRq29JlKpFImJiQgODoZQKNQ2RYOSeSYTCxIW4BIu4bsR3ynnm0Lu1THV3E01b6BuuSt+5dEGFbI6urbvGqRFUti626JNbxr3kpiuGTNm4OeffwYACAQCODs7Y8KECfjkk09gZmaGqKgonDhxAvfu3UPnzp2xefNmiMVijqM2biKRCD179sShQ4cwZswYABW33j106BDmzZuncZ3i4mK1YpXPr7jddnW3AheLxRrfS6FQqPOHdF3WMRSTOk9CRGIE/r77N+4U3oFHc9Ub5xhz7rUx1dxNNW9At9x1eY2okNWRoltBp4mdwDDUrYBwLzm7CDyGwQAH9Z90T+QUQ86y8He01Mu+Q0JCEBMTA6lUirNnzyI8PBwMw2DFihWIjIyESCQCAHTo0AEZGRnw9vbWSxzkuYiICISHh8PPzw+9e/fG6tWrUVRUhJkzZwIApk+fjjZt2iA6OhoAMGrUKKxatQrdu3dXdi1YvHgxRo0apSxoSd04WDlgsPtgHLp5CPGp8fhg4Adch0SI0aFRC3RQVliGq3uvAqjoH0tIU8BjGCRnF+NEjmqfohM5xUjOLgZPj1+4xGIxHBwc4OLigjFjxiAoKEjZD0pRxC5ZsgRjx47Vuoj98ccf4e3tDTMzM3h5eWHDhg3KZf3798cHH6gWA/fv34dQKERSUhIA4Ndff4Wfnx+sra3h4OCAKVOmVDuGqjGaNGkSvvrqKyxZsgTdunVDSkoKDhw4oLwALCsrC9nZ2cr2H3/8Md555x18/PHH6NSpE2bPno3hw4dj48aNXKVgVJSjF6TS6AWE6AMVsjq48ucVlJeUo3m75nDs4ch1OMRIsSyLMpn2j1525ujf2hzJ2cVIuleEMhmLpHtFSM4uRv/W5uhlZ67Vdqr7GVlbly5dwsmTJ5UFbH5+PqZMmQI7OzusWLFCq21s2bIFS5YswWeffYa0tDR8/vnnWLx4sbILw9SpUxEbG6sSa1xcHJycnODv7w+goj/W8uXLcf78eezevRuZmZmYMWNGvXIzNPPmzcOtW7cgkUjw999/o0+fPsplR48exebNm5XTAoEAS5cuxfXr11FSUoKsrCysX7/e6PoPc2Ws91gIeAKk5KQg/YHmkSMIIXVHXQt0oBytYJIPdSsgeiOVA6suPKzTuidzS3Ayt6Ta6ZpEdGkJgY5/1nv27IGVlRXKy8shkUjA4/Gwbt06AMC0adNw+vRpZGRkYMuWLfj6668xYMCAGre3dOlSfP311xg7diwAwMPDA5cvX8bGjRsRHh6OiRMnYsGCBTh+/LiycN26dSsmT56s/Dc5a9Ys5fbatm2LtWvXolevXigsLISVlZVuCRJSTy3MW2BYu2HYd20f4i7FYWngUq5DIsSoUCGrpdKnpbi+/zoAugkCIQqDBw/Gd999h6KiInzzzTcQCAQYN24cAOD333/XaVtFRUW4ceMGZs+ejTlz5ijnl5eXo1mzZgAAOzs7DBs2DFu2bIG/vz9u3ryJU6dOqfwMfvbsWSxbtgznz5/H48ePIZfLAVT8pN6pU6f6pkyIzsJ8wrDv2j7EpsZiScASrsMhxKhQIaulK79fgaxMhlZerWDf2Z7rcIgRE/Iqzo7q6nRuMU7mloDPADIW6N/aHH1baz+mp5AH6Nq7wNLSEu3btwcAbNq0CV27dsVPP/2E2bNn67YhAIWFhQCA//u//1P5KRyAykVHU6dOxdtvv41vv/0WW7duRefOndG5c2cAFcXw8OHDMXz4cGzZsgV2dnbIysrC8OHDUVZWpnNMhDSE0V6jIeaLkf4gHRdyL6BTS/pCRUhDoT6yWqJuBaSxMAwDEV+3x7/3K7oQ+Dta4L1ureDvaIGTuSX4936J1tuo7981j8fDhx9+iI8//hglJdp1Z6isdevWcHJyQkZGBtq3b6/y8PB4PmzR6NGjUVpaigMHDmDr1q2YOnWqcll6ejoePnyIL774Av7+/vDy8jKpC72auuTsIrWLEhUqLk4sauSIGoeN2AYjO44EQLesJaShUSGrhZJHJbiRcAMA3QSBND2K0Qn8HS2UQ3ANcLCAv6OFxtEM9GnChAng8/lYv359ndaPiopCdHQ01q5di6tXr+LixYuIiYnBqlWrlG0sLS0xZswYLF68GGlpaZg8ebJymaurK0QiEb799ltkZGTgjz/+wPLly+udF2kYXI6wwbUwn+ejF9T3wkpCyHNUyGrhyu9XIC+Xw76zPey87bgOhxAVFePEWqiNI6soZuWN+KEpEAgwb948rFy5EkVFup9de+WVV/Djjz8iJiYGnTt3RkBAADZv3qxyRhao6F5w/vx5+Pv7w9XVVTnfzs4OmzdvxrZt29CpUyd88cUX+Oqrr+qdF2kYlb9gncorBQCcyitV+yJmjEZ2HAlLoSUyn2Ri47mNSHqchGO3jkEml3EdWqORyWU4duuYSeZuqhrjPWdYE/pqmJ+fj2bNmuHp06da3UZRKpVi3759eLruKW7+dRODPx2MQR8NaoRIuafIfcSIESZ3F5LGzL20tBQ3b96Eh4cHzMzM9LovbcjlcuTn58PGxkbjrUmNWXW51/Qe6XpMMVa6vg5/ZhYg9bGkolM2w6ClGR9OFgKI+cyzBw9iXsVzkWIe7/lzEa/+XWG4MChmEJKzklXmOds4Y03IGoz1HstRVI1jZ9pOzD8wH3fy7yjnmUruMrkMRzKOYP/x/QgdGIrBbQeDzzP+m43U5z3X5ZhCF3tVQy6T49axW3hw4AHuHK54E2i0AkIIqT9Xa2FFIfusGH1YKsPDUt3O1NRU6Ir5PGXBK660/Hn7iuUCBo1WEP9w4SyEZr0AqBayd/PvYn3KSTyQuuHVLj0bJZbG9sOFs4hLPalS0ACmkXvVYm7VrVUmUcDvTNuJ8fHjwUL1XOnd/LsYHz8e2ydub7D8qZDVIG1nGg7MP4D8O/nKeTwhD7kXctGifQsOIyPEsNU0juv+/fuVY8MS4/ZIUbSycoDhoZ2NEG0shSiTsZDIWUhkLCQyOSQyFmXK6YqH4mNRIq9oC2nd4+ABKmd5xTUVwirTPJXiWcCruRiWyWXYf30vgr0iAQCHrz7v8z2440IEey3C3mvrEOhkDSFfCD6PDx7DUz74TJXpSssrL2uKZ6mf574IAKsx9/3XN2C2bzejO0tpqgW8TC7D/9IvY3DHhSrvNwCwYDGk4zvYkp6G0Z6yBnnPqZCtIm1nGuLHx6PKlwjIpXLEj4/HxO0T4T2W7hdPSF2kpKRUu6xNmzaNFwjhzImcYvydV4L+9mI8PnMYzf2G4GSeBE6WQgS2saxxXZZlUc6iosBVFLuVCt3KhXDV5WWKYvjZNADIAZTKWJTK6tfDjs9ArdAVVToLnFOYhUclD5CWcxDBXpFoZdkeKXe2o0ubl9DTNQxnsrbiYnYiQmITwbJysJBX/P/Zczkrf3aBGKu2rOL/LOSsHAALHiqKWQYVJ7wVF9AxDMADAx7DVFrGUz5nwIDP8JTzqxbMVYtmbYvtB8UPkHQrCfmSpyqF/JCOEQj2ikRiejQOX12FMaX/oo1Nm2cxMcrCXDFd+XnVZYrpmpY19nbkrBx/XDlXYwH/59W1sBfeBo/HU77HLFjl/6vOkz97r7VdXpd1GiKOW09v4anMTeMXt4r3fRES06ORnJWMQPfAev3bA6iQVSGXyXFg/gG1IrayAwsOwHO0J3h80+o/SEhDUIw5S0xT5RE2ercUYh+AfvZm4PP5SM6uGMmgpgu+GIaBkAGEPAYQAkDdzuawbMWZXvXCV/WMcHXLFc/L5BUfFjIWKC5nUaz88KjaTcIeoZ2e3wihu8sEdHeZoJz2c50CP9cpdcpFX+SsTEPBDI2FdkURU/n/z5d5sCz82lVMF0oeINgrEkGeH4BheHhacg+e9kHoYDcEcrYc5awMLCuDnJVDzpZXxCCv9LzqMo3TZTq0lak9r1tbzdMsK4NULkewVyR4jBBHrq5CYIf5CPL6QFnA70nl+p3Wr5q+vLzi69cg+6BCtpKs5CyV7gRqWCD/dj6ykrPgHujeaHERQogxqDzChlT6vE+AonhtrBE2GEZx5hSwrsd25OzzYrisUveHsirdIzKe3Maea4kwE1rDTGCNdq38wTA8sKwcOfmXwTA8MODBzdYdFkLLZ2e2Ks6pVBSPUJ3HslWWVUyweD4fldYHdO9ywGP4AKOfn/oZpuJEUDNzJzQzd9LLPpqaoZ7vYqjnu8rpYK9IBHtFQs4+72ajePcUZ9+V7yhb+Z1VXV65zbMNqbQF+/yvoPJfBAsWjOL5szZMlTaqZ/Uqt3m+jAFb8efFAmAqlheVFSHzSQZYsLj75CKCvSIx1PN98Bi+sogFAEdrxzq+mqqokK2kILugQdsRoi0TGjzE4NB703D8HavvOmCIQ2/xGAZmAga1jTcywKEDFh8Mxt38uxjccSHa2wWgXCaBgC/Gpew/ceTqN3C2ccbN+Tf10k9U8TdcufBVKZCfLVRbrlI8o8o2WNX2bOXy5/l6Mrkc47eNx4Oih+juMgm93KZCJpeCzxPiv9vbcCn7T9hZ2GPDyO8A8CB/tq782Tblz4p2eaXniuVy9vl+5FXaspWWa9zWs9dFXmn9ip/Ja99+1eWatq/tUYOn+LKgpy8NXBCJgebWqmdbeQwfMnk5Dl9dBQYMnG2c4e/aMNdEGEwh6+7ujlu3bqnMi46OxqJFixpsH9aO2n0317YdIbVRDO9VXFwMc3NzjqMhmhQXV/zkbWrD0JGGw+fxsSZkDdannFT2D6z8UyvA4M1u/fV2sZPiIjBG+R+VpXrZZ2WL+s3E+pST6OU2VS33B0U38Ga3/ujUwriOf+WycrRd2w7ZBTkI7LgQQz3fVX55OXp1NU5kbISTtRP+mXOmYri/Kl8wAPUvDVXbKL84VDlDr826yjZVzv5D7XmVLyzKfUKxZQ1fjoCLuRfw8/lf0d4uAB3tB0MuLwefJ8CQju/gyNVVWB2yusH+3g2mkAWATz75BHPmzFFOW1s3bEHp6u8KG2cb5N/N1/x1igFsnG3g6u+qYSEhuuPz+bC1tVXeRtXCwoLTK4/lcjnKyspQWlpqkuPIVs6dZVkUFxcjLy8Ptra24PON54wJaXytm4cg2GsQ/r65QfnT6uGrq2AjboZgr0Vo3dzwzkhryxRzF/AFWB3yDdannMRQz3fVCnipvBRvduuPZmLj/ILc274P5ODjYbm7Wu5hvmEY690w/WMBAytkra2t4eDgoLft8/g8hKwJqRi1QNHnQ+FZbRGyOoQu9CINSvE3rShmucSyLEpKSmBubt4kh/LRp+pyt7W11etxh5gGRf/g97p+hCNd+qsMjn86T9Kod+BrbKaauykW8AoncorxsNwdAxzM4Gcdim4SILRXKMSWZjiR444TOcUN1p3IoArZL774AsuXL4erqyumTJmChQsXQiBo2BS8x3pj4vaJauPI2jjbIGR1CA29RRocwzBwdHSEvb29ygUwXJBKpUhKSsKgQYNM7qd0TbkLhUI6E0saROX+wQFuAShKLUKAWwD4PL5B9g/WhanmbqoFPFD1ws7n77lQKHw2PFnD5W4whezbb7+NHj16oEWLFjh58iQiIyORnZ2NVatWVbuORCKBRCJRTufnVxSmUqm0xoKh/aj2eGPEG7h59CZOJ55G3+C+8Aj0AI/P47zQaCyKPE0l38q4zJ3rokkul6O8vBx8Pp/zWBqbptzlcjnkcnm165jivw9CiHZMtYAHGvfCTk4L2UWLFmHFihU1tklLS4OXlxciIiKU87p06QKRSITXXnsN0dHREIvFGteNjo5GVFSU2vyEhARYWGj3QjYf1BxXJFdw5eAVrdobm8TERK5D4Azlbpp0yV1xIRghhBBucFrIvvPOO5gxY0aNbdq2batxfp8+fVBeXo7MzEx4enpqbBMZGalSAOfn58PFxQXDhg2DjY1NrfFJpVIkJiYiODjYJH9mpdwpd1NSl9wVv/IQQgjhBqeFrJ2dHezs7Oq0bkpKCng8Huzt7attIxaLNZ6tFQqFOn1I69remFDulLup0SV3U32NCCGkqTCIPrKnTp3C33//jcGDB8Pa2hqnTp3CwoUL8fLLL6N58+Zab0cxKLS2Z1GkUimKi4uRn59vch9YlDvlTrnXTnEsMfWbJuh6bAXob41yN63cTTVvQP/HVoMoZMViMWJjY7Fs2TJIJBJ4eHhg4cKFKt0GtFFQUHFHLhcXF32ESQgxUQUFBWjWrBnXYXCGjq2EEH3Q5tjKsCZ0KkEul+PevXuwtrbWaoxMRZ/a27dva9Wn1phQ7pQ75V47lmVRUFAAJycnk7uBRGW6HlsB+luj3E0rd1PNG9D/sdUgzsg2FB6PB2dnZ53Xs7GxMbk/PAXKnXI3NbrmbspnYhXqemwF6G+Ncjctppo3oL9jq+meQiCEEEIIIQaNCllCCCGEEGKQqJCtgVgsxtKlS6u94YIxo9wpd1NjyrlzwZRfb8rd9HI31bwB/eduUhd7EUIIIYQQ40FnZAkhhBBCiEGiQpYQQgghhBgkKmQJIYQQQohBokK2BuvXr4e7uzvMzMzQp08f/PPPP1yHpHdJSUkYNWoUnJycwDAMdu/ezXVIjSY6Ohq9evWCtbU17O3tMWbMGFy5coXrsBrFd999hy5duijH+evXrx/279/PdViN7osvvgDDMFiwYAHXoRg1Uzy2AqZ7fKVjKx1bAf0dX6mQrUZcXBwiIiKwdOlSnDt3Dl27dsXw4cORl5fHdWh6VVRUhK5du2L9+vVch9Lojh07hjfffBOnT59GYmIipFIphg0bhqKiIq5D0ztnZ2d88cUXOHv2LM6cOYMhQ4Zg9OjRSE1N5Tq0RvPvv/9i48aN6NKlC9ehGDVTPbYCpnt8pWOraR9bAT0fX1miUe/evdk333xTOS2TyVgnJyc2Ojqaw6gaFwB2165dXIfBmby8PBYAe+zYMa5D4UTz5s3ZH3/8keswGkVBQQHboUMHNjExkQ0ICGDnz5/PdUhGi46tFUz5+ErHVtM5trKs/o+vdEZWg7KyMpw9exZBQUHKeTweD0FBQTh16hSHkZHG9PTpUwBAixYtOI6kcclkMsTGxqKoqAj9+vXjOpxG8eabb2LkyJEq/+ZJw6NjKwHo2GpKx1ZA/8dXgV62auAePHgAmUyG1q1bq8xv3bo10tPTOYqKNCa5XI4FCxZgwIAB8PX15TqcRnHx4kX069cPpaWlsLKywq5du9CpUyeuw9K72NhYnDt3Dv/++y/XoRg9OrYSOraazrEVaJzjKxWyhGjw5ptv4tKlSzh+/DjXoTQaT09PpKSk4OnTp9i+fTvCw8Nx7Ngxoz7g3r59G/Pnz0diYiLMzMy4DocQo0fHVtM4tgKNd3ylQlaDVq1agc/nIzc3V2V+bm4uHBwcOIqKNJZ58+Zhz549SEpKgrOzM9fhNBqRSIT27dsDAHr27Il///0Xa9aswcaNGzmOTH/Onj2LvLw89OjRQzlPJpMhKSkJ69atg0QiAZ/P5zBC40LHVtNGx1bTObYCjXd8pT6yGohEIvTs2ROHDh1SzpPL5Th06JBJ9WsxNSzLYt68edi1axcOHz4MDw8PrkPilFwuh0Qi4ToMvRo6dCguXryIlJQU5cPPzw9Tp05FSkoKFbENjI6tpomOrapM4dgKNN7xlc7IViMiIgLh4eHw8/ND7969sXr1ahQVFWHmzJlch6ZXhYWFuH79unL65s2bSElJQYsWLeDq6sphZPr35ptvYuvWrfj9999hbW2NnJwcAECzZs1gbm7OcXT6FRkZidDQULi6uqKgoABbt27F0aNHcfDgQa5D0ytra2u1fnqWlpZo2bKlyfTfa2ymemwFTPf4SsdW0zu2Ao14fG3QMRCMzLfffsu6urqyIpGI7d27N3v69GmuQ9K7I0eOsADUHuHh4VyHpnea8gbAxsTEcB2a3s2aNYt1c3NjRSIRa2dnxw4dOpRNSEjgOixO0PBb+meKx1aWNd3jKx1b6diqoI/jK8OyLNtwZTEhhBBCCCGNg/rIEkIIIYQQg0SFLCGEEEIIMUhUyBJCCCGEEINEhSwhhBBCCDFIVMgSQgghhBCDRIUsIYQQQggxSFTIEkIIIYQQg0SFLCGEEEIIMUhUyBJSDwzDYPfu3VyHQQghRoWOrURbVMgSgzVjxgwwDKP2CAkJ4To0QggxWHRsJYZEwHUAhNRHSEgIYmJiVOaJxWKOoiGEEONAx1ZiKOiMLDFoYrEYDg4OKo/mzZsDqPhp6rvvvkNoaCjMzc3Rtm1bbN++XWX9ixcvYsiQITA3N0fLli3x6quvorCwUKXNpk2b4OPjA7FYDEdHR8ybN09l+YMHD/DSSy/BwsICHTp0wB9//KFc9vjxY0ydOhV2dnYwNzdHhw4d1D4cCCGkqaFjKzEUVMgSo7Z48WKMGzcO58+fx9SpUxEWFoa0tDQAQFFREYYPH47mzZvj33//xbZt2/DXX3+pHEy/++47vPnmm3j11Vdx8eJF/PHHH2jfvr3KPqKiojBx4kRcuHABI0aMwNSpU/Ho0SPl/i9fvoz9+/cjLS0N3333HVq1atV4LwAhhOgBHVtJk8ESYqDCw8NZPp/PWlpaqjw+++wzlmVZFgA7d+5clXX69OnDvv766yzLsuwPP/zANm/enC0sLFQu37t3L8vj8dicnByWZVnWycmJ/eijj6qNAQD78ccfK6cLCwtZAOz+/ftZlmXZUaNGsTNnzmyYhAkhpBHQsZUYEuojSwza4MGD8d1336nMa9GihfJ5v379VJb169cPKSkpAIC0tDR07doVlpaWyuUDBgyAXC7HlStXwDAM7t27h6FDh9YYQ5cuXZTPLS0tYWNjg7y8PADA66+/jnHjxuHcuXMYNmwYxowZg/79+9cpV0IIaSx0bCWGggpZYtAsLS3Vfo5qKObm5lq1EwqFKtMMw0AulwMAQkNDcevWLezbtw+JiYkYOnQo3nzzTXz11VcNHi8hhDQUOrYSQ0F9ZIlRO336tNq0t7c3AMDb2xvnz59HUVGRcvmJEyfA4/Hg6ekJa2truLu749ChQ/WKwc7ODuHh4fjf//6H1atX44cffqjX9gghhGt0bCVNBZ2RJQZNIpEgJydHZZ5AIFB2+t+2bRv8/PwwcOBAbNmyBf/88w9++uknAMDUqVOxdOlShIeHY9myZbh//z7eeustTJs2Da1btwYALFu2DHPnzoW9vT1CQ0NRUFCAEydO4K233tIqviVLlqBnz57w8fGBRCLBnj17lAd7QghpqujYSgwFFbLEoB04cACOjo4q8zw9PZGeng6g4qrX2NhYvPHGG3B0dMRvv/2GTp06AQAsLCxw8OBBzJ8/H7169YKFhQXGjRuHVatWKbcVHh6O0tJSfPPNN3j33XfRqlUrjB8/Xuv4RCIRIiMjkZmZCXNzc/j7+yM2NrYBMieEEP2hYysxFAzLsizXQRCiDwzDYNeuXRgzZgzXoRBCiNGgYytpSqiPLCGEEEIIMUhUyBJCCCGEEINEXQsIIYQQQohBojOyhBBCCCHEIFEhSwghhBBCDBIVsoQQQgghxCBRIUsIIYQQQgwSFbKEEEIIIcQgUSFLCCGEEEIMEhWyhBBCCCHEIFEhSwghhBBCDBIVsoQQQgghxCD9PxuvFsJDqnpoAAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 700x300 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# modified plot to replace to previous one\n",
        "\n",
        "plot_training_results(r2list, rmselist, r2list_eval, rmselist_eval)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zY1fRf9Oed5X"
      },
      "source": [
        "# 3. Transfer learning on Argentina corn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyq2KLsWekez"
      },
      "source": [
        "# 4. Adding new fearture to improve the Argentina lstm model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "i3Q-9683LMh0",
        "outputId": "f24af7bf-135c-4df3-8d30-c258e12806cf"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 1034,\n  \"fields\": [\n    {\n      \"column\": \"Region1\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 127,\n        \"samples\": [\n          \"san andres de giles\",\n          \"bragado\",\n          \"coronel suarez\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Region2\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"la pampa\",\n          \"santa fe\",\n          \"santiago del estero\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Year\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 12,\n        \"samples\": [\n          \"2006\",\n          \"2010\",\n          \"2007\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-bbb4d74a-9bc2-4af1-a08c-a34ee01f4e4b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Region1</th>\n",
              "      <th>Region2</th>\n",
              "      <th>Year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9 de julio</td>\n",
              "      <td>chaco</td>\n",
              "      <td>2007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>constitucion</td>\n",
              "      <td>santa fe</td>\n",
              "      <td>2014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>gualeguay</td>\n",
              "      <td>entre rios</td>\n",
              "      <td>2009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>san martin</td>\n",
              "      <td>santa fe</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>san cristobal</td>\n",
              "      <td>santa fe</td>\n",
              "      <td>2011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1029</th>\n",
              "      <td>cruz alta</td>\n",
              "      <td>tucuman</td>\n",
              "      <td>2008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1030</th>\n",
              "      <td>san martin</td>\n",
              "      <td>santa fe</td>\n",
              "      <td>2008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1031</th>\n",
              "      <td>general roca</td>\n",
              "      <td>cordoba</td>\n",
              "      <td>2011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1032</th>\n",
              "      <td>pergamino</td>\n",
              "      <td>buenos aires</td>\n",
              "      <td>2007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1033</th>\n",
              "      <td>san cristobal</td>\n",
              "      <td>santa fe</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1034 rows √ó 3 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bbb4d74a-9bc2-4af1-a08c-a34ee01f4e4b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-bbb4d74a-9bc2-4af1-a08c-a34ee01f4e4b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-bbb4d74a-9bc2-4af1-a08c-a34ee01f4e4b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-02425174-e043-4070-a121-410b522042a3\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-02425174-e043-4070-a121-410b522042a3')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-02425174-e043-4070-a121-410b522042a3 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_f3046021-0f6a-4fb5-8f12-5c0255f1f20b\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_f3046021-0f6a-4fb5-8f12-5c0255f1f20b button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "            Region1       Region2  Year\n",
              "0        9 de julio         chaco  2007\n",
              "1      constitucion      santa fe  2014\n",
              "2         gualeguay    entre rios  2009\n",
              "3        san martin      santa fe  2015\n",
              "4     san cristobal      santa fe  2011\n",
              "...             ...           ...   ...\n",
              "1029      cruz alta       tucuman  2008\n",
              "1030     san martin      santa fe  2008\n",
              "1031   general roca       cordoba  2011\n",
              "1032      pergamino  buenos aires  2007\n",
              "1033  san cristobal      santa fe  2015\n",
              "\n",
              "[1034 rows x 3 columns]"
            ]
          },
          "execution_count": 120,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# 1034 samples in training set\n",
        "\n",
        "\n",
        "data = np.load('/content/train_keys.npz') #train argentina\n",
        "\n",
        "split_data = [item.split('_') for item in data['data']]\n",
        "\n",
        "df = pd.DataFrame(split_data, columns=['Region1', 'Region2', 'Year'])\n",
        "df.to_csv('output_file.csv', index=False)\n",
        "df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dB_Z58Mo0fZv",
        "outputId": "f518a440-f115-492b-c839-6b6b17a46677"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[668.9,\n",
              " 502.3,\n",
              " 516.1,\n",
              " 502.3,\n",
              " 502.3,\n",
              " 502.3,\n",
              " 502.3,\n",
              " 502.3,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 501.8,\n",
              " 501.8,\n",
              " 516.1,\n",
              " 501.8,\n",
              " 501.8,\n",
              " 422.6,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 422.6,\n",
              " 668.9,\n",
              " 502.3,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 501.8,\n",
              " 502.3,\n",
              " 504.1,\n",
              " 501.8,\n",
              " 501.8,\n",
              " 504.1,\n",
              " 501.8,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 542.1,\n",
              " 504.1,\n",
              " 516.1,\n",
              " 501.8,\n",
              " 501.8,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 502.3,\n",
              " 502.3,\n",
              " 504.1,\n",
              " 422.6,\n",
              " 501.8,\n",
              " 504.1,\n",
              " 502.3,\n",
              " 516.1,\n",
              " 422.6,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 501.8,\n",
              " 501.8,\n",
              " 504.1,\n",
              " 501.8,\n",
              " 502.3,\n",
              " 501.8,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 682.3,\n",
              " 504.1,\n",
              " 422.6,\n",
              " 502.3,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 355.6,\n",
              " 516.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 501.8,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 422.6,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 516.1,\n",
              " 542.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 542.1,\n",
              " 502.3,\n",
              " 504.1,\n",
              " 501.8,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 668.9,\n",
              " 516.1,\n",
              " 422.6,\n",
              " 501.8,\n",
              " 504.1,\n",
              " 502.3,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 501.8,\n",
              " 355.6,\n",
              " 516.1,\n",
              " 502.3,\n",
              " 502.3,\n",
              " 504.1,\n",
              " 682.3,\n",
              " 668.9,\n",
              " 504.1,\n",
              " 516.1,\n",
              " 502.3,\n",
              " 504.1,\n",
              " 682.3,\n",
              " 668.9,\n",
              " 668.9,\n",
              " 504.1,\n",
              " 501.8,\n",
              " 501.8,\n",
              " 502.3,\n",
              " 502.3,\n",
              " 502.3,\n",
              " 516.1,\n",
              " 668.9,\n",
              " 504.1,\n",
              " 502.3,\n",
              " 501.8,\n",
              " 422.6,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 501.8,\n",
              " 422.6,\n",
              " 504.1,\n",
              " 422.6,\n",
              " 355.6,\n",
              " 516.1,\n",
              " 502.3,\n",
              " 355.6,\n",
              " 502.3,\n",
              " 504.1,\n",
              " 501.8,\n",
              " 542.1,\n",
              " 516.1,\n",
              " 504.1,\n",
              " 668.9,\n",
              " 516.1,\n",
              " 501.8,\n",
              " 501.8,\n",
              " 504.1,\n",
              " 516.1,\n",
              " 516.1,\n",
              " 502.3,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 501.8,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 502.3,\n",
              " 355.6,\n",
              " 422.6,\n",
              " 504.1,\n",
              " 502.3,\n",
              " 502.3,\n",
              " 502.3,\n",
              " 504.1,\n",
              " 502.3,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 501.8,\n",
              " 501.8,\n",
              " 504.1,\n",
              " 501.8,\n",
              " 501.8,\n",
              " 502.3,\n",
              " 504.1,\n",
              " 502.3,\n",
              " 504.1,\n",
              " 516.1,\n",
              " 516.1,\n",
              " 501.8,\n",
              " 682.3,\n",
              " 502.3,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 516.1,\n",
              " 502.3,\n",
              " 501.8,\n",
              " 542.1,\n",
              " 516.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 502.3,\n",
              " 504.1,\n",
              " 501.8,\n",
              " 422.6,\n",
              " 516.1,\n",
              " 682.3,\n",
              " 422.6,\n",
              " 502.3,\n",
              " 504.1,\n",
              " 668.9,\n",
              " 504.1,\n",
              " 502.3,\n",
              " 502.3,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 355.6,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 502.3,\n",
              " 516.1,\n",
              " 502.3,\n",
              " 501.8,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 502.3,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 682.3,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 668.9,\n",
              " 504.1,\n",
              " 501.8,\n",
              " 504.1,\n",
              " 501.8,\n",
              " 504.1,\n",
              " 516.1,\n",
              " 504.1,\n",
              " 422.6,\n",
              " 516.1,\n",
              " 502.3,\n",
              " 504.1,\n",
              " 355.6,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 422.6,\n",
              " 422.6,\n",
              " 501.8,\n",
              " 422.6,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 668.9,\n",
              " 501.8,\n",
              " 516.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 501.8,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 502.3,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 422.6,\n",
              " 501.8,\n",
              " 516.1,\n",
              " 501.8,\n",
              " 504.1,\n",
              " 502.3,\n",
              " 504.1,\n",
              " 668.9,\n",
              " 422.6,\n",
              " 504.1,\n",
              " 501.8,\n",
              " 504.1,\n",
              " 422.6,\n",
              " 668.9,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 501.8,\n",
              " 504.1,\n",
              " 501.8,\n",
              " 668.9,\n",
              " 422.6,\n",
              " 504.1,\n",
              " 422.6,\n",
              " 504.1,\n",
              " 501.8,\n",
              " 668.9,\n",
              " 504.1,\n",
              " 502.3,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 668.9,\n",
              " 504.1,\n",
              " 501.8,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 668.9,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 422.6,\n",
              " 542.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 502.3,\n",
              " 501.8,\n",
              " 504.1,\n",
              " 502.3,\n",
              " 501.8,\n",
              " 516.1,\n",
              " 682.3,\n",
              " 682.3,\n",
              " 502.3,\n",
              " 504.1,\n",
              " 502.3,\n",
              " 504.1,\n",
              " 422.6,\n",
              " 355.6,\n",
              " 504.1,\n",
              " 355.6,\n",
              " 504.1,\n",
              " 668.9,\n",
              " 501.8,\n",
              " 355.6,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 501.8,\n",
              " 501.8,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 668.9,\n",
              " 355.6,\n",
              " 542.1,\n",
              " 542.1,\n",
              " 504.1,\n",
              " 516.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 355.6,\n",
              " 516.1,\n",
              " 504.1,\n",
              " 502.3,\n",
              " 501.8,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 502.3,\n",
              " 422.6,\n",
              " 504.1,\n",
              " 355.6,\n",
              " 422.6,\n",
              " 504.1,\n",
              " 516.1,\n",
              " 502.3,\n",
              " 504.1,\n",
              " 420.1,\n",
              " 502.3,\n",
              " 504.1,\n",
              " 501.8,\n",
              " 682.3,\n",
              " 504.1,\n",
              " 542.1,\n",
              " 504.1,\n",
              " 501.8,\n",
              " 668.9,\n",
              " 516.1,\n",
              " 516.1,\n",
              " 668.9,\n",
              " 501.8,\n",
              " 502.3,\n",
              " 501.8,\n",
              " 501.8,\n",
              " 504.1,\n",
              " 516.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 502.3,\n",
              " 502.3,\n",
              " 516.1,\n",
              " 422.6,\n",
              " 668.9,\n",
              " 501.8,\n",
              " 504.1,\n",
              " 501.8,\n",
              " 504.1,\n",
              " 355.6,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 542.1,\n",
              " 504.1,\n",
              " 422.6,\n",
              " 504.1,\n",
              " 422.6,\n",
              " 502.3,\n",
              " 504.1,\n",
              " 501.8,\n",
              " 501.8,\n",
              " 504.1,\n",
              " 422.6,\n",
              " 501.8,\n",
              " 420.1,\n",
              " 668.9,\n",
              " 504.1,\n",
              " 422.6,\n",
              " 501.8,\n",
              " 504.1,\n",
              " 668.9,\n",
              " 501.8,\n",
              " 504.1,\n",
              " 501.8,\n",
              " 504.1,\n",
              " 668.9,\n",
              " 542.1,\n",
              " 668.9,\n",
              " 501.8,\n",
              " 501.8,\n",
              " 504.1,\n",
              " 516.1,\n",
              " 516.1,\n",
              " 501.8,\n",
              " 502.3,\n",
              " 504.1,\n",
              " 516.1,\n",
              " 501.8,\n",
              " 501.8,\n",
              " 682.3,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 502.3,\n",
              " 682.3,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 502.3,\n",
              " 668.9,\n",
              " 501.8,\n",
              " 502.3,\n",
              " 501.8,\n",
              " 516.1,\n",
              " 668.9,\n",
              " 502.3,\n",
              " 682.3,\n",
              " 501.8,\n",
              " 516.1,\n",
              " 504.1,\n",
              " 516.1,\n",
              " 516.1,\n",
              " 501.8,\n",
              " 504.1,\n",
              " 668.9,\n",
              " 502.3,\n",
              " 502.3,\n",
              " 504.1,\n",
              " 502.3,\n",
              " 668.9,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 668.9,\n",
              " 502.3,\n",
              " 504.1,\n",
              " 542.1,\n",
              " 504.1,\n",
              " 502.3,\n",
              " 668.9,\n",
              " 422.6,\n",
              " 668.9,\n",
              " 504.1,\n",
              " 502.3,\n",
              " 502.3,\n",
              " 501.8,\n",
              " 542.1,\n",
              " 516.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 668.9,\n",
              " 501.8,\n",
              " 502.3,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 682.3,\n",
              " 355.6,\n",
              " 422.6,\n",
              " 682.3,\n",
              " 542.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 542.1,\n",
              " 502.3,\n",
              " 504.1,\n",
              " 516.1,\n",
              " 504.1,\n",
              " 501.8,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 422.6,\n",
              " 504.1,\n",
              " 355.6,\n",
              " 504.1,\n",
              " 355.6,\n",
              " 355.6,\n",
              " 504.1,\n",
              " 501.8,\n",
              " 422.6,\n",
              " 501.8,\n",
              " 502.3,\n",
              " 502.3,\n",
              " 502.3,\n",
              " 542.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 542.1,\n",
              " 504.1,\n",
              " 502.3,\n",
              " 355.6,\n",
              " 504.1,\n",
              " 502.3,\n",
              " 501.8,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 422.6,\n",
              " 504.1,\n",
              " 542.1,\n",
              " 420.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 501.8,\n",
              " 504.1,\n",
              " 668.9,\n",
              " 504.1,\n",
              " 516.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 516.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 502.3,\n",
              " 422.6,\n",
              " 504.1,\n",
              " 501.8,\n",
              " 501.8,\n",
              " 542.1,\n",
              " 501.8,\n",
              " 502.3,\n",
              " 516.1,\n",
              " 502.3,\n",
              " 504.1,\n",
              " 668.9,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 502.3,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 516.1,\n",
              " 502.3,\n",
              " 668.9,\n",
              " 502.3,\n",
              " 516.1,\n",
              " 504.1,\n",
              " 422.6,\n",
              " 502.3,\n",
              " 502.3,\n",
              " 504.1,\n",
              " 516.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 502.3,\n",
              " 504.1,\n",
              " 668.9,\n",
              " 516.1,\n",
              " 504.1,\n",
              " 355.6,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 422.6,\n",
              " 504.1,\n",
              " 502.3,\n",
              " 516.1,\n",
              " 502.3,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 501.8,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 668.9,\n",
              " 504.1,\n",
              " 542.1,\n",
              " 504.1,\n",
              " 668.9,\n",
              " 501.8,\n",
              " 504.1,\n",
              " 502.3,\n",
              " 516.1,\n",
              " 516.1,\n",
              " 516.1,\n",
              " 504.1,\n",
              " 516.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 668.9,\n",
              " 502.3,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 422.6,\n",
              " 504.1,\n",
              " 355.6,\n",
              " 504.1,\n",
              " 501.8,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 502.3,\n",
              " 504.1,\n",
              " 501.8,\n",
              " 668.9,\n",
              " 504.1,\n",
              " 682.3,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 422.6,\n",
              " 542.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 422.6,\n",
              " 504.1,\n",
              " 501.8,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 502.3,\n",
              " 516.1,\n",
              " 504.1,\n",
              " 502.3,\n",
              " 668.9,\n",
              " 504.1,\n",
              " 542.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 422.6,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 501.8,\n",
              " 668.9,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 668.9,\n",
              " 501.8,\n",
              " 504.1,\n",
              " 501.8,\n",
              " 355.6,\n",
              " 504.1,\n",
              " 422.6,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 355.6,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 516.1,\n",
              " 504.1,\n",
              " 501.8,\n",
              " 422.6,\n",
              " 502.3,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 501.8,\n",
              " 668.9,\n",
              " 422.6,\n",
              " 668.9,\n",
              " 504.1,\n",
              " 502.3,\n",
              " 502.3,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 422.6,\n",
              " 516.1,\n",
              " 501.8,\n",
              " 420.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 422.6,\n",
              " 502.3,\n",
              " 422.6,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 422.6,\n",
              " 516.1,\n",
              " 501.8,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 502.3,\n",
              " 504.1,\n",
              " 516.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 516.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 542.1,\n",
              " 504.1,\n",
              " 355.6,\n",
              " 504.1,\n",
              " 668.9,\n",
              " 422.6,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 422.6,\n",
              " 501.8,\n",
              " 502.3,\n",
              " 504.1,\n",
              " 502.3,\n",
              " 502.3,\n",
              " 668.9,\n",
              " 504.1,\n",
              " 516.1,\n",
              " 502.3,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 355.6,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 502.3,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 501.8,\n",
              " 422.6,\n",
              " 501.8,\n",
              " 504.1,\n",
              " 668.9,\n",
              " 420.1,\n",
              " 501.8,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 516.1,\n",
              " 422.6,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 501.8,\n",
              " 542.1,\n",
              " 504.1,\n",
              " 516.1,\n",
              " 504.1,\n",
              " 668.9,\n",
              " 504.1,\n",
              " 422.6,\n",
              " 504.1,\n",
              " 501.8,\n",
              " 516.1,\n",
              " 504.1,\n",
              " 542.1,\n",
              " 668.9,\n",
              " 516.1,\n",
              " 502.3,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 501.8,\n",
              " 502.3,\n",
              " 422.6,\n",
              " 504.1,\n",
              " 516.1,\n",
              " 516.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 501.8,\n",
              " 502.3,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 502.3,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 502.3,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 542.1,\n",
              " 355.6,\n",
              " 504.1,\n",
              " 355.6,\n",
              " 516.1,\n",
              " 502.3,\n",
              " 542.1,\n",
              " 668.9,\n",
              " 504.1,\n",
              " 355.6,\n",
              " 504.1,\n",
              " 501.8,\n",
              " 682.3,\n",
              " 504.1,\n",
              " 501.8,\n",
              " 355.6,\n",
              " 504.1,\n",
              " 502.3,\n",
              " 504.1,\n",
              " 501.8,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 422.6,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 682.3,\n",
              " 501.8,\n",
              " 542.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 501.8,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 501.8,\n",
              " 516.1,\n",
              " 502.3,\n",
              " 502.3,\n",
              " 501.8,\n",
              " 668.9,\n",
              " 501.8,\n",
              " 542.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 502.3,\n",
              " 422.6,\n",
              " 501.8,\n",
              " 516.1,\n",
              " 501.8,\n",
              " 542.1,\n",
              " 501.8,\n",
              " 504.1,\n",
              " 501.8,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 516.1,\n",
              " 516.1,\n",
              " 501.8,\n",
              " 516.1,\n",
              " 504.1,\n",
              " 422.6,\n",
              " 502.3,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 668.9,\n",
              " 502.3,\n",
              " 501.8,\n",
              " 504.1,\n",
              " 502.3,\n",
              " 504.1,\n",
              " 501.8,\n",
              " 516.1,\n",
              " 501.8,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 516.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 422.6,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 516.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 502.3,\n",
              " 504.1,\n",
              " 668.9,\n",
              " 422.6,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 502.3,\n",
              " 504.1,\n",
              " 542.1,\n",
              " 502.3,\n",
              " 504.1,\n",
              " 516.1,\n",
              " 504.1,\n",
              " 422.6,\n",
              " 502.3,\n",
              " 422.6,\n",
              " 504.1,\n",
              " 420.1,\n",
              " 668.9,\n",
              " 502.3,\n",
              " 668.9,\n",
              " 501.8,\n",
              " 504.1,\n",
              " 355.6,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 516.1,\n",
              " 516.1,\n",
              " 516.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 501.8,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 501.8,\n",
              " 504.1,\n",
              " 501.8,\n",
              " 668.9,\n",
              " 355.6,\n",
              " 516.1,\n",
              " 504.1,\n",
              " 516.1,\n",
              " 504.1,\n",
              " 502.3,\n",
              " 504.1,\n",
              " 668.9,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 501.8,\n",
              " 504.1,\n",
              " 668.9,\n",
              " 501.8,\n",
              " 504.1,\n",
              " 502.3,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 668.9,\n",
              " 516.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 516.1,\n",
              " 542.1,\n",
              " 420.1,\n",
              " 504.1,\n",
              " 502.3,\n",
              " 501.8,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 516.1,\n",
              " 501.8,\n",
              " 504.1,\n",
              " 422.6,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 668.9,\n",
              " 668.9,\n",
              " 501.8,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 422.6,\n",
              " 355.6,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 504.1,\n",
              " 516.1,\n",
              " 516.1,\n",
              " 504.1,\n",
              " ...]"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# the 'rainfall.csv' is the added rainfall value version of 'output_file.csv'\n",
        "df_rainfall = pd.read_csv('/content/rainfall.csv')\n",
        "rf = df_rainfall['Rainfall']\n",
        "a = rf.tolist()\n",
        "a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwr6urnPEWsw",
        "outputId": "fbfa72ba-3382-41d5-e404-52c3b74f1d61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([32, 32, 32, 10])\n",
            "torch.Size([32, 32, 32, 10])\n",
            "torch.Size([32, 32, 32, 10])\n",
            "torch.Size([32, 32, 32, 10])\n",
            "torch.Size([32, 32, 32, 10])\n",
            "torch.Size([32, 32, 32, 10])\n",
            "torch.Size([32, 32, 32, 10])\n",
            "torch.Size([32, 32, 32, 10])\n",
            "torch.Size([32, 32, 32, 10])\n",
            "torch.Size([32, 32, 32, 10])\n",
            "torch.Size([32, 32, 32, 10])\n",
            "torch.Size([32, 32, 32, 10])\n",
            "torch.Size([32, 32, 32, 10])\n",
            "torch.Size([32, 32, 32, 10])\n",
            "torch.Size([32, 32, 32, 10])\n",
            "torch.Size([32, 32, 32, 10])\n",
            "torch.Size([32, 32, 32, 10])\n",
            "torch.Size([32, 32, 32, 10])\n",
            "torch.Size([32, 32, 32, 10])\n",
            "torch.Size([32, 32, 32, 10])\n",
            "torch.Size([32, 32, 32, 10])\n",
            "torch.Size([32, 32, 32, 10])\n",
            "torch.Size([32, 32, 32, 10])\n",
            "torch.Size([32, 32, 32, 10])\n",
            "torch.Size([32, 32, 32, 10])\n",
            "torch.Size([32, 32, 32, 10])\n",
            "torch.Size([32, 32, 32, 10])\n",
            "torch.Size([32, 32, 32, 10])\n",
            "torch.Size([32, 32, 32, 10])\n",
            "torch.Size([32, 32, 32, 10])\n",
            "torch.Size([32, 32, 32, 10])\n",
            "torch.Size([32, 32, 32, 10])\n",
            "torch.Size([10, 32, 32, 10])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data, rainfall_data):\n",
        "        self.data = data  \n",
        "        self.rainfall_data = rainfall_data  \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x, y = self.data[idx]\n",
        "        rainfall = self.rainfall_data[idx]\n",
        "\n",
        "        # Create a rainfall array of shape (32, 32, 1)\n",
        "        rainfall = np.full((32, 32, 1), rainfall)        \n",
        "        x = np.concatenate((x, rainfall), axis=-1)\n",
        "        x = torch.tensor(x, dtype=torch.float32)\n",
        "        return x, y\n",
        "\n",
        "\n",
        "train_data = dataset.get_subset('train')\n",
        "rainfall_data = a  \n",
        "train_data_add = CustomDataset(train_data, rainfall_data)\n",
        "\n",
        "# Create DataLoader\n",
        "train_loader_add = DataLoader(train_data_add, batch_size=32, shuffle=False)\n",
        "\n",
        "# Check outputs\n",
        "for x, y in train_loader_add:\n",
        "    print(x.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "imivswYTkayf"
      },
      "outputs": [],
      "source": [
        "\n",
        "class LSTM_ADD_Config():\n",
        "    B, W, C = 32,32,10\n",
        "    H = 32 #all season lengths will be 32\n",
        "    loss_lambda = 0.75\n",
        "    lstm_layers = 1\n",
        "    lstm_H = 200\n",
        "    dense = 356\n",
        "    season_len = 32\n",
        "\n",
        "    train_step = 10000000\n",
        "    lr = 0.01\n",
        "    #keep probability\n",
        "    drop_out = 0.75\n",
        "\n",
        "    def __init__(self, season_frac=None):\n",
        "        if season_frac is not None:\n",
        "            self.H = int(season_frac*self.H)\n",
        "\n",
        "def dense(input_data, H, N=None, name = \"dense\"):\n",
        "    if not N:\n",
        "        N = input_data.get_shape()[-1]\n",
        "    with tf.variable_scope(name):\n",
        "        W = tf.get_variable(\"W\", [N, H], initializer=tf.contrib.layers.variance_scaling_initializer())\n",
        "        b = tf.get_variable(\"b\", [1, H])\n",
        "        tf.summary.histogram(name + \".W\", W)\n",
        "        tf.summary.histogram(name + \".b\", b)\n",
        "        return tf.matmul(input_data, W, name=\"matmul\") + b\n",
        "\n",
        "def lstm_add_net(input_data,output_data,config,keep_prob = 1,name=\"lstm_add_net\"):\n",
        "    with tf.variable_scope(name):\n",
        "        lstm_cell = tf.nn.rnn_cell.LSTMCell(config.lstm_H,state_is_tuple=True)\n",
        "        lstm_cell = tf.nn.rnn_cell.DropoutWrapper(lstm_cell, output_keep_prob=keep_prob)\n",
        "        cell = tf.nn.rnn_cell.MultiRNNCell([lstm_cell] * config.lstm_layers,state_is_tuple=True)\n",
        "        state = cell.zero_state(config.B, tf.float32)\n",
        "        outputs, final_state = tf.nn.dynamic_rnn(cell, input_data,\n",
        "                       initial_state=state, time_major=True)\n",
        "        tf.summary.histogram(name + '.outputs', outputs)\n",
        "        output_final = tf.squeeze(tf.slice(outputs, [config.H-1,0,0] , [1,-1,-1]))\n",
        "        tf.summary.histogram(name + '.output_final', output_final)\n",
        "        fc1 = dense(output_final, config.dense, name=\"dense\")\n",
        "\n",
        "        logit = tf.squeeze(dense(fc1,1,name='logit'))\n",
        "        tf.summary.histogram(name + '.logit', logit)\n",
        "        loss_err = tf.nn.l2_loss(logit - output_data)\n",
        "        loss_reg = tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables()])\n",
        "        total_loss = config.loss_lambda * loss_err + (1 - config.loss_lambda) * loss_reg\n",
        "\n",
        "        tf.summary.scalar(name + '.loss_err', loss_err)\n",
        "        tf.summary.scalar(name + '.loss_reg', loss_reg)\n",
        "        tf.summary.scalar(name + '.loss_total', total_loss)\n",
        "\n",
        "        return logit,total_loss,fc1\n",
        "\n",
        "class LSTM_ADDModel(tf.keras.Model):\n",
        "    def __init__(self, config, name=\"LSTM_ADDModel\"):\n",
        "        super(LSTM_ADDModel, self).__init__(name=name)\n",
        "        self.config = config\n",
        "        self.lstm = tf.keras.layers.LSTM(\n",
        "            units=config.lstm_H,\n",
        "            return_sequences=False,\n",
        "            return_state=False,\n",
        "            dropout=1 - config.drop_out,\n",
        "        )\n",
        "        self.dense1 = tf.keras.layers.Dense(config.dense, activation=\"relu\")\n",
        "        self.logit = tf.keras.layers.Dense(1, activation=None)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        x = tf.transpose(inputs, perm=[2, 0, 1, 3])\n",
        "        x = tf.reshape(x, [-1, self.config.W, self.config.H * self.config.C])\n",
        "\n",
        "        # LSTM layer\n",
        "        x = self.lstm(x, training=training)\n",
        "\n",
        "        # Dense layer\n",
        "        x = self.dense1(x)\n",
        "        output = self.logit(x)\n",
        "\n",
        "        return output\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "bIeiDmOzImsz"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train_add_model(model, config, num_epochs=2):\n",
        "\n",
        "    loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=config.lr)\n",
        "\n",
        "    train_loader = train_loader_add\n",
        "\n",
        "    model = LSTM_ADDModel(config)\n",
        "    r2list = []\n",
        "    rmselist = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "    # Iterate through the train_loader to access batches of data\n",
        "        y_true = []\n",
        "        y_pred = []\n",
        "        batch_size = config.B\n",
        "        for batch_idx, (batch_x, batch_y) in enumerate(train_loader):\n",
        "            with tf.GradientTape() as tape:\n",
        "                predictions = model(batch_x, training=True)\n",
        "                loss = loss_fn(batch_y, predictions)\n",
        "\n",
        "            gradients = tape.gradient(loss, model.trainable_variables)\n",
        "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "            y_true.extend(batch_y.numpy().flatten())\n",
        "            y_pred.extend(predictions.numpy().flatten())\n",
        "            print(f\"Epoch {epoch + 1}, Batch {batch_idx + 1}, Loss: {loss.numpy()}\")\n",
        "\n",
        "        r2 = r2_score(y_true, y_pred)\n",
        "        rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "        r2list.append(r2)\n",
        "        rmselist.append(rmse)\n",
        "    return model, r2list, rmselist\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tPk71CB4J2XT",
        "outputId": "6bca4b8e-0920-4e44-8ab4-f1a0dac594e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Batch 1, Loss: 7.172255516052246\n",
            "Epoch 1, Batch 2, Loss: 5.087353706359863\n",
            "Epoch 1, Batch 3, Loss: 3.425551414489746\n",
            "Epoch 1, Batch 4, Loss: 1.5421671867370605\n",
            "Epoch 1, Batch 5, Loss: 0.665388822555542\n",
            "Epoch 1, Batch 6, Loss: 1.0005890130996704\n",
            "Epoch 1, Batch 7, Loss: 0.9293491840362549\n",
            "Epoch 1, Batch 8, Loss: 0.861047625541687\n",
            "Epoch 1, Batch 9, Loss: 0.7611544728279114\n",
            "Epoch 1, Batch 10, Loss: 0.9766716957092285\n",
            "Epoch 1, Batch 11, Loss: 0.949386477470398\n",
            "Epoch 1, Batch 12, Loss: 0.7636209726333618\n",
            "Epoch 1, Batch 13, Loss: 1.0048158168792725\n",
            "Epoch 1, Batch 14, Loss: 0.7253059148788452\n",
            "Epoch 1, Batch 15, Loss: 0.6954406499862671\n",
            "Epoch 1, Batch 16, Loss: 0.7764778137207031\n",
            "Epoch 1, Batch 17, Loss: 0.9878766536712646\n",
            "Epoch 1, Batch 18, Loss: 0.9826576709747314\n",
            "Epoch 1, Batch 19, Loss: 1.097381353378296\n",
            "Epoch 1, Batch 20, Loss: 0.6760913133621216\n",
            "Epoch 1, Batch 21, Loss: 1.229600191116333\n",
            "Epoch 1, Batch 22, Loss: 0.9220498204231262\n",
            "Epoch 1, Batch 23, Loss: 0.6088199019432068\n",
            "Epoch 1, Batch 24, Loss: 0.7207849621772766\n",
            "Epoch 1, Batch 25, Loss: 0.80647873878479\n",
            "Epoch 1, Batch 26, Loss: 0.8242796659469604\n",
            "Epoch 1, Batch 27, Loss: 0.6293070316314697\n",
            "Epoch 1, Batch 28, Loss: 0.7274734377861023\n",
            "Epoch 1, Batch 29, Loss: 0.5348632335662842\n",
            "Epoch 1, Batch 30, Loss: 0.867424726486206\n",
            "Epoch 1, Batch 31, Loss: 0.7247178554534912\n",
            "Epoch 1, Batch 32, Loss: 0.6597129106521606\n",
            "Epoch 1, Batch 33, Loss: 0.8468165397644043\n",
            "Epoch 2, Batch 1, Loss: 0.9717431664466858\n",
            "Epoch 2, Batch 2, Loss: 0.4972155690193176\n",
            "Epoch 2, Batch 3, Loss: 0.8960422277450562\n",
            "Epoch 2, Batch 4, Loss: 0.6355770826339722\n",
            "Epoch 2, Batch 5, Loss: 0.5512385964393616\n",
            "Epoch 2, Batch 6, Loss: 0.6886427402496338\n",
            "Epoch 2, Batch 7, Loss: 0.6815150380134583\n",
            "Epoch 2, Batch 8, Loss: 1.0127276182174683\n",
            "Epoch 2, Batch 9, Loss: 0.8379970788955688\n",
            "Epoch 2, Batch 10, Loss: 0.811427116394043\n",
            "Epoch 2, Batch 11, Loss: 0.9021755456924438\n",
            "Epoch 2, Batch 12, Loss: 0.7495449781417847\n",
            "Epoch 2, Batch 13, Loss: 0.6071896553039551\n",
            "Epoch 2, Batch 14, Loss: 0.6386849880218506\n",
            "Epoch 2, Batch 15, Loss: 0.7283924221992493\n",
            "Epoch 2, Batch 16, Loss: 0.6914840340614319\n",
            "Epoch 2, Batch 17, Loss: 0.5713133811950684\n",
            "Epoch 2, Batch 18, Loss: 0.8312275409698486\n",
            "Epoch 2, Batch 19, Loss: 1.0908610820770264\n",
            "Epoch 2, Batch 20, Loss: 0.505748987197876\n",
            "Epoch 2, Batch 21, Loss: 0.9126353859901428\n",
            "Epoch 2, Batch 22, Loss: 0.8470171689987183\n",
            "Epoch 2, Batch 23, Loss: 0.5836488008499146\n",
            "Epoch 2, Batch 24, Loss: 0.682183563709259\n",
            "Epoch 2, Batch 25, Loss: 0.5618402361869812\n",
            "Epoch 2, Batch 26, Loss: 0.6494457721710205\n",
            "Epoch 2, Batch 27, Loss: 0.6178935170173645\n",
            "Epoch 2, Batch 28, Loss: 0.7273303270339966\n",
            "Epoch 2, Batch 29, Loss: 0.45053625106811523\n",
            "Epoch 2, Batch 30, Loss: 0.7985947728157043\n",
            "Epoch 2, Batch 31, Loss: 0.6713489294052124\n",
            "Epoch 2, Batch 32, Loss: 0.6344389915466309\n",
            "Epoch 2, Batch 33, Loss: 0.8203471899032593\n",
            "Epoch 3, Batch 1, Loss: 0.9363237619400024\n",
            "Epoch 3, Batch 2, Loss: 0.5136037468910217\n",
            "Epoch 3, Batch 3, Loss: 0.8365223407745361\n",
            "Epoch 3, Batch 4, Loss: 0.7021692395210266\n",
            "Epoch 3, Batch 5, Loss: 0.5361059904098511\n",
            "Epoch 3, Batch 6, Loss: 0.7476431727409363\n",
            "Epoch 3, Batch 7, Loss: 0.7045507431030273\n",
            "Epoch 3, Batch 8, Loss: 0.8824673891067505\n",
            "Epoch 3, Batch 9, Loss: 0.7714618444442749\n",
            "Epoch 3, Batch 10, Loss: 0.7939088344573975\n",
            "Epoch 3, Batch 11, Loss: 0.7884981632232666\n",
            "Epoch 3, Batch 12, Loss: 0.6517897248268127\n",
            "Epoch 3, Batch 13, Loss: 0.6718392968177795\n",
            "Epoch 3, Batch 14, Loss: 0.639208972454071\n",
            "Epoch 3, Batch 15, Loss: 0.6608644723892212\n",
            "Epoch 3, Batch 16, Loss: 0.6801533699035645\n",
            "Epoch 3, Batch 17, Loss: 0.6825079917907715\n",
            "Epoch 3, Batch 18, Loss: 0.8757961988449097\n",
            "Epoch 3, Batch 19, Loss: 1.0459883213043213\n",
            "Epoch 3, Batch 20, Loss: 0.5340456366539001\n",
            "Epoch 3, Batch 21, Loss: 1.0238616466522217\n",
            "Epoch 3, Batch 22, Loss: 0.8883926868438721\n",
            "Epoch 3, Batch 23, Loss: 0.5711223483085632\n",
            "Epoch 3, Batch 24, Loss: 0.6850736141204834\n",
            "Epoch 3, Batch 25, Loss: 0.7356709241867065\n",
            "Epoch 3, Batch 26, Loss: 0.7392182350158691\n",
            "Epoch 3, Batch 27, Loss: 0.6650238633155823\n",
            "Epoch 3, Batch 28, Loss: 0.7727150917053223\n",
            "Epoch 3, Batch 29, Loss: 0.5074002742767334\n",
            "Epoch 3, Batch 30, Loss: 0.7352863550186157\n",
            "Epoch 3, Batch 31, Loss: 0.6951589584350586\n",
            "Epoch 3, Batch 32, Loss: 0.6786532402038574\n",
            "Epoch 3, Batch 33, Loss: 0.8686337471008301\n",
            "Epoch 4, Batch 1, Loss: 0.9459261894226074\n",
            "Epoch 4, Batch 2, Loss: 0.4749051630496979\n",
            "Epoch 4, Batch 3, Loss: 0.8573378324508667\n",
            "Epoch 4, Batch 4, Loss: 0.6447268724441528\n",
            "Epoch 4, Batch 5, Loss: 0.525818407535553\n",
            "Epoch 4, Batch 6, Loss: 0.6741967797279358\n",
            "Epoch 4, Batch 7, Loss: 0.6323999166488647\n",
            "Epoch 4, Batch 8, Loss: 0.923372209072113\n",
            "Epoch 4, Batch 9, Loss: 0.7543320059776306\n",
            "Epoch 4, Batch 10, Loss: 0.8098269104957581\n",
            "Epoch 4, Batch 11, Loss: 0.8910084962844849\n",
            "Epoch 4, Batch 12, Loss: 0.7047989368438721\n",
            "Epoch 4, Batch 13, Loss: 0.6269890069961548\n",
            "Epoch 4, Batch 14, Loss: 0.6453700661659241\n",
            "Epoch 4, Batch 15, Loss: 0.7113384008407593\n",
            "Epoch 4, Batch 16, Loss: 0.687605619430542\n",
            "Epoch 4, Batch 17, Loss: 0.6085879802703857\n",
            "Epoch 4, Batch 18, Loss: 0.849109411239624\n",
            "Epoch 4, Batch 19, Loss: 1.0860430002212524\n",
            "Epoch 4, Batch 20, Loss: 0.5035759210586548\n",
            "Epoch 4, Batch 21, Loss: 0.9477107524871826\n",
            "Epoch 4, Batch 22, Loss: 0.8666684031486511\n",
            "Epoch 4, Batch 23, Loss: 0.5916117429733276\n",
            "Epoch 4, Batch 24, Loss: 0.6816129684448242\n",
            "Epoch 4, Batch 25, Loss: 0.5705496072769165\n",
            "Epoch 4, Batch 26, Loss: 0.6661586165428162\n",
            "Epoch 4, Batch 27, Loss: 0.6364365816116333\n",
            "Epoch 4, Batch 28, Loss: 0.7112741470336914\n",
            "Epoch 4, Batch 29, Loss: 0.45158714056015015\n",
            "Epoch 4, Batch 30, Loss: 0.7009519338607788\n",
            "Epoch 4, Batch 31, Loss: 0.6877121329307556\n",
            "Epoch 4, Batch 32, Loss: 0.6882883310317993\n",
            "Epoch 4, Batch 33, Loss: 0.7594869136810303\n",
            "Epoch 5, Batch 1, Loss: 0.9476021528244019\n",
            "Epoch 5, Batch 2, Loss: 0.502220094203949\n",
            "Epoch 5, Batch 3, Loss: 0.8752228021621704\n",
            "Epoch 5, Batch 4, Loss: 0.656993567943573\n",
            "Epoch 5, Batch 5, Loss: 0.523779034614563\n",
            "Epoch 5, Batch 6, Loss: 0.6985243558883667\n",
            "Epoch 5, Batch 7, Loss: 0.6329332590103149\n",
            "Epoch 5, Batch 8, Loss: 0.9308477640151978\n",
            "Epoch 5, Batch 9, Loss: 0.7782943248748779\n",
            "Epoch 5, Batch 10, Loss: 0.8005744814872742\n",
            "Epoch 5, Batch 11, Loss: 0.8734828233718872\n",
            "Epoch 5, Batch 12, Loss: 0.6845712661743164\n",
            "Epoch 5, Batch 13, Loss: 0.6345176696777344\n",
            "Epoch 5, Batch 14, Loss: 0.6405191421508789\n",
            "Epoch 5, Batch 15, Loss: 0.6886443495750427\n",
            "Epoch 5, Batch 16, Loss: 0.6891168355941772\n",
            "Epoch 5, Batch 17, Loss: 0.6067577004432678\n",
            "Epoch 5, Batch 18, Loss: 0.8345652222633362\n",
            "Epoch 5, Batch 19, Loss: 1.0544039011001587\n",
            "Epoch 5, Batch 20, Loss: 0.5253187417984009\n",
            "Epoch 5, Batch 21, Loss: 0.9694915413856506\n",
            "Epoch 5, Batch 22, Loss: 0.8661167621612549\n",
            "Epoch 5, Batch 23, Loss: 0.5868356227874756\n",
            "Epoch 5, Batch 24, Loss: 0.673922598361969\n",
            "Epoch 5, Batch 25, Loss: 0.5953774452209473\n",
            "Epoch 5, Batch 26, Loss: 0.6686984896659851\n",
            "Epoch 5, Batch 27, Loss: 0.6329006552696228\n",
            "Epoch 5, Batch 28, Loss: 0.7383242249488831\n",
            "Epoch 5, Batch 29, Loss: 0.46847695112228394\n",
            "Epoch 5, Batch 30, Loss: 0.7129544019699097\n",
            "Epoch 5, Batch 31, Loss: 0.6834028363227844\n",
            "Epoch 5, Batch 32, Loss: 0.6700550317764282\n",
            "Epoch 5, Batch 33, Loss: 0.807886004447937\n",
            "Epoch 6, Batch 1, Loss: 0.9417620897293091\n",
            "Epoch 6, Batch 2, Loss: 0.46969079971313477\n",
            "Epoch 6, Batch 3, Loss: 0.8659875392913818\n",
            "Epoch 6, Batch 4, Loss: 0.6340062618255615\n",
            "Epoch 6, Batch 5, Loss: 0.5085006356239319\n",
            "Epoch 6, Batch 6, Loss: 0.6946108937263489\n",
            "Epoch 6, Batch 7, Loss: 0.6432644724845886\n",
            "Epoch 6, Batch 8, Loss: 0.9249712824821472\n",
            "Epoch 6, Batch 9, Loss: 0.7781723141670227\n",
            "Epoch 6, Batch 10, Loss: 0.7965652942657471\n",
            "Epoch 6, Batch 11, Loss: 0.8473371267318726\n",
            "Epoch 6, Batch 12, Loss: 0.6874328255653381\n",
            "Epoch 6, Batch 13, Loss: 0.6360349059104919\n",
            "Epoch 6, Batch 14, Loss: 0.6380177736282349\n",
            "Epoch 6, Batch 15, Loss: 0.6866128444671631\n",
            "Epoch 6, Batch 16, Loss: 0.6806365251541138\n",
            "Epoch 6, Batch 17, Loss: 0.6227129697799683\n",
            "Epoch 6, Batch 18, Loss: 0.8486161231994629\n",
            "Epoch 6, Batch 19, Loss: 1.068942666053772\n",
            "Epoch 6, Batch 20, Loss: 0.5104840993881226\n",
            "Epoch 6, Batch 21, Loss: 0.9590863585472107\n",
            "Epoch 6, Batch 22, Loss: 0.8562339544296265\n",
            "Epoch 6, Batch 23, Loss: 0.583710789680481\n",
            "Epoch 6, Batch 24, Loss: 0.6751298904418945\n",
            "Epoch 6, Batch 25, Loss: 0.6096473932266235\n",
            "Epoch 6, Batch 26, Loss: 0.6679665446281433\n",
            "Epoch 6, Batch 27, Loss: 0.63267582654953\n",
            "Epoch 6, Batch 28, Loss: 0.7524591088294983\n",
            "Epoch 6, Batch 29, Loss: 0.4580245614051819\n",
            "Epoch 6, Batch 30, Loss: 0.7230523824691772\n",
            "Epoch 6, Batch 31, Loss: 0.687618613243103\n",
            "Epoch 6, Batch 32, Loss: 0.6671730279922485\n",
            "Epoch 6, Batch 33, Loss: 0.8023802042007446\n",
            "Epoch 7, Batch 1, Loss: 0.8981314897537231\n",
            "Epoch 7, Batch 2, Loss: 0.47483500838279724\n",
            "Epoch 7, Batch 3, Loss: 0.8581169247627258\n",
            "Epoch 7, Batch 4, Loss: 0.6382248997688293\n",
            "Epoch 7, Batch 5, Loss: 0.5077896118164062\n",
            "Epoch 7, Batch 6, Loss: 0.7009336948394775\n",
            "Epoch 7, Batch 7, Loss: 0.6487737894058228\n",
            "Epoch 7, Batch 8, Loss: 0.9191493988037109\n",
            "Epoch 7, Batch 9, Loss: 0.7757992744445801\n",
            "Epoch 7, Batch 10, Loss: 0.7993259429931641\n",
            "Epoch 7, Batch 11, Loss: 0.854766309261322\n",
            "Epoch 7, Batch 12, Loss: 0.6817446947097778\n",
            "Epoch 7, Batch 13, Loss: 0.6323956251144409\n",
            "Epoch 7, Batch 14, Loss: 0.6402345299720764\n",
            "Epoch 7, Batch 15, Loss: 0.6848711967468262\n",
            "Epoch 7, Batch 16, Loss: 0.6616140604019165\n",
            "Epoch 7, Batch 17, Loss: 0.6300424337387085\n",
            "Epoch 7, Batch 18, Loss: 0.8496631383895874\n",
            "Epoch 7, Batch 19, Loss: 1.0729387998580933\n",
            "Epoch 7, Batch 20, Loss: 0.5079739689826965\n",
            "Epoch 7, Batch 21, Loss: 0.9754335880279541\n",
            "Epoch 7, Batch 22, Loss: 0.8486592769622803\n",
            "Epoch 7, Batch 23, Loss: 0.5852420330047607\n",
            "Epoch 7, Batch 24, Loss: 0.6720028519630432\n",
            "Epoch 7, Batch 25, Loss: 0.6119410991668701\n",
            "Epoch 7, Batch 26, Loss: 0.6708420515060425\n",
            "Epoch 7, Batch 27, Loss: 0.6363980770111084\n",
            "Epoch 7, Batch 28, Loss: 0.7407820224761963\n",
            "Epoch 7, Batch 29, Loss: 0.45618870854377747\n",
            "Epoch 7, Batch 30, Loss: 0.7316173315048218\n",
            "Epoch 7, Batch 31, Loss: 0.6873178482055664\n",
            "Epoch 7, Batch 32, Loss: 0.6722245216369629\n",
            "Epoch 7, Batch 33, Loss: 0.7982395887374878\n",
            "Epoch 8, Batch 1, Loss: 0.9294679760932922\n",
            "Epoch 8, Batch 2, Loss: 0.4737967848777771\n",
            "Epoch 8, Batch 3, Loss: 0.8575042486190796\n",
            "Epoch 8, Batch 4, Loss: 0.6392402648925781\n",
            "Epoch 8, Batch 5, Loss: 0.5074606537818909\n",
            "Epoch 8, Batch 6, Loss: 0.6954261064529419\n",
            "Epoch 8, Batch 7, Loss: 0.6438837051391602\n",
            "Epoch 8, Batch 8, Loss: 0.9237982034683228\n",
            "Epoch 8, Batch 9, Loss: 0.7949333190917969\n",
            "Epoch 8, Batch 10, Loss: 0.8044442534446716\n",
            "Epoch 8, Batch 11, Loss: 0.8977206349372864\n",
            "Epoch 8, Batch 12, Loss: 0.6647158861160278\n",
            "Epoch 8, Batch 13, Loss: 0.7013293504714966\n",
            "Epoch 8, Batch 14, Loss: 0.6514626741409302\n",
            "Epoch 8, Batch 15, Loss: 0.6907206773757935\n",
            "Epoch 8, Batch 16, Loss: 0.6605626344680786\n",
            "Epoch 8, Batch 17, Loss: 0.6829342842102051\n",
            "Epoch 8, Batch 18, Loss: 0.8707374930381775\n",
            "Epoch 8, Batch 19, Loss: 1.0636905431747437\n",
            "Epoch 8, Batch 20, Loss: 0.5431740880012512\n",
            "Epoch 8, Batch 21, Loss: 1.0477182865142822\n",
            "Epoch 8, Batch 22, Loss: 0.8524844646453857\n",
            "Epoch 8, Batch 23, Loss: 0.5836818218231201\n",
            "Epoch 8, Batch 24, Loss: 0.6945791840553284\n",
            "Epoch 8, Batch 25, Loss: 0.682619571685791\n",
            "Epoch 8, Batch 26, Loss: 0.6667097210884094\n",
            "Epoch 8, Batch 27, Loss: 0.6492817401885986\n",
            "Epoch 8, Batch 28, Loss: 0.8317157030105591\n",
            "Epoch 8, Batch 29, Loss: 0.48269665241241455\n",
            "Epoch 8, Batch 30, Loss: 0.7194830179214478\n",
            "Epoch 8, Batch 31, Loss: 0.7007521390914917\n",
            "Epoch 8, Batch 32, Loss: 0.6980836391448975\n",
            "Epoch 8, Batch 33, Loss: 0.8476403951644897\n",
            "Epoch 9, Batch 1, Loss: 0.9290850162506104\n",
            "Epoch 9, Batch 2, Loss: 0.4812297523021698\n",
            "Epoch 9, Batch 3, Loss: 0.8696218729019165\n",
            "Epoch 9, Batch 4, Loss: 0.6595429182052612\n",
            "Epoch 9, Batch 5, Loss: 0.5097317099571228\n",
            "Epoch 9, Batch 6, Loss: 0.7081584334373474\n",
            "Epoch 9, Batch 7, Loss: 0.6706832051277161\n",
            "Epoch 9, Batch 8, Loss: 0.8980772495269775\n",
            "Epoch 9, Batch 9, Loss: 0.7576864957809448\n",
            "Epoch 9, Batch 10, Loss: 0.7985864281654358\n",
            "Epoch 9, Batch 11, Loss: 0.8293726444244385\n",
            "Epoch 9, Batch 12, Loss: 0.6638894081115723\n",
            "Epoch 9, Batch 13, Loss: 0.6632124781608582\n",
            "Epoch 9, Batch 14, Loss: 0.6434674263000488\n",
            "Epoch 9, Batch 15, Loss: 0.6757708787918091\n",
            "Epoch 9, Batch 16, Loss: 0.6751876473426819\n",
            "Epoch 9, Batch 17, Loss: 0.6611412763595581\n",
            "Epoch 9, Batch 18, Loss: 0.8714628219604492\n",
            "Epoch 9, Batch 19, Loss: 1.073563575744629\n",
            "Epoch 9, Batch 20, Loss: 0.49471601843833923\n",
            "Epoch 9, Batch 21, Loss: 0.9979028701782227\n",
            "Epoch 9, Batch 22, Loss: 0.8872626423835754\n",
            "Epoch 9, Batch 23, Loss: 0.5840520858764648\n",
            "Epoch 9, Batch 24, Loss: 0.6754363775253296\n",
            "Epoch 9, Batch 25, Loss: 0.6399474143981934\n",
            "Epoch 9, Batch 26, Loss: 0.6976929903030396\n",
            "Epoch 9, Batch 27, Loss: 0.6242460012435913\n",
            "Epoch 9, Batch 28, Loss: 0.7362356781959534\n",
            "Epoch 9, Batch 29, Loss: 0.4606967270374298\n",
            "Epoch 9, Batch 30, Loss: 0.7411563992500305\n",
            "Epoch 9, Batch 31, Loss: 0.6874536871910095\n",
            "Epoch 9, Batch 32, Loss: 0.6701223850250244\n",
            "Epoch 9, Batch 33, Loss: 0.8033790588378906\n",
            "Epoch 10, Batch 1, Loss: 0.9293431639671326\n",
            "Epoch 10, Batch 2, Loss: 0.4727092385292053\n",
            "Epoch 10, Batch 3, Loss: 0.8575166463851929\n",
            "Epoch 10, Batch 4, Loss: 0.6344943046569824\n",
            "Epoch 10, Batch 5, Loss: 0.510913610458374\n",
            "Epoch 10, Batch 6, Loss: 0.6853388547897339\n",
            "Epoch 10, Batch 7, Loss: 0.6407173275947571\n",
            "Epoch 10, Batch 8, Loss: 0.9280389547348022\n",
            "Epoch 10, Batch 9, Loss: 0.774991512298584\n",
            "Epoch 10, Batch 10, Loss: 0.8006377220153809\n",
            "Epoch 10, Batch 11, Loss: 0.8696950078010559\n",
            "Epoch 10, Batch 12, Loss: 0.6940468549728394\n",
            "Epoch 10, Batch 13, Loss: 0.6265308260917664\n",
            "Epoch 10, Batch 14, Loss: 0.637183666229248\n",
            "Epoch 10, Batch 15, Loss: 0.6864901185035706\n",
            "Epoch 10, Batch 16, Loss: 0.6849358081817627\n",
            "Epoch 10, Batch 17, Loss: 0.6017191410064697\n",
            "Epoch 10, Batch 18, Loss: 0.8360145092010498\n",
            "Epoch 10, Batch 19, Loss: 1.07342529296875\n",
            "Epoch 10, Batch 20, Loss: 0.5205755829811096\n",
            "Epoch 10, Batch 21, Loss: 0.9470934867858887\n",
            "Epoch 10, Batch 22, Loss: 0.8562211990356445\n",
            "Epoch 10, Batch 23, Loss: 0.5840089321136475\n",
            "Epoch 10, Batch 24, Loss: 0.6762363910675049\n",
            "Epoch 10, Batch 25, Loss: 0.5871826410293579\n",
            "Epoch 10, Batch 26, Loss: 0.6643574237823486\n",
            "Epoch 10, Batch 27, Loss: 0.6340004205703735\n",
            "Epoch 10, Batch 28, Loss: 0.7270314693450928\n",
            "Epoch 10, Batch 29, Loss: 0.4492299556732178\n",
            "Epoch 10, Batch 30, Loss: 0.7242619395256042\n",
            "Epoch 10, Batch 31, Loss: 0.6796401739120483\n",
            "Epoch 10, Batch 32, Loss: 0.6596352458000183\n",
            "Epoch 10, Batch 33, Loss: 0.7841495275497437\n",
            "Epoch 11, Batch 1, Loss: 0.9305828213691711\n",
            "Epoch 11, Batch 2, Loss: 0.4733915627002716\n",
            "Epoch 11, Batch 3, Loss: 0.8802310228347778\n",
            "Epoch 11, Batch 4, Loss: 0.6307820677757263\n",
            "Epoch 11, Batch 5, Loss: 0.5052394866943359\n",
            "Epoch 11, Batch 6, Loss: 0.6816238760948181\n",
            "Epoch 11, Batch 7, Loss: 0.6398539543151855\n",
            "Epoch 11, Batch 8, Loss: 0.9310364723205566\n",
            "Epoch 11, Batch 9, Loss: 0.786156177520752\n",
            "Epoch 11, Batch 10, Loss: 0.7979965209960938\n",
            "Epoch 11, Batch 11, Loss: 0.8561755418777466\n",
            "Epoch 11, Batch 12, Loss: 0.6937462091445923\n",
            "Epoch 11, Batch 13, Loss: 0.6216272115707397\n",
            "Epoch 11, Batch 14, Loss: 0.6338613033294678\n",
            "Epoch 11, Batch 15, Loss: 0.679381251335144\n",
            "Epoch 11, Batch 16, Loss: 0.6817036271095276\n",
            "Epoch 11, Batch 17, Loss: 0.5989029407501221\n",
            "Epoch 11, Batch 18, Loss: 0.8248190879821777\n",
            "Epoch 11, Batch 19, Loss: 1.0699326992034912\n",
            "Epoch 11, Batch 20, Loss: 0.5078409314155579\n",
            "Epoch 11, Batch 21, Loss: 0.9363582730293274\n",
            "Epoch 11, Batch 22, Loss: 0.8601154685020447\n",
            "Epoch 11, Batch 23, Loss: 0.5845009088516235\n",
            "Epoch 11, Batch 24, Loss: 0.6737093925476074\n",
            "Epoch 11, Batch 25, Loss: 0.5889947414398193\n",
            "Epoch 11, Batch 26, Loss: 0.6638991236686707\n",
            "Epoch 11, Batch 27, Loss: 0.6364352703094482\n",
            "Epoch 11, Batch 28, Loss: 0.7266157269477844\n",
            "Epoch 11, Batch 29, Loss: 0.4491078853607178\n",
            "Epoch 11, Batch 30, Loss: 0.7242360711097717\n",
            "Epoch 11, Batch 31, Loss: 0.6869230270385742\n",
            "Epoch 11, Batch 32, Loss: 0.6700226068496704\n",
            "Epoch 11, Batch 33, Loss: 0.7851672172546387\n",
            "Epoch 12, Batch 1, Loss: 0.9299236536026001\n",
            "Epoch 12, Batch 2, Loss: 0.4703379273414612\n",
            "Epoch 12, Batch 3, Loss: 0.8574061989784241\n",
            "Epoch 12, Batch 4, Loss: 0.6315107941627502\n",
            "Epoch 12, Batch 5, Loss: 0.5064721703529358\n",
            "Epoch 12, Batch 6, Loss: 0.6918612122535706\n",
            "Epoch 12, Batch 7, Loss: 0.6397033929824829\n",
            "Epoch 12, Batch 8, Loss: 0.9289661645889282\n",
            "Epoch 12, Batch 9, Loss: 0.7817897796630859\n",
            "Epoch 12, Batch 10, Loss: 0.7980724573135376\n",
            "Epoch 12, Batch 11, Loss: 0.853718638420105\n",
            "Epoch 12, Batch 12, Loss: 0.6899579167366028\n",
            "Epoch 12, Batch 13, Loss: 0.6224215030670166\n",
            "Epoch 12, Batch 14, Loss: 0.6343265771865845\n",
            "Epoch 12, Batch 15, Loss: 0.6819925904273987\n",
            "Epoch 12, Batch 16, Loss: 0.6831965446472168\n",
            "Epoch 12, Batch 17, Loss: 0.5995450019836426\n",
            "Epoch 12, Batch 18, Loss: 0.8296700716018677\n",
            "Epoch 12, Batch 19, Loss: 1.0699846744537354\n",
            "Epoch 12, Batch 20, Loss: 0.5096514225006104\n",
            "Epoch 12, Batch 21, Loss: 0.9433386325836182\n",
            "Epoch 12, Batch 22, Loss: 0.8535324335098267\n",
            "Epoch 12, Batch 23, Loss: 0.583632230758667\n",
            "Epoch 12, Batch 24, Loss: 0.6764717698097229\n",
            "Epoch 12, Batch 25, Loss: 0.5854911208152771\n",
            "Epoch 12, Batch 26, Loss: 0.6598340272903442\n",
            "Epoch 12, Batch 27, Loss: 0.636772096157074\n",
            "Epoch 12, Batch 28, Loss: 0.7362105250358582\n",
            "Epoch 12, Batch 29, Loss: 0.4438031315803528\n",
            "Epoch 12, Batch 30, Loss: 0.7167691588401794\n",
            "Epoch 12, Batch 31, Loss: 0.6883155703544617\n",
            "Epoch 12, Batch 32, Loss: 0.6712290644645691\n",
            "Epoch 12, Batch 33, Loss: 0.7910010814666748\n",
            "Epoch 13, Batch 1, Loss: 0.9287950396537781\n",
            "Epoch 13, Batch 2, Loss: 0.48004937171936035\n",
            "Epoch 13, Batch 3, Loss: 0.8580528497695923\n",
            "Epoch 13, Batch 4, Loss: 0.6344929933547974\n",
            "Epoch 13, Batch 5, Loss: 0.5046391487121582\n",
            "Epoch 13, Batch 6, Loss: 0.7032325863838196\n",
            "Epoch 13, Batch 7, Loss: 0.6494271159172058\n",
            "Epoch 13, Batch 8, Loss: 0.9288097023963928\n",
            "Epoch 13, Batch 9, Loss: 0.7794359922409058\n",
            "Epoch 13, Batch 10, Loss: 0.7972286939620972\n",
            "Epoch 13, Batch 11, Loss: 0.8362083435058594\n",
            "Epoch 13, Batch 12, Loss: 0.6796155571937561\n",
            "Epoch 13, Batch 13, Loss: 0.6287140846252441\n",
            "Epoch 13, Batch 14, Loss: 0.6343308687210083\n",
            "Epoch 13, Batch 15, Loss: 0.6837056875228882\n",
            "Epoch 13, Batch 16, Loss: 0.680126965045929\n",
            "Epoch 13, Batch 17, Loss: 0.6075789928436279\n",
            "Epoch 13, Batch 18, Loss: 0.8326034545898438\n",
            "Epoch 13, Batch 19, Loss: 1.0691900253295898\n",
            "Epoch 13, Batch 20, Loss: 0.5104457139968872\n",
            "Epoch 13, Batch 21, Loss: 0.9488315582275391\n",
            "Epoch 13, Batch 22, Loss: 0.8560448288917542\n",
            "Epoch 13, Batch 23, Loss: 0.5840839743614197\n",
            "Epoch 13, Batch 24, Loss: 0.6764342784881592\n",
            "Epoch 13, Batch 25, Loss: 0.5841082334518433\n",
            "Epoch 13, Batch 26, Loss: 0.6653020977973938\n",
            "Epoch 13, Batch 27, Loss: 0.6367341876029968\n",
            "Epoch 13, Batch 28, Loss: 0.7198964953422546\n",
            "Epoch 13, Batch 29, Loss: 0.44705355167388916\n",
            "Epoch 13, Batch 30, Loss: 0.7243533134460449\n",
            "Epoch 13, Batch 31, Loss: 0.6871545314788818\n",
            "Epoch 13, Batch 32, Loss: 0.6695771217346191\n",
            "Epoch 13, Batch 33, Loss: 0.7807273864746094\n",
            "Epoch 14, Batch 1, Loss: 0.9301928877830505\n",
            "Epoch 14, Batch 2, Loss: 0.47375863790512085\n",
            "Epoch 14, Batch 3, Loss: 0.8579528331756592\n",
            "Epoch 14, Batch 4, Loss: 0.6291402578353882\n",
            "Epoch 14, Batch 5, Loss: 0.5051549673080444\n",
            "Epoch 14, Batch 6, Loss: 0.6920355558395386\n",
            "Epoch 14, Batch 7, Loss: 0.637727677822113\n",
            "Epoch 14, Batch 8, Loss: 0.9321929812431335\n",
            "Epoch 14, Batch 9, Loss: 0.7859128713607788\n",
            "Epoch 14, Batch 10, Loss: 0.7978922128677368\n",
            "Epoch 14, Batch 11, Loss: 0.8542838096618652\n",
            "Epoch 14, Batch 12, Loss: 0.693527340888977\n",
            "Epoch 14, Batch 13, Loss: 0.6176045536994934\n",
            "Epoch 14, Batch 14, Loss: 0.6277981996536255\n",
            "Epoch 14, Batch 15, Loss: 0.6649553775787354\n",
            "Epoch 14, Batch 16, Loss: 0.6862671971321106\n",
            "Epoch 14, Batch 17, Loss: 0.5856324434280396\n",
            "Epoch 14, Batch 18, Loss: 0.8188471794128418\n",
            "Epoch 14, Batch 19, Loss: 1.0678274631500244\n",
            "Epoch 14, Batch 20, Loss: 0.5089101195335388\n",
            "Epoch 14, Batch 21, Loss: 0.9329718351364136\n",
            "Epoch 14, Batch 22, Loss: 0.8445947170257568\n",
            "Epoch 14, Batch 23, Loss: 0.5830609202384949\n",
            "Epoch 14, Batch 24, Loss: 0.6773849725723267\n",
            "Epoch 14, Batch 25, Loss: 0.5681714415550232\n",
            "Epoch 14, Batch 26, Loss: 0.6526013612747192\n",
            "Epoch 14, Batch 27, Loss: 0.632603108882904\n",
            "Epoch 14, Batch 28, Loss: 0.7147327661514282\n",
            "Epoch 14, Batch 29, Loss: 0.4442901611328125\n",
            "Epoch 14, Batch 30, Loss: 0.7207908630371094\n",
            "Epoch 14, Batch 31, Loss: 0.6871733665466309\n",
            "Epoch 14, Batch 32, Loss: 0.6698576211929321\n",
            "Epoch 14, Batch 33, Loss: 0.7746856212615967\n",
            "Epoch 15, Batch 1, Loss: 0.9338098168373108\n",
            "Epoch 15, Batch 2, Loss: 0.4852011799812317\n",
            "Epoch 15, Batch 3, Loss: 0.8580076098442078\n",
            "Epoch 15, Batch 4, Loss: 0.6283624172210693\n",
            "Epoch 15, Batch 5, Loss: 0.5042468309402466\n",
            "Epoch 15, Batch 6, Loss: 0.6940304636955261\n",
            "Epoch 15, Batch 7, Loss: 0.6382707953453064\n",
            "Epoch 15, Batch 8, Loss: 0.9325737357139587\n",
            "Epoch 15, Batch 9, Loss: 0.7885243892669678\n",
            "Epoch 15, Batch 10, Loss: 0.7974241375923157\n",
            "Epoch 15, Batch 11, Loss: 0.8496803045272827\n",
            "Epoch 15, Batch 12, Loss: 0.6925302147865295\n",
            "Epoch 15, Batch 13, Loss: 0.6162549257278442\n",
            "Epoch 15, Batch 14, Loss: 0.6290278434753418\n",
            "Epoch 15, Batch 15, Loss: 0.6773966550827026\n",
            "Epoch 15, Batch 16, Loss: 0.6851451992988586\n",
            "Epoch 15, Batch 17, Loss: 0.5864655375480652\n",
            "Epoch 15, Batch 18, Loss: 0.8184570074081421\n",
            "Epoch 15, Batch 19, Loss: 1.0674242973327637\n",
            "Epoch 15, Batch 20, Loss: 0.5089470744132996\n",
            "Epoch 15, Batch 21, Loss: 0.915778636932373\n",
            "Epoch 15, Batch 22, Loss: 0.854496955871582\n",
            "Epoch 15, Batch 23, Loss: 0.5844605565071106\n",
            "Epoch 15, Batch 24, Loss: 0.6784759759902954\n",
            "Epoch 15, Batch 25, Loss: 0.5637202262878418\n",
            "Epoch 15, Batch 26, Loss: 0.6530951857566833\n",
            "Epoch 15, Batch 27, Loss: 0.6365275979042053\n",
            "Epoch 15, Batch 28, Loss: 0.7156438827514648\n",
            "Epoch 15, Batch 29, Loss: 0.4439236521720886\n",
            "Epoch 15, Batch 30, Loss: 0.718432605266571\n",
            "Epoch 15, Batch 31, Loss: 0.6869636178016663\n",
            "Epoch 15, Batch 32, Loss: 0.6663647890090942\n",
            "Epoch 15, Batch 33, Loss: 0.7743275761604309\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 700x300 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model, r2list, rmselist = train_add_model(LSTM_ADDModel, LSTM_ADD_Config(), num_epochs=15)\n",
        "\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(7, 3))\n",
        "\n",
        "    # Plot R2\n",
        "axes[0].plot(r2list, label='R¬≤_train', color='purple', marker='o')\n",
        "axes[0].set_title('R¬≤ Over Epochs')\n",
        "axes[0].set_xlabel('Epochs')\n",
        "axes[0].set_ylabel('R¬≤')\n",
        "axes[0].grid(True)\n",
        "axes[0].legend()\n",
        "\n",
        "\n",
        "axes[1].plot(rmselist, label='RMSE_train', color='green', marker='o')\n",
        "axes[1].set_title('RMSE Over Epochs')\n",
        "axes[1].set_xlabel('Epochs')\n",
        "axes[1].set_ylabel('RMSE')\n",
        "axes[1].grid(True)\n",
        "axes[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
